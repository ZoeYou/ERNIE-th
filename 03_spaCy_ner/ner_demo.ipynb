{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import wikipedia\n",
    "import warnings\n",
    "import urllib \n",
    "import time\n",
    "import json\n",
    "import sqlite3\n",
    "import requests\n",
    "import mwparserfromhell\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import wikitextparser as wtp\n",
    "\n",
    "from spacy import displacy\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from wasabi import msg\n",
    "from collections import defaultdict   \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Term:\n",
    "    \"\"\"Class of term\"\"\"\n",
    "    def __init__(self, term_name, title, summary):\n",
    "        self.term_name = term_name\n",
    "        self.title = title\n",
    "        self.summary = summary\n",
    "        \n",
    "    @property\n",
    "    def title(self):\n",
    "        return '{}'.format(self.title)\n",
    "    \n",
    "    @property\n",
    "    def summary(self):\n",
    "        return '{}'.format(self.summary)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Term({}: {}, {})\".format(self.term_name, self.title, self.summary)\n",
    "    \n",
    "    \n",
    "    \n",
    "def insert_term(term):\n",
    "    with conn:\n",
    "        cursor.execute(\"INSERT INTO algodraftapp_wiki_info VALUES (:term_name, :title, :summary)\", \n",
    "                       {'term_name':term.term_name, 'title': term.title, 'summary':term.summary})\n",
    "\n",
    "        \n",
    "def remove_term_by_termname(termname):\n",
    "    with conn:\n",
    "        cursor.execute(\"DELETE from algodraftapp_wiki_info WHERE term_name = :term_name\",\n",
    "                      {'term_name':termname})\n",
    "\n",
    "        \n",
    "def remove_term_by_title(title):\n",
    "    with conn:\n",
    "        cursor.execute(\"DELETE from algodraftapp_wiki_info WHERE title = :title\",{'title':title}) \n",
    "\n",
    "\n",
    "def update_summary(term_name, summary):\n",
    "    with conn:\n",
    "        cursor.execute(\"\"\"UPDATE algodraftapp_wiki_info SET summary = :summary\n",
    "                        WHERE term_name = :term_name\"\"\",\n",
    "                       {'term_name':term_name, 'summary': summary})\n",
    "\n",
    "        \n",
    "def update_title(term_name, title):\n",
    "    with conn:\n",
    "        cursor.execute(\"\"\"UPDATE algodraftapp_wiki_info SET title = :title\n",
    "                        WHERE term_name = :term_name\"\"\",\n",
    "                       {'term_name':term_name, 'title': title})    \n",
    "\n",
    "        \n",
    "def update_hyponyms(term_name, hyponyms):\n",
    "    with conn:\n",
    "        cursor.execute(\"\"\"UPDATE algodraftapp_wiki_info SET hyponyms = :hyponyms\n",
    "                        WHERE term_name = :term_name\"\"\",\n",
    "                       {'term_name':term_name, 'hyponyms': hyponyms}) \n",
    "        \n",
    "        \n",
    "def update_synonyms(term_name, synonyms):\n",
    "    with conn:\n",
    "        cursor.execute(\"\"\"UPDATE algodraftapp_wiki_info SET synonyms = :synonyms\n",
    "                        WHERE term_name = :term_name\"\"\",\n",
    "                       {'term_name':term_name, 'synonyms': synonyms}) \n",
    "\n",
    "        \n",
    "def update_hypernyms(term_name, hyperonyms):\n",
    "    with conn:\n",
    "        cursor.execute(\"\"\"UPDATE algodraftapp_wiki_info SET hyperonyms = :hyperonyms\n",
    "                        WHERE term_name = :term_name\"\"\",\n",
    "                       {'term_name':term_name, 'hyperonyms': hyperonyms})         \n",
    "        \n",
    "\n",
    "def get_term_by_termname(termname):\n",
    "    cursor.execute(\"SELECT * FROM algodraftapp_wiki_info WHERE term_name= :term_name\", {'term_name': termname})\n",
    "    return cursor.fetchone()        \n",
    "\n",
    "\n",
    "def get_terms_by_title(title):\n",
    "    cursor.execute(\"SELECT * FROM algodraftapp_wiki_info WHERE title= :title\", {'title': title})\n",
    "    return cursor.fetchall()\n",
    "\n",
    "\n",
    "def find_wiki_title(term):\n",
    "    \"\"\"\n",
    "    Using the wikipedia API to find the corresponding page title\n",
    "    \"\"\"\n",
    "    title = wikipedia.search(term)\n",
    "    if title:\n",
    "        return title[0]\n",
    "\n",
    "\n",
    "def find_wiki_summary(term, nb_sent=2):\n",
    "    \"\"\"\n",
    "    Using the wikipedia API to find the corresponding wikipedia abstract (the first paragraph of the wikipedia page)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ret = wikipedia.summary(term, sentences = nb_sent).replace('\\n  \\n    \\n      \\n        ', '')\n",
    "        return re.sub(r'\\n      \\n    \\n    {\\\\displaystyle .}\\n  ','',ret)\n",
    "    \n",
    "    # if it is a ambiguous term, the function will return None as value of summary\n",
    "    except wikipedia.exceptions.WikipediaException:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def parse(title, API_URL):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"rvslots\": \"main\",\n",
    "        \"rvlimit\": 1,\n",
    "        \"titles\": title,\n",
    "        \"format\": \"json\",\n",
    "        \"formatversion\": \"2\",\n",
    "        \"redirects\" : 1\n",
    "    }\n",
    "    headers = {\"User-Agent\": \"My-Bot-Name/1.0\"}\n",
    "    req = requests.get(API_URL, headers=headers, params=params)\n",
    "    res = req.json()\n",
    "    revision = res[\"query\"][\"pages\"][0]['revisions'][0]\n",
    "    text = revision[\"slots\"][\"main\"][\"content\"]\n",
    "    return mwparserfromhell.parse(text)\n",
    "\n",
    "\n",
    "def is_plural_or_initialism(ret1):\n",
    "    for template in ['plural of', 'initialism of', 'present participle of', 'alternative spelling of', 'abbreviation of', 'inflection of']:\n",
    "        if template in ret1:\n",
    "            return wtp.parse(ret1).templates[0].arguments[1].string[1:]\n",
    "\n",
    "\n",
    "def sub_temps(description):\n",
    "    templates = wtp.parse(description).templates\n",
    "    while templates:\n",
    "        temp = templates.pop(0)\n",
    "        if 'lb' in temp.string:\n",
    "            description = description.replace(temp.string,f'({temp.arguments[1].string[1:]})')\n",
    "        if 'l|en' in temp.string:\n",
    "            description = description.replace(temp.string,f'{temp.arguments[1].string[1:]}')\n",
    "    templates = wtp.parse(description).templates\n",
    "    if templates: description = sub_temps(description)\n",
    "    else: return description\n",
    "\n",
    "\n",
    "def get_description_wiktionary(term):\n",
    "#     term = term.lower()\n",
    "    try:\n",
    "        wikicode = parse(term, \"https://en.wiktionary.org/w/api.php\")\n",
    "        parsed = wtp.parse(str(wikicode))\n",
    "\n",
    "        for sec in parsed.sections:\n",
    "            if str(sec.title).lower() in ['noun','proper noun'] :\n",
    "                break\n",
    "        description = sec.get_lists()[0].items[0]\n",
    "        \n",
    "        # check if it is the plural or the initialism of another term\n",
    "        check = is_plural_or_initialism(description)\n",
    "        if check: \n",
    "            return get_description_wiktionary(check)\n",
    "        \n",
    "        description = sub_temps(description) \n",
    "        description = wtp.parse(description).plain_text().strip()\n",
    "        \n",
    "        if description[-1] == ':': description = description[:-1]      \n",
    "        return description\n",
    "    \n",
    "    except KeyError:\n",
    "        return\n",
    "\n",
    "\n",
    "def get_description_wikipedia(term):              \n",
    "    try:\n",
    "        wikicode = parse(term, \"https://en.wikipedia.org/w/api.php\")\n",
    "        templates = wikicode.filter_templates()    \n",
    "        flag = 0\n",
    "        for temp in templates:\n",
    "            if str(temp.name) == 'short description':\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag:\n",
    "            return str(temp.get(1))\n",
    "    except KeyError or ValueError:\n",
    "        return   \n",
    "     \n",
    "to_remove = ['conductive material forms electrodes',\n",
    "             'successful detection',\n",
    "             'smaller ratio',\n",
    "             'successful watermark detection',\n",
    "             'unsupervised',\n",
    "             'detected',\n",
    "             'K',\n",
    "             'm=2Ap',\n",
    "             'knowledge of K=(KS',\n",
    "             'depth log2(N',\n",
    "             'images N',\n",
    "             'cm2',\n",
    "             'K.\\n',\n",
    "             'quantum code C;<br/',\n",
    "             'codeword c(M',\n",
    "             'by-letter encryption U(KS',\n",
    "             'priority ranks'\n",
    "            ]\n",
    "        \n",
    "# def text_to_html(text, nlp): # gives the html under BeatifulSoup format\n",
    "#     doc = nlp(text)\n",
    "\n",
    "#     html = displacy.render(doc, style=\"ent\", options={\"ents\": [\"TERM\"]}, jupyter=False, page=True)\n",
    "#     soup = BeautifulSoup(html)\n",
    "#     marks =  soup.find_all('mark')\n",
    "#     url = ''\n",
    "    \n",
    "#     for mark in tqdm(marks):\n",
    "#         try:\n",
    "#             term = mark.get_text(strip=True,separator=', ').split(', ')[0] # get the term annotated\n",
    "#             if term in to_remove: continue\n",
    "            \n",
    "#             wiki_info = DICT_PAGE_TITLE[term] # get wikipedia pagetitle and summary from json file\n",
    "#             url = f'https://en.wikipedia.org/wiki/{\"_\".join(wiki_info[\"title\"].split())}' \n",
    "#             summary = wiki_info['summary']\n",
    "            \n",
    "#         except KeyError:\n",
    "#             wiki_title = find_wiki_title(term)  \n",
    "            \n",
    "#             if wiki_title:\n",
    "#                 url = f'https://en.wikipedia.org/wiki/{\"_\".join(wiki_title.split())}' \n",
    "#                 wiki_summary = find_wiki_summary(wiki_title)\n",
    "#                 DICT_PAGE_TITLE.update({term:{'title':wiki_title, 'summary': wiki_summary}})\n",
    "                    \n",
    "#         link = soup.new_tag('a', href=url) # create the html tag for link       \n",
    "#         mark.wrap(link) #add html tag <a> (the one to make links) to around our annotated word\n",
    "#     return soup\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def term_lemmatized(term):\n",
    "    words = term.split()\n",
    "    if len(words)>1:\n",
    "        target = words[-1]\n",
    "        lemma = wnl.lemmatize(target)\n",
    "        words[-1] = lemma\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        target = words[0]\n",
    "        return wnl.lemmatize(target)\n",
    "\n",
    "\n",
    "def text_to_json(text, nlp): # gives the ner results in json format\n",
    "    \n",
    "    dict_position = defaultdict(list)\n",
    "    dict_position_trigger = defaultdict(list)\n",
    "    dict_res = {}\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'TERM':    \n",
    "            term = ent.text\n",
    "            try:\n",
    "                while (term[0] == '.' or term[-1] in ['.','_', ';', '\\n', ' ', ',']) and ent.end_char>0 and (not term.isupper() or (term.isupper() and len(term)<=4)) :\n",
    "                    ent.end_char -= 1\n",
    "                    term = term[:-1]  \n",
    "                if(term in to_remove): continue\n",
    "                dict_position[term].append((ent.start_char, ent.end_char))\n",
    "            except IndexError: # single character term\n",
    "                continue         \n",
    "            \n",
    "        else: # if it is a trigger word\n",
    "            dict_position_trigger[ent.text].append((ent.start_char, ent.end_char))          \n",
    "            \n",
    "    dict_position = dict(dict_position)\n",
    "    \n",
    "    msg.info(\"Analysing errors...\")\n",
    "    for error, pos_l in tqdm(dict_position_trigger.items()): # only for ERROR\n",
    "        dict_res.update({error: {'label': 'ERROR', 'position': pos_l}})\n",
    "    msg.good(\"Done!\")\n",
    "    \n",
    "    msg.info(\"Analysing terms...\")\n",
    "    cnt = 0\n",
    "    for term, pos_l in tqdm(dict_position.items()): # only for TERM\n",
    "\n",
    "        wiki_info_queryset = get_term_by_termname(term) # get wikipedia pagetitle and summary from database\n",
    "        if wiki_info_queryset: # if exists in database\n",
    "            url = f'https://en.wikipedia.org/wiki/{\"_\".join(wiki_info_queryset[1].split())}' \n",
    "            wiki_summary = wiki_info_queryset[2] \n",
    "        else:  \n",
    "            # get the lemmatized term \n",
    "            term_lem = term_lemmatized(term)\n",
    "            wiki_title = find_wiki_title(term_lem)\n",
    "            \n",
    "            # find summary  \n",
    "            try:\n",
    "                ## witionary\n",
    "                wiki_summary = get_description_wiktionary(term_lem)\n",
    "                if not wiki_summary or '|' in wiki_summary:\n",
    "                    ## wikipedia short description\n",
    "                    wiki_summary = get_description_wikipedia(term_lem)\n",
    "                if (not wiki_summary) and wiki_title:                  \n",
    "                    if not wiki_summary: ## wiktionary definition\n",
    "                        wiki_summary = get_description_wiktionary(wiki_title)\n",
    "                    if not wiki_summary: ## wikipedia short description\n",
    "                        wiki_summary = get_description_wikipedia(wiki_title)\n",
    "                    if not wiki_summary: ## wikipedia page abstract\n",
    "                        wiki_summary = find_wiki_summary(wiki_title)\n",
    "                        \n",
    "                term_split = term_lem.split()\n",
    "                term_split.pop(0)\n",
    "                while (not(wiki_summary and wiki_title)) and len(term_split)>0: # truncate the term)\n",
    "                    term_lem = ' '.join(term_split)\n",
    "                    wiki_title = find_wiki_title(term_lem)\n",
    "                    wiki_summary = get_description_wiktionary(term_lem)\n",
    "\n",
    "                    if not wiki_summary or '|' in wiki_summary:\n",
    "                        wiki_summary = get_description_wikipedia(term_lem)\n",
    "                    if (not wiki_summary) and wiki_title:                  \n",
    "                        if not wiki_summary:\n",
    "                            wiki_summary = get_description_wiktionary(wiki_title)\n",
    "                        if not wiki_summary:\n",
    "                            wiki_summary = get_description_wikipedia(wiki_title)\n",
    "                        if not wiki_summary:\n",
    "                            wiki_summary = find_wiki_summary(wiki_title) \n",
    "                    term_split.pop(0)\n",
    "                    \n",
    "            except KeyError: # for terms of ERROR\n",
    "                continue\n",
    "                \n",
    "            # find wiki title\n",
    "            if wiki_title:\n",
    "                url = f'https://en.wikipedia.org/wiki/{\"_\".join(wiki_title.split())}' \n",
    "                # insert new term into database   \n",
    "                insert_term(Term(term, wiki_title, wiki_summary))\n",
    "                \n",
    "            else: # returns the wikipedia main page\n",
    "                url = f'https://www.wikipedia.org'\n",
    "                insert_term(Term(term, '', ''))                         \n",
    "            cnt += 1\n",
    "\n",
    "        dict_res.update({term:{'label': 'TERM', 'position': pos_l,'wikilink': url,'summary': wiki_summary}})\n",
    "    msg.good(\"Done!\")\n",
    "    \n",
    "    if cnt:\n",
    "#         conn.commit()\n",
    "#         conn.close()\n",
    "        msg.good(f\"Found wikipedia information for {cnt} new terms.\")\n",
    "            \n",
    "    return dict_res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('./wiki_info.db')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./claims.txt', encoding = 'utf-8', mode='r') as f:\n",
    "    claims = f.read().replace('<p>', '').replace('</p>','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(r\"../03_spaCy_ner/output/G_2018/model-last/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open('./demo.html', 'w', encoding=\"utf-8\").write(str(text_to_html(claims,nlp))) #create file with html data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 22290.37it/s]\n",
      " 25%|██▍       | 160/653 [00:00<00:00, 1596.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Analysing errors...\u001b[0m\n",
      "\u001b[38;5;2m✔ Done!\u001b[0m\n",
      "\u001b[38;5;4mℹ Analysing terms...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 653/653 [00:00<00:00, 2308.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Done!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# save results\n",
    "with open('demo.json', \"w\") as f: \n",
    "    json.dump(text_to_json(claims, nlp), f, indent = 4) # test with the first patent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base')",
   "language": "python",
   "name": "python37464bitbase5e3e771f486c4f06aa164a453e60de03"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
