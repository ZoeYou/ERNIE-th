<!DOCTYPE html>
<html lang="en">
<head>
<title>displaCy</title>
</head>
<body style="font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr">
<figure style="margin-bottom: 6rem">
<div class="entities" style="line-height: 2.5; direction: ltr">_______________System and method for transforming video data into directional object count_____20191212_____XMLs/xml/ipa191212.xml_____US-20190378283-A1 : US-16435008 : US-62682906_____G06T0007254000 : G06T0005500000 : G06N0020000000 : G06T0007215000The present invention is a computer-implemented system and method for transforming video data into directional object counts. The method of transforming 
<a href="https://en.wikipedia.org/wiki/Data_compression"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is uniquely efficient in that it uses only a single column or row of pixels in a 
<a href="https://en.wikipedia.org/wiki/Video_camera"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video camera
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to define the background from a moving object, count the number of objects and determine their direction. By taking an image of a single column or row every frame and concatenating them together, the result is an image of the object that has passed, referred to herein as a sweep image. In order to determine the direction, two different methods can be used. Method one involves constructing another image using the same method. The two images are then compared, and the direction is determined by the location of the object in the second image compared to the location of the object in the first image. Due to this recording method, elongation or compression of the objects can occur because of acceleration or deceleration of the objects and can be uniquely utilized to determine the speed or movement path of the objects. The second method of determining direction involves comparing the object in the image to an established marker. The transformations can also be used to produce labeled data for 
<a href="https://en.wikipedia.org/wiki/Machine_learning"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    training machine learning
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 models: bounding-boxes provided in sweep image can be transformed to bound boxes in video, and boxes in video can be transformed into boxes in the sweep image._____d:CROSS-REFERENCE TO RELATED APPLICATIONThis application claims the benefit of U.S. Provisional Patent Application Ser. No. 62/682,906, entitled “System and method for transforming video data into directional object count,” filed Jun. 9, 2018, the contents of which are incorporated herein by reference.FIELD OF THE INVENTIONThe present invention relates to methods, systems, and apparatuses for discerning useful information and data about moving objects in an image sequence. More specifically, the invention provides useful information including moving object count as potentially providing the direction and speed of the object travel, all using low-cost 
<a href="https://en.wikipedia.org/wiki/Imaging"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video imaging
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 data of that area.BACKGROUND OF THE INVENTIONImages and video can be rich mediums to encode and transfer information; however, machines are notoriously ineffective at extracting meaning from those mediums. A computer that could see and understand what it sees like a human can is of obvious value. It became evident that what humans, even infants, could easily do, machines could not. It is much easier to teach a machine the steps to perform complex tasks involving higher 
<a href="https://en.wikipedia.org/wiki/Mathematics"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mathematics than
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to teach them the comparably intuitive task of 
<a href="https://en.wikipedia.org/wiki/Outline_of_object_recognition"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object recognition
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. In the 1970s, the attention shifted to attempting to break 
<a href="https://en.wikipedia.org/wiki/Computer_vision"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer vision
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 into its component pieces. These new approaches laid the groundwork for many vision techniques that exist today such as edge detection, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and motion estimation.These approaches have allowed significant advances in 
<a href="https://en.wikipedia.org/wiki/Computer_vision"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer vision
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and they support the growing application of 
<a href="https://en.wikipedia.org/wiki/Machine_learning"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    machine learning
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 approaches to understand images; often, 
<a href="https://en.wikipedia.org/wiki/Outline_of_machine_learning"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    machine learning algorithms
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 use traditional 
<a href="https://en.wikipedia.org/wiki/Computer_vision"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer vision
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 approaches to improve efficiency and accuracy. Traditional 
<a href="https://en.wikipedia.org/wiki/Computer_vision"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer vision
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and machine-learned 
<a href="https://en.wikipedia.org/wiki/Computer_vision"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer vision
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 face trade-offs and can often complement one another in the field. Both require significant computing resources when evaluating video because they must mathematically evaluate each pixel in a sequence of images to extract useful information.There are several traditional approaches to extracting a moving object from an image sequence. The first common approach in the 
<a href="https://en.wikipedia.org/wiki/Prior_art"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    prior art
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is the use of video cameras combined with background subtraction to detect objects in each frame of video and then to track the object over time. 
<a href="https://en.wikipedia.org/wiki/Approach"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    This approach
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, called “standard-background-detection,” while somewhat effective is computationally quite expensive seeing that it must compare each pixel in each frame of video to a background to decide if it is sufficiently different. It then connects the pixels that are sufficiently different into distinct objects and associates these objects over time.This approach accounts for all variables associated with the object and its relative movement, including the direction in both the two-dimensional and three-dimensional field and the size of the object. If there is little to no constraint on the 
<a href="https://en.wikipedia.org/wiki/Motion"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object motion
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, this more general approach may be required. However, in many applications, the flow is constrained, for example, on most roads and in entrances or exits from parking areas. In these cases, the flow of cars has very limited directions and a narrow set of expected positions. In other cases, such as the flow of people through a corridor entrance, the motion can be limited, either in or out.In general, determining the contents of the background image comprises 1) generating a background image that is as close to what one should expect the background to look like; and 2) updating the image to account for temporal changes including changes in illumination or shadows.U.S. Pat. No. 5,748,775 issued May 5, 1998, is a method and apparatus for extracting moving objects by sequentially subtracting input images from an updated background image. More specifically, it records temporal changes in the video frame, such as shadows and illumination, and updates the background image accordingly for use in background subtraction. The method accounts for temporal changes by statistically processing subdivisions of each frame to obtain a statistical quantity that reflects the change in condition. This is performed for each pixel of each frame of video. Referring to FIG. 1A, consider a traditional method for 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. Camera is aimed at a fixed position capturing an image sequence where images 100, 101, 102, and 103 are individual frames trying to extract moving object 160. An attentive reader will notice that the background has multiple sections 110, 111, 112, each subject to individual temporal changes depending on 
<a href="https://en.wikipedia.org/wiki/Component"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    the components
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 within each. For example, if the weather becomes cloudy, section 110 will need to be updated to reflect the change; however, sections 111 and 112 could remain the same. In the same sense, the mountains in section 112 could become covered with snow while 110 and 112 remain unchanged, and so on. The considerable amount of combinations make a per pixel background calculation necessary for each frame. When considering one frame 104, traditional backgro
<a href="https://en.wikipedia.org/wiki/Order_of_operations"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    und subtraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 first creates an updated background image 105 relative to that frame through techniques such as weighted averaging, and performs a subtraction of the binarized data to obtain an extracted image of the object, as seen in 106. This process is repeated for each subsequent frame. In many applications, this approach may be appropriate; however, it can be computationally expensive, seeing that each pixel in each frame of the video must be processed to update the 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.There are many applications that do not require this level of computation—referring now to FIG. 1B, consider the same camera to be capturing the same sequence of images 120, 121, 122, and 123 with the same object 170 moving from left to right. In this approach, now consider 130 to be an isolated column of pixels positioned orthogonal to the object's direction of travel. Each column 140, 141, 142, and 143 represents one image in the image sequence and can be concatenated into a new image 150. This new image would then embody the entire video with each column representing one frame. In updating the 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, it would then be sufficient to account for temporal differences by processing each column of pixels in one image, rather than processing each pixel of many images. This method maintains a similar level of accuracy as that of traditional background updating, but because it is only analyzing one column of pixels for each frame of video, it uses a fraction of the computational power.BRIEF SUMMARY OF THE INVENTIONThe present invention is a computer-implemented method and system for transforming video data into a directional object count. In accordance with one approach, from each image in the image sequence a single column or row of pixels is isolated. The set of these is transformed via sequential concatenation into a separate image referred to herein as a “sweep image.” Each sweep image is transformed into a per-pixel detection signal via techniques of background subtraction comprised of: a) initializing the one dimensional 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
; b) processing the sweep image while adaptively updating the 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to account for temporal changes such as changes in illumination or ghosting; and c) detecting differences between the currently processed column of the sweep image and the background. In another embodiment, 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    the system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 uses multiple 1-dimensional background models, for example, to model the scene element under both sunny and cloudy lighting. The per-pixel detection signal is then transformed into object location, and a detection count is performed via techniques including, for example, quasi-connected components (QCC) (see, for example, Boult, T. E., R. Micheals, X. Gao, 
<a href="https://en.wikipedia.org/wiki/Shantrelle_P._Lewis"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    P. Lewis
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, 
<a href="https://en.wikipedia.org/wiki/Thomas_C._Power"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    C. Power
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, W. Yin, and A. Erkan. “Frame-rate omnidirectional surveillance and tracking of camouflaged and occluded targets.” In Proceedings Second IEEE Workshop on Visual Surveillance (VS′99)(Cat. No. 98-89223), pp. 48-55. IEEE, 1999, which is incorporated herein by reference) which obtain locational data about each object in the form of bounding boxes and their corresponding centroids. The position of centroids are evaluated with reference to the imaginary or drawn centerline separating direction of travel to produce a final directional object count.In accordance with a second approach, the image sequence is transformed into multiple sweep images by selecting and concatenating data from two or more columns or rows of pixels. Transformations to object detections and location are then applied. By analyzing the relative motion between the leading edge of an object, one can determine the direction of travel. One embodiment does this by transforming each sweep image to obtain data about position and scale-invariant feature transformation points (SIFT points (see, for example, U.S. Pat. No. 6,711,293, “Method and apparatus for identifying scale invariant features in an image and use of same for locating an object in an image”, David Lowe's patent for the SIFT algorithm, Mar. 23, 2004, which is incorporated herein by reference). A comparison of data of the set of points in each sweep image can determine object counts and each object's direction of travel. In another embodiment, these counts and estimates of direction are then combined with information about the centerline to produce a final directional object count.In contrast to known art, both approaches use sequential concatenation combined with methods of one-dimensional background subtraction to obtain a directional object count. The present computer-implemented method and system drastically reduce the computational power required over the prior 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 methods.In another embodiment, the video subsystem is configured such that only one row or column from a 2-dimensional 
<a href="https://en.wikipedia.org/wiki/Video_sensor_technology"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sensor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is all that is transferred to a 
<a href="https://en.wikipedia.org/wiki/Central_processing_unit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    main computer processor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, thus reducing the data flow into the 
<a href="https://en.wikipedia.org/wiki/Central_processing_unit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer processor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. In one embodiment this can be accomplished by using a one-dimensional region of interest on a 
<a href="https://en.wikipedia.org/wiki/Video_display_controller"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video chip
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. In another embodiment, a thin 2-dimensional region is binned to produce the 1-dimensional signal. Those skilled in the art will appreciate how this can improve 
<a href="https://en.wikipedia.org/wiki/Low_light"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    low light
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 operation. Another embodiment might use triggered or asymmetric temporal sampling of the 
<a href="https://en.wikipedia.org/wiki/Video"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video signal
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to obtain the one-dimensional signal.BRIEF DESCRIPTION OF THE DRAWINGSFIG. 1A shows a standard-background-subtraction process for a crowded background environment, where the 
<a href="https://en.wikipedia.org/wiki/Video_camera"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video camera
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is aimed at a fixed position. Each frame of video for a duration of time is seen in 100, 101, 102, and 103. 104 depicts one frame in which 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    the system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is calculating an updated background image 105. The result of subtracting image 104 from its relative background 105 is shown in image 106.FIG. 1B shows the process of background-subtraction when used in combination with sweep images. Images 120, 121, 122, and 123 are sequential frames in an image sequence where a particular column of pixels 130 is extract and the set of extracted columns concatenated into corresponding columns 140, 141, 142, and 143 to create a new sweep image 150.FIG. 2 shows a perspective view of a 
<a href="https://en.wikipedia.org/wiki/Video_camera"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video camera
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 looking directly down onto a street with 230, 231, 232, and 233 being one vehicle driving across the 
<a href="https://en.wikipedia.org/wiki/Video_camera"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video camera
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
's view. Each image 200, 201, 202 and 203 represents one frame of video, with there being many frames in between these examples. From each image, the transform isolates two columns of pixels 210 and 220 and, which are concatenated into separate sweep images. Column 210 is being captured for each frame of video to transform it into sweep image 240 while column 220 is being captured for each frame of video to transform it into sweep image 280.FIG. 3 illustrates an image sequence with images 300, 301, 302, and 303 being individual frames capturing two moving vehicles. Images 310 and 315 are sweep images made from the concatenation of two columns of pixels. The vehicles in images 310 and 315 correspond with those in images 300-303; however, the difference in appearance is used to illustrate the elongation and compression that can arise in sweep images.FIG. 4 is a set of two sweep images 410 and 415. The figure shows an imaginary centerline 430 for embodiments of the present invention that use information about the centerline to indicate the direction of travel.FIG. 5 is a 
<a href="https://en.wikipedia.org/wiki/Flow_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    flow diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating one technique to transform the sweep images and detect differences between the sweep image and its background using a multi-background model. The diagram illustrates the flow of processing one pixel from the sweep image.FIG. 6A illustrates a block of pixels 600 with n representing each pixel.FIG. 6B shows a section of an image 603 where each subdivision consists of a group of pixels.FIG. 7 illustrates using an external process that detects objects in the video frames which can be transferred to produce ground-truth boxes in the sweep image.FIG. 8 shows using ground-truth boxes in the sweep image to determine ground-truth boxes in raw video frames.FIG. 9 is schematic of the present computer-implemented system._____c:1. A method of transforming video data into moving object counts comprising the steps of:extracting a 1-dimensional region from each frame of video;constructing a sweep image by appending a plurality of the 1-dimensional regions to form a 2-dimensional image;processing the 2-dimensional sweep image to determine each distinct object within the 2-dimensional image; andreturning the count of detected objects.2. The method according to claim 1, where the step of processing the 2-dimensional sweep image to determine each distinct object comprises the steps of:processing the 2-dimensional sweep image to estimate a 1-dimensional background region corresponding to no objects present;comparing the 1-dimensional regions of the 2-dimensional sweep image to the 1-dimensional background region to determine regions of significant changes; andanalyzing regions of significant changes to determine separate objects.3. The method according to claim 1, where the step of processing the 2-dimensional sweep image to determine each distinct object comprises the steps of:computing edge response perpendicular to the 1-dimensional region used to create the 2-dimensional sweep image; andconnecting pairs of adjacent regions with significant edge response to determine separate objects.4. The method according to claim 1, wherein a direction of object motion is estimated based on relative object position within the 1-dimensional slice.5. The method according to claim 1, wherein:at least two 2-dimensional sweep images are constructed by extracting approximately parallel 1-dimensional regions from different regions from each frame of video;a 
<a href="https://en.wikipedia.org/wiki/Facility_location_problem"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    location analysis
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 step determines a location of each object in each 2-dimensional sweep image;a matching step is performed associating objects between the at least two 2-dimensional sweep images; andan analysis is performed to compute a direction of motion by comparing a relative location of matching objects between the at least two 2-dimensional sweep images.6. The method according to claim 5, wherein background is computed as a 1-dimensional representation where each pixel is a pixel vise median of associated pixels in each 1-dimensional region making up the 2-dimensional sweep image.7. The method according to claim 5, wherein the step of processing the 2-dimensional sweep image to determine each distinct object is accomplished using the quasi-connected components algorithm providing for multi-resolution processing with a plurality of thresholds.8. The method according to claim 5 wherein the analysis to compute the direction of motion is augmented with a secondary process that matches visual features between the at least two 2-dimensional sweep images and fuses the resulting estimate with results based on matched object location.9. The method according to claim 8, wherein the fusion of direction further includes an estimate of the direction of object motion based on relative object position within the 1-dimensional region.10. The method according to claim 1, wherein the step of constructing the 2-dimensional sweep image is used to produce ground truth data for 
<a href="https://en.wikipedia.org/wiki/Machine_learning"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    machine learning
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 by using a secondary process analyzing the 2-dimensional sweep image to determine a ground truth bounding box of each object within the 2-dimensional sweep image, with a set of such ground truth bounding boxes being used in 
<a href="https://en.wikipedia.org/wiki/Machine_learning"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    machine learning
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 models to optimize parameters of 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 the 2-dimensional sweep image.12. The method according to claim 10, wherein the step of constructing the 2-dimensional sweep image stores an identifier of each frame and Wherein detected object location is then used to determine first and last frames when the object was passing over a back-projection of the 1-dimensional region, with frames of video then being used for training a in 
<a href="https://en.wikipedia.org/wiki/Higher_education"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    chine learning model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to optimize detection parameters from an original video.13. The method according to claim 11, where data from the original frame in the neighborhood of the back-projected 1-dimensional region is used to determine a front and a rear of the object, which is then used to extract bounding boxes for each object to be used for 
<a href="https://en.wikipedia.org/wiki/Machine_learning"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    machine learning
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to optimize a secondary systems detection parameters.14. An system for transforming a sequence of image data into moving object counts comprising:an input means for sequentially entering input images containing a moving object region to be counted;a sweep image generation means for extracting 1-dimensional regions for each image and combining them into a 2-dimensional sweep image;storage means for storing the 2-dimensional sweep image;
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 means for processing the 2-dimensional sweep image to detect object locations;counting means to process the detected object locations and determine object counts; andan output means that communicates the object count to an external system.15. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to claim 14, wherein the object detection means estimates the expected 1-dimensional signal when no object is present and compares that with the 1-dimensional signals in the 2-dimensional sweep image to detect and localize each object.16. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to claim 14, wherein the object detection means estimates edge features perpendicular to the 1-dimensional signal of the 2-dimensional sweep image and uses the estimated edge features to detect and localize each object.17. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to claim 14, further including a direction computation means that combines the object locations from the object detection means and the direction of motion is estimated based on the relative object position within the 1-dimensional slice.18. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to claim 14, wherein the 2-dimensional sweep image generation means produces at least two 2-dimensional sweep images from non-overlapping 1-dimensional regions, the object detection means produces object locations for each of the at least two 2-dimensional sweep images, and 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    the system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 includes a direction computation means that combines the object locations from the object detection means and determines the direction of motion.19. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to claim 18, wherein multiple estimates of direction of notion are fused.20. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to claim 14, further including a ground-truth estimation means that estimated each object location within the 2-dimensional sweep image and a machine-learning means that uses the result of the ground estimation means to 
<a href="https://en.wikipedia.org/wiki/Musical_tuning"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    tune system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 parameters to improve performance._______________
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    IMAGE PROCESSING
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 APPARATUS, IMAGE CAPTURING APPARATUS, 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    IMAGE PROCESSING
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 METHOD, AND NON-TRANSITORY COMPUTER-READABLE STORAGE MEDIUM_____20191219_____XMLs/xml/ipa191219.xml_____US-20190385316-A1 : US-16440089 : JP-2018-113845_____G06T0007254000 : G06K0009620000A captured image is registered, as a background image, from among captured images sequentially inputted. In a case where it is determined that a moving object is included in a first background image registered at a first timing, the registered first background image is replaced with a second background image registered at a second timing prior to the first timing._____d:BACKGROUND OF THE INVENTIONField of the InventionThe present invention relates to a technique of detecting an object within an image.Description of the Related ArtConventionally, for a detection of the removal of a specific object in which 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is used, a 
<a href="https://en.wikipedia.org/wiki/Difference_and_Repetition"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background difference
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method, in which a comparison between a captured image and a background image that is obtained in advance is performed and a region not existing in the background image that is obtained in advance is extracted, is often used. FIG. 1 illustrates an example of removal detection processing in which the background difference method is used. Reference numeral 1000 is a background image including an object 101, and reference numerals 1001 to 1003 are captured images captured in this order. Reference numeral 1004 indicates a difference between the background image 1000 and the captured image 1001, reference numeral 1005 indicates a difference between the background image 1000 and the captured image 1002, and reference numeral 1006 indicates a difference between the background image 1000 and the captured image 1003.Since the object 101 is removed at a timing between the captured image 1001 and the captured image 1002, the object 101, which appears in the captured image 1001, does not appear in the captured images 1002 and 1003 as illustrated in FIG. 1. For this reason, in the differences 1005 and 1006, a background difference 103 exists within a removal detection region 102 which was set in advance. An alert 104 (display of the frame surrounding the background difference 103) occurs when it is determined that the removal occurred in a case where the size of the background difference 103 is a stipulated size or more and the difference was present in the captured image for a fixed time or more.In order to guarantee the accuracy of the removal detection, it is necessary to extract an accurate background difference, and to do so it is necessary to always set an appropriate background image. Although it is necessary to update the background image whenever a change in environment such as an illumination condition occurs in order to set an appropriate background image, there is a possibility that a background image that includes a moving object will be set when the background image is updated.FIG. 2 illustrates an example of removal detection processing in a case where a background image including a moving object is set. Reference numeral 2000 is a background image that includes a hand 201 which is a moving object. 
<a href="https://en.wikipedia.org/wiki/Frame_of_reference"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Reference n
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
umerals 2001 to 2003 are captured images which are captured in that order. Reference numeral 2004 indicates a difference between the background image 2000 and the captured image 2001, reference numeral 2005 indicates a difference between the background image 2000 and the captured image 2002, and reference numeral 2006 indicates a difference between the background image 2000 and the captured image 2003.The hand 201 which is captured in the captured image 2001 does not appear in the captured images 2002 and 2003. Here, the background difference 203 occurs within the removal detection region 102 in the differences 2005 and 2006. By this, there is a possibility that an alert 204 (display of the frame surrounding the 
<a href="https://en.wikipedia.org/wiki/Difference_and_Repetition"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background difference
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 203) will occur when it is determined that the removal occurred.A method in which, in order to resolve such a problem, the updating of the background image by an updating means is interrupted when a human body which is not appropriate as a background is detected, and the processing is resumed when the human body is no longer detected is disclosed in Japanese Patent No. 4811653. Additionally, a method in which a frame for which a difference does not exist is extracted by difference-between-frames processing and updating of the background image is performed by using such a frame is disclosed in Japanese Patent Laid-Open No. 2013-257785.However, in the conventional technique disclosed in the above-described Japanese Patent No. 4811653, it is necessary to determine the moving object as a human body and application is thought to be difficult in cases of quick changes and where an angle of view is limited. Also, in the conventional technique disclosed in Japanese Patent Laid-Open No. 2013-257785, it is necessary to also calculate a difference between frames in addition to the 
<a href="https://en.wikipedia.org/wiki/Difference_and_Repetition"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background difference
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and the computation load becomes large.SUMMARY OF THE INVENTIONIn the present invention, a technique for registering a background image appropriate for removal detection is provided.According to the first aspect of the present invention, there is provided an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus comprising: a registration unit configured to register, as a background image, a captured image from among captured images sequentially inputted; a determination unit configured to determine whether or not a moving object is included in a first background image that the registration unit registered at a first timing; and a replacing unit configured to, in a case where the determination unit determines that a moving object is included in the first background image, replace the first background image that the registration unit registered with a second background image registered at a second timing prior to the first timing.According to the second aspect of the present invention, there is provided an image capturing apparatus, comprising: an image capturing unit configured to obtain a captured image, and an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus, comprising: a registration unit configured to register, as a background image, a captured image from among captured images sequentially inputted; a determination unit configured to determine whether or not a moving object is included in a first background image that the registration unit registered at a first timing; and a replacing unit configured to, in a case where the determination unit determines that a moving object is included in the first background image, replace the first background image that the registration unit registered with a second background image registered at a second timing prior to the first timing.According to the third aspect of the present invention, there is provided an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method that an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus performs, the method comprising: registering, as a background image, a captured image from among captured images sequentially inputted; determining whether or not a moving object is included in a first background image registered at a first timing; and in a case where it is determined that a moving object is included in the first background image, replacing the first background image with a second background image registered at a second timing prior to the first timing.According to the fourth aspect of the present invention, there is provided a non-transitory computer-readable 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    storage medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 storing a 
<a href="https://en.wikipedia.org/wiki/Computer_program"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer program
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for causing a computer to function as: a registration unit configured to register, as a background image, a captured image from among captured images sequentially inputted; a determination unit configured to determine whether or not a moving object is included in a first background image that the registration unit registered at a first timing; and a replacing unit configured to, in a case where the determination unit determines that a moving object is included in the first background image, replace the first background image that the registration unit registered with a second background image registered at a second timing prior to the first timing.Further features of the present invention will become apparent from the following description of exemplary embodiments (with reference to the attached drawings).BRIEF DESCRIPTION OF THE DRAWINGSFIG. 1 is a view which illustrates an example of removal detection processing using a 
<a href="https://en.wikipedia.org/wiki/Difference_and_Repetition"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background difference
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method.FIG. 2 is a view which illustrates an example of removal detection processing in the case of setting a background image including a moving object.FIG. 3 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 which illustrates an example of a functional arrangement of an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus.FIG. 4 is a view which illustrates an example of updating a background image.FIG. 5 is a flowchart describing an operation in an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus 300.FIG. 6 is a view which illustrates an example of a configuration of table information.FIG. 7 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 which illustrates an example of a functional arrangement of the 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus.FIG. 8 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 which illustrates an example of a hardware configuration of a computer apparatus.FIG. 9 is a view which illustrates an example of a configuration of a system._____c:1. An 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus comprising:a registration unit configured to register, as a background image, a captured image from among captured images sequentially inputted;a determination unit configured to determine whether or not a moving object is included in a first background image that the registration unit registered at a first timing; anda replacing unit configured to, in a case where the determination unit determines that a moving object is included in the first background image, replace the first background image that the registration unit registered with a second background image registered at a second timing prior to the first timing.2. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the determination unit determines, based on a difference image for a difference between a sequentially inputted captured image and the second background image, whether or not a moving object is included in the first background image.3. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the replacing unit, in a case where the determination unit determines that a moving object is included in the first background image, replaces the first background image with the second background image which is most similar to a captured image inputted after the determination.4. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein in a case where the determination unit determines that a moving object is included in the first background image, and a similarity between a captured image inputted after the determination and the second background image does not exceed a predetermined value, the replacing unit replaces the first background image with that captured image.5. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the registration unit, in accordance with a frequency of the registrations, controls a number of the second background images to be registered.6. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, further comprising:a detection unit configured to, based on a difference between a sequentially inputted captured image and the first background image, determine whether or not an object is included in the captured image and detect an existence or absence of a removal in accordance with a result of the determination; anda notification unit configured to notify the result of detection by the detection unit.7. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 6, wherein the notification unit counts and notifies the number of the detection of a removal.8. An image capturing apparatus, comprising:an image capturing unit configured to obtain a captured image, andan 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus, comprising:a registration unit configured to register, as a background image, a captured image from among captured images sequentially inputted;a determination unit configured to determine whether or not a moving object is included in a first background image that the registration unit registered at a first timing; anda replacing unit configured to, in a case where the determination unit determines that a moving object is included in the first background image, replace the first background image that the registration unit registered with a second background image registered at a second timing prior to the first timing.9. An 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method that an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus performs, the method comprising:registering, as a background image, a captured image from among captured images sequentially inputted;determining whether or not a moving object is included in a first background image registered at a first timing; andin a case where it is determined that a moving object is included in the first background image, replacing the first background image with a second background image registered at a second timing prior to the first timing.10. A non-transitory computer-readable 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    storage medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 storing a 
<a href="https://en.wikipedia.org/wiki/Computer_program"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer program
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for causing a computer to function as:a registration unit configured to register, as a background image, a captured image from among captured images sequentially inputted;a determination unit configured to determine whether or not a moving object is included in a first background image that the registration unit registered at a first timing; anda replacing unit configured to, in a case where the determination unit determines that a moving object is included in the first background image, replace the first background image that the registration unit registered with a second background image registered at a second timing prior to the first timing._______________MOVING OBJECT DETECTION METHOD AND SYSTEM_____20181220_____XMLs/xml/ipa181220.xml_____US-20180365845-A1 : US-15737155 : CN-201610692267.0 : WO-PCT/CN2016/108836-00_____G06T0007254000 : G06T0007231000A moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method and a moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system are provided. The method includes: predetermining a background image corresponding to a scene monitored by a 
<a href="https://en.wikipedia.org/wiki/Closed-circuit_television"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video monitoring
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device; performing a subtraction processing on a grayscale image to be detected and the background image to acquire a difference image; and binarizing the difference image and determining a moving object in the grayscale image to be detected, where the determining the background image includes: dividing a first grayscale image frame and a second grayscale image frame in a grayscale image frame sequence captured by the video monitoring device into image blocks to acquire a first image block set and a second image block set respectively, and determining the background image using a difference between the first image block set and the second image block set._____d:This application claims priority to Chinese Patent Application No. 201610692267.0, titled “MOVING OBJECT DETECTION METHOD AND SYSTEM,” filed on Aug. 19, 2016 with the State 
<a href="https://en.wikipedia.org/wiki/Intellectual_property"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Intellectual Property
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 Office of People's Republic of China, which is incorporated herein by reference in its entirety.FIELDThe present disclosure relates to the technical field of monitoring picture processing, and in particular to a moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method and a moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system.BACKGROUNDWith rapid development of the 
<a href="https://en.wikipedia.org/wiki/Computing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer technology
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and continuous reduction of costs of various monitoring devices, 
<a href="https://en.wikipedia.org/wiki/Closed-circuit_television"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video monitoring
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 systems are extensively applied in fields of finance, transportation, military and the like. The technology for detecting and tracking a moving object in a 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 has been an important research subject in the 
<a href="https://en.wikipedia.org/wiki/Computer_vision"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer vision
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 field.In recent years, many scholars have proposed solutions for moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, such as the 
<a href="https://en.wikipedia.org/wiki/Mixture_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Gaussian mixture model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 algorithm (GMM), the 
<a href="https://en.wikipedia.org/wiki/Block_cipher"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    codebook algorithm
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 (Codebook), the visual 
<a href="https://en.wikipedia.org/wiki/Vibe"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 algorithm (Vibe) and the GMG algorithm. According to the 
<a href="https://en.wikipedia.org/wiki/Mixture_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Gaussian mixture model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 algorithm, multiple independent Gaussian distributions are established for each pixel, thus a moving object in a complex scene can be well extracted. However, this algorithm requires time for 
<a href="https://en.wikipedia.org/wiki/Junior_Samples"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    training samples
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. In addition, it is difficult to establish an effective background model using the 
<a href="https://en.wikipedia.org/wiki/Mixture_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Gaussian mixture model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 algorithm in a case that the lighting condition changes abruptly since parameters are fixed. According to the codebook algorithm, a 
<a href="https://en.wikipedia.org/wiki/Code-excited_linear_prediction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    codebook structure
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is established for each pixel, thereby providing a good real-time performance. However, a large amount of memory is occupied, and the algorithm is susceptible to subtle disturbances in the background. The visual 
<a href="https://en.wikipedia.org/wiki/Vibe"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 algorithm adopts a random sample model, thus a complete moving object can be rapidly extracted, and the algorithm has certain immunity to noises. However, the disadvantages of the algorithm includes that, sample values of the 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 are repeatedly selected, a fixed segmentation threshold cannot adapt to dynamic change of the background in a complex video scene, and noises caused by changes of the lighting cannot be effectively eliminated using the fixed updating factor. The GMG algorithm is a non-parametric method, which generates a time-varying background model using the Bayesian inference. The algorithm shows a poor performance in a lighting-varying scene.As can be seen, a process of detecting a moving object in a 
<a href="https://en.wikipedia.org/wiki/Closed-circuit_television"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video monitoring
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 picture according to the conventional technology is relatively cumbersome and a detection effect needs further improvement.In view of the above, problems to be solved include how to improve the effect of moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and how to reduce the complexity of the detection process.SUMMARYIn view of this, the purpose of the present disclosure is to provide a moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method and a moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system, with which a effect of moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 can be improved and the complexity of a detection process can be reduced. The solution is as follows.A moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method is provided, which includes:predetermining a background image corresponding to a scene monitored by a 
<a href="https://en.wikipedia.org/wiki/Closed-circuit_television"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video monitoring
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device;performing a subtraction processing on a grayscale image to be detected and the background image to acquire a difference image; andbinarizing the difference image and determining a moving object in the grayscale image to be detected,where the determining the background image includes:dividing a first grayscale image frame and a second grayscale image frame in a grayscale image frame sequence captured by the video monitoring device into image blocks to acquire a first image block set and a second image block set respectively, anddetermining the background image using a difference between the first image block set and the second image block set.Preferably, a time instant at which the grayscale image to be detected is captured may be greater than or equal to a time instant at which the first grayscale image frame is captured, the time instant at which the first grayscale image frame is captured may be greater than a time instant at which the second grayscale image frame is captured, and there may be N grayscale image frames between the first grayscale image frame and the second grayscale image frame, where N is a positive integer.Preferably, the dividing the first grayscale image frame and the second grayscale image frame in the grayscale image frame sequence captured by the video monitoring device into image blocks may include:dividing the first grayscale image frame into K image blocks which do not overlap with each other and include all pixels in the first grayscale image frame, to acquire the first image block set, anddividing the second grayscale image frame into K image blocks which do not overlap with each other and include all pixels in the second grayscale image frame, to acquire the second image block set, whereK is a positive integer, the image blocks have the same size, and the K image blocks in the first image block set have a one-to-one correspondence with the K image blocks in the second image block set.Preferably, the determining the background image using the difference between the first image block set and the second image block set may include:calculating the difference between the first image block set and the second image block set; andacquiring the background image using the difference and the first image block set.Preferably, the calculating the difference between the first image block set and the second image block set may include:calculating an image block grayscale difference between each image block in the first image block set and a corresponding image block in the second image block set, to acquire an image block grayscale difference set, where an i-th element in the image block grayscale difference set is calculated according to an equation:di=|gt,i−gt−N,i|,where gt,i represents an i-th image block in the first image block set, which is represented by gt, gt−N,i represents an i-th image block in the second image block set, which is represented by gt−N, and di represents an image block grayscale difference between the i-th image block in the first image block set and the i-th image block in the second image block set, where i=1, 2, . . . , K; andcalculating a difference between each image block in the first image block set and a corresponding image block in the second image block set using the image block grayscale difference set to acquire a difference set, where an i-th element in the difference set is calculated according to an equation:si=∑1≤x≤n,1≤y≤m
<a href="https://en.wikipedia.org/wiki/Closed-circuit_television"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    

    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
<a href="https://en.wikipedia.org/wiki/Closed-circuit_television"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    

    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
di(x,y),where n represents the length of each image block, m represents the width of each image block, both n and m are in units of pixels, di(x,y) represents a grayscale difference in di corresponding to an (x,y)-th pixel in the image block, and si represents a difference between an i-th image block in the first image block set and an i-th image block in the second image block set.Preferably, the acquiring the background image using the difference and the first image block set may include:converting the difference set to a determination set using a preset conversion equation:wi={1,si≤TH10,else,where TH1 represents a preset difference threshold, and wi represents an i-th determination element in the determination set; andinputting the determination set and the first image block set into a preset first background image construction equation to acquire the background image, where the first background image construction equation is expressed as:bi={gt,i,wi=1bi,wi=0,where bi represents an i-th image block in the background image, and bi′ represents an i-th image block in a previous background image.Preferably, the acquiring the background image using the difference and the first image block set may include:inputting the difference set and the first image block set into a preset second background image construction equation to acquire the background image, where the second background image construction equation is expressed as:bi={gt,i,si≤TH1bi′,else,where TH1 represents a preset difference threshold, bi represents an i-th image block in the background image, and bi′ represents an i-th image block in a previous background image.Preferably, in a case that the previous background image is a first background image, the first background image may be an image in which grayscale values of all pixels are 0.Preferably, the binarizing the difference image and determining the moving object in the grayscale image to be detected may include:binarizing the difference image using a preset 
<a href="https://en.wikipedia.org/wiki/Thresholding_(image_processing)"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    binarization processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 equation to acquire a binarized image, where the 
<a href="https://en.wikipedia.org/wiki/Thresholding_(image_processing)"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    binarization processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 equation is expressed as:F′(p,q)={0,F(p,q)≤TH2255,else,where TH2 represents a preset grayscale threshold, F(p,q) represents a grayscale value corresponding to a (p,q)-th pixel in the difference image, which is represented by F, and F′(p,q) represents a grayscale value corresponding to a (p,q)-th pixel in the binarized image; andextracting pixels of which grayscale values are 255 from the binarized image to acquire the moving object in the grayscale image to be detected.A moving object detection system is further provided according to the present disclosure, which includes:a background image determination module, configured to predetermine a background image corresponding to a scene monitored by a 
<a href="https://en.wikipedia.org/wiki/Closed-circuit_television"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video monitoring
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device;a difference image acquisition module, configured to perform a subtraction processing on a grayscale image to be detected and the background image to acquire a difference image; anda moving object determination module, configured to binarize the difference image and determine a moving object in the grayscale image to be detected,where the background image determination module being configured to determine the background image includes:the background image determination module being configured todivide a first grayscale image frame and a second grayscale image frame in a grayscale image frame sequence captured by the video monitoring device into image blocks to acquire a first image block set and a second image block set respectively, anddetermine the background image using a difference between the first image block set and the second image block set.In the present disclosure, two grayscale image frames in the captured grayscale image frame sequence are divided into image blocks in advance, and the background image is determined using the difference between two image block sets acquired by division. As can be seen, the background image is determined based on the difference between the image blocks according to the present disclosure. As compared with the scheme of determining the background image based on the difference between pixels, the 
<a href="https://en.wikipedia.org/wiki/Data_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    data processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 amount is greatly reduced according to the present disclosure. Moreover, according to the present disclosure, the image frames for constructing the background image are grayscale image frames. Therefore, the overall 
<a href="https://en.wikipedia.org/wiki/Data_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    data processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 amount for determining the background image is small, since grayscale images contain less information, which is beneficial for reducing the complexity of moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. Further, after the background image is determined, according to the present disclosure, the subtraction processing is performed on a grayscale image to be detected and the above background image to acquire a difference image, and the difference image is binarized, so as to determine a moving object in the above grayscale image to be detected. As can be seen, according to the present disclosure, during construction of the background image, calculation of the difference image and binarization, all the processed data is grayscale data, thus the capability of the moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 process according to the present disclosure of rejecting external interference factors can be improved, thereby improving the effect of moving object detection. In summary, the effect of the moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is improved and the complexity of the detection process is reduced according to the present disclosure.BRIEF DESCRIPTION OF THE DRAWINGSFIG. 1 is a flow chart of a moving object detection method according to an embodiment of the present disclosure; andFIG. 2 is schematic structural diagram of a moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system according to an embodiment of the present disclosure._____c:1. A moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method, comprising:predetermining a background image corresponding to a scene monitored by a 
<a href="https://en.wikipedia.org/wiki/Closed-circuit_television"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video monitoring
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device;performing a subtraction processing on a grayscale image to be detected and the background image to acquire a difference image; andbinarizing the difference image and determining a moving object in the grayscale image to be detected,wherein the determining the background image comprises:dividing a first grayscale image frame and a second grayscale image frame in a grayscale image frame sequence captured by the video monitoring device into image blocks to acquire a first image block set and a second image block set respectively, anddetermining the background image using a difference between the first image block set and the second image block set.2. The moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method according to claim 1, wherein a time instant at which the grayscale image to be detected is captured is greater than or equal to a time instant at which the first grayscale image frame is captured, the time instant at which the first grayscale image frame is captured is greater than a time instant at which the second grayscale image frame is captured, and there are N grayscale image frames between the first grayscale image frame and the second grayscale image frame, wherein N is a positive integer.3. The moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method according to claim 2, wherein the dividing the first grayscale image frame and the second grayscale image frame in the grayscale image frame sequence captured by the video monitoring device into image blocks comprises:dividing the first grayscale image frame into K image blocks which do not overlap with each other and include all pixels in the first grayscale image frame, to acquire the first image block set, anddividing the second grayscale image frame into K image blocks which do not overlap with each other and include all pixels in the second grayscale image frame, to acquire the second image block set, whereinK is a positive integer, the image blocks have the same size, and the K image blocks in the first image block set have a one-to-one correspondence with the K image blocks in the second image block set.4. The moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method according to claim 3, wherein the determining the background image using the difference between the first image block set and the second image block set comprises:calculating the difference between the first image block set and the second image block set; andacquiring the background image using the difference and the first image block set.5. The moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method according to claim 4, wherein the calculating the difference between the first image block set and the second image block set comprises:calculating an image block grayscale difference between each image block in the first image block set and a corresponding image block in the second image block set, to acquire an image block grayscale difference set, wherein an i-th element in the image block grayscale difference set is calculated according to an equation:wherein gt,i represents an i-th image block in the first image block set, which is represented by gt, gt−N,i represents an i-th image block in the second image block set, which is represented by gt−N, and di represents an image block grayscale difference between the i-th image block in the first image block set and the i-th image block in the second image block set, wherein i=1, 2, . . . , K; andcalculating a difference between each image block in the first image block set and a corresponding image block in the second image block set using the image block grayscale difference set to acquire a difference set, wherein an i-th element in the difference set is calculated according to an equation:wherein n represents the length of each image block, m represents the width of each image block, both n and m are in units of pixels, di(x,y) represents a grayscale difference in di corresponding to an (x,y)-th pixel in the image block, and si represents a difference between an i-th image block in the first image block set and an i-th image block in the second image block set.6. The moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method according to claim 5, wherein the acquiring the background image using the difference and the first image block set comprises:converting the difference set to a determination set using a preset conversion equation:wherein TH1 represents a preset difference threshold, and wi represents an i-th determination element in the determination set; andinputting the determination set and the first image block set into a preset first background image construction equation to acquire the background image, wherein the first background image construction equation is expressed as:wherein bi represents an i-th image block in the background image, and bi′ represents an i-th image block in a previous background image.7. The moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method according to claim 5, wherein the acquiring the background image using the difference and the first image block set comprises:inputting the difference set and the first image block set into a preset second background image construction equation to acquire the background image, wherein the second background image construction equation is expressed as:wherein TH1 represents a preset difference threshold, bi represents an i-th image block in the background image, and bi′ represents an i-th image block in a previous background image.8. The moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method according to claim 6, wherein in a case that the previous background image is a first background image, the first background image is an image in which grayscale values of all pixels are 0.9. The moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method according to claim 6, wherein the binarizing the difference image and determining the moving object in the grayscale image to be detected comprises:binarizing the difference image using a preset 
<a href="https://en.wikipedia.org/wiki/Thresholding_(image_processing)"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    binarization processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 equation to acquire a binarized image, wherein the 
<a href="https://en.wikipedia.org/wiki/Thresholding_(image_processing)"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    binarization processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 equation is expressed as:wherein TH2 represents a preset grayscale threshold, F(p,q) represents a grayscale value corresponding to a (p,q)-th pixel in the difference image, which is represented by F, and F′(p,q) represents a grayscale value corresponding to a (p,q)-th pixel in the binarized image; andextracting pixels of which grayscale values are 255 from the binarized image to acquire the moving object in the grayscale image to be detected.10. A moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system, comprising:a background image determination module, configured to predetermine a background image corresponding to a scene monitored by a 
<a href="https://en.wikipedia.org/wiki/Closed-circuit_television"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video monitoring
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device;a difference image acquisition module, configured to perform a subtraction processing on a grayscale image to be detected and the background image to acquire a difference image; anda moving object determination module, configured to binarize the difference image and determine a moving object in the grayscale image to be detected,wherein the background image determination module being configured to determine the background image comprises:the background image determination module being configured todivide a first grayscale image frame and a second grayscale image frame in a grayscale image frame sequence captured by the video monitoring device into image blocks to acquire a first image block set and a second image block set respectively, anddetermine the background image using a difference between the first image block set and the second image block set.11. The moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method according to claim 7, wherein in a case that the previous background image is a first background image, the first background image is an image in which grayscale values of all pixels are 0.12. The moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method according to claim 7, wherein the binarizing the difference image and determining the moving object in the grayscale image to be detected comprises:binarizing the difference image using a preset 
<a href="https://en.wikipedia.org/wiki/Thresholding_(image_processing)"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    binarization processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 equation to acquire a binarized image, wherein the 
<a href="https://en.wikipedia.org/wiki/Thresholding_(image_processing)"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    binarization processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 equation is expressed as:wherein TH2 represents a preset grayscale threshold, F(p,q) represents a grayscale value corresponding to a (p,q)-th pixel in the difference image, which is represented by F, and F′(p,q) represents a grayscale value corresponding to a (p,q)-th pixel in the binarized image; andextracting pixels of which grayscale values are 255 from the binarized image to acquire the moving object in the grayscale image to be detected._______________OBJECT TRACKING SYSTEM AND METHOD THEREOF_____20191205_____XMLs/xml/ipa191205.xml_____US-20190370984-A1 : US-16241613 : TW-107118678_____G06T0007254000 : G06K0009000000 : G06T0007194000 : G06T0007130000 : G06T0005000000 : G06T0005500000An 
<a href="https://en.wikipedia.org/wiki/Tracking_system"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 includes a foreground identifying module, an object grouping module, and an 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 module. The foreground identifying module determines an attribute information of each pixel position of a current processing frame according to a difference between a pixel value of each pixel position of the current processing frame and that of a 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 frame, so as to generate a current foreground frame. The object grouping module sets a label to each pixel position according to the attribute information of surrounding pixel positions of each pixel position, and connects adjacent pixel positions with the same label to form an object. The 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 module determines the object being a tracked object according to whether the pixel positions corresponding to the object are at least partially overlapped with the pixel positions corresponding to the tracked object._____d:CROSS REFERENCE TO RELATED APPLICATIONThis application claims the benefits of the Taiwan Patent Application Serial Number 107118678, filed on May 31, 2018, the subject matter of which is incorporated herein by reference.BACKGROUND OF THE INVENTION1. Field of the InventionThe present invention relates to an 
<a href="https://en.wikipedia.org/wiki/Image_processor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and a method thereof, and more particularly to an 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system and a method thereof.2. Description of Related ArtObject tracking has been a popular research subject in the field of 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for a long time. In practical 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 tasks, the tracking accuracy and subsequent operations are highly dependent on occlusion among different objects and appearance change of objects, making object tracking a technically challenging work. For the same reason, most existing tracking technologies are somehow incompetent. For example, the algorithm-based solutions are too complicated to be used in real-time 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and their use must be confined in a simple monitoring environment, leaving much room for improvement. Besides, most known tracking solutions require considerable labor in pre-processing and/or post-processing operations, such as that for building a robust background model in advance, resulting in unsatisfactory labor and time costs.In view of this, the present invention provides an 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system and a method thereof for addressing the foregoing problems and being applied in any real-world environment.SUMMARY OF THE INVENTIONOne objective of the present invention is to provide an 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system, which comprises: a foreground identifying module, determining an attribute information of each pixel position of a current processing frame of a 
<a href="https://en.wikipedia.org/wiki/Fatal_Frame"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    frame series
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to a difference between a pixel value of each pixel position of the current processing frame and that of a 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 frame, so as to generate a current foreground frame, wherein the attribute information is of a foreground attribute or a background attribute; an object grouping module, setting a label to each said pixel position according to the attribute information of a plurality of surrounding pixel positions of each said pixel position of the current 
<a href="https://en.wikipedia.org/wiki/Foreground_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    foreground frame
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and connecting a plurality of adjacent said pixel positions with the same label to form an object; and an 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 module, determining the object is a specific object according to whether the pixel positions corresponding to the object in the current foreground frame are at least partially overlapped with the pixel positions corresponding to the specific object in a previous foreground frame of the 
<a href="https://en.wikipedia.org/wiki/Fatal_Frame"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    frame series
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and converting the label corresponding to the object into the label of the specific object.Another objective of the present invention is to provide an object tracking method, which is configured to be performed by an 
<a href="https://en.wikipedia.org/wiki/Tracking_system"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and comprises steps of: making a foreground identifying module determine an attribute information of each pixel position of a current processing frame according to a difference between a pixel value of each pixel position of the current processing frame and that of a 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 frame, so as to generate a current foregro
<a href="https://en.wikipedia.org/wiki/Frame_story"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    und frame
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, wherein the attribute information is of a foreground attribute or a background attribute; making an object grouping module set a label to each said pixel position according to the attribute information of a plurality of surrounding pixel positions of each said pixel position, and connect a plurality of adjacent said pixel positions with the same label to form an object; and making an 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 module determine the object is a specific object according to whether the pixel positions corresponding to the object in the current foreground frame are at least partially overlapped with the pixel positions corresponding to the specific object in a previous foreground frame, and convert the label corresponding to the object into the label of the specific object.Other objects, advantages, and novel features of the invention will become more apparent from the following detailed description when taken in conjunction with the accompanying drawings.BRIEF DESCRIPTION OF THE DRAWINGSFIG. 1(A) is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of an 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system according to one embodiment of the present invention;FIG. 1(B) is an overall flowchart of an 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method according to one embodiment of the present invention;FIG. 2(A) is a detailed 
<a href="https://en.wikipedia.org/wiki/Piping_and_instrumentation_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    operational diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a foreground identifying module according to one embodiment of the present invention;FIG. 2(B) is a detailed 
<a href="https://en.wikipedia.org/wiki/Piping_and_instrumentation_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    operational diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a foreground identifying module according to another embodiment of the present invention;FIG. 3(A) is a detailed 
<a href="https://en.wikipedia.org/wiki/Piping_and_instrumentation_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    operational diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of an object grouping module according to one embodiment of the present invention;FIG. 3(B) illustrates the operation of step S31 of FIG. 3(A) according to one embodiment of the present invention;FIG. 3(C) illustrates the operation of step S32(a) of FIG. 3(A) according to one embodiment of the present invention;FIG. 3(D) illustrates the operation of step S32(b) of FIG. 3(A) according to one embodiment of the present invention;FIG. 3(E) illustrates the operation of step S32(c) of FIG. 3(A) according to one embodiment of the present invention;FIG. 3(F) illustrates the operation of step S33 of FIG. 3(A) according to one embodiment of the present invention;FIG. 3(G) illustrates the operation of step S34 of FIG. 3(A) according to one embodiment of the present invention;FIGS. 3(H) to 3(J) illustrate the operation of step S35 of FIG. 3(A) according to one embodiment of the present invention;FIG. 4 is an 
<a href="https://en.wikipedia.org/wiki/Piping_and_instrumentation_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    operational diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of an 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 module according to one embodiment of the present invention; andFIG. 5 is an 
<a href="https://en.wikipedia.org/wiki/Piping_and_instrumentation_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    operational diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of an object occlusion resolving module according to one embodiment of the present invention._____c:1. An 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system, comprising:a foreground identifying module, determining an attribute information of each pixel position of a current processing frame of a 
<a href="https://en.wikipedia.org/wiki/Fatal_Frame"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    frame series
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to a difference between a pixel value of each pixel position of the 
<a href="https://en.wikipedia.org/wiki/Word_processor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    current processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 frame and that of a 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 frame, so as to generate a current foreground frame, wherein the attribute information is of a foreground attribute or a background attribute;an object grouping module, setting a label to each said pixel position according to the attribute information of a plurality of surrounding pixel positions of each said pixel position, and connecting a plurality of adjacent said pixel positions with the same label to form at least one object; andan 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 module, determining the at least one object being a specific object according to whether the pixel positions corresponding to the at least one object in the current foreground frame are at least partially overlapped with the pixel positions corresponding to the specific object in a previous foreground frame of the 
<a href="https://en.wikipedia.org/wiki/Fatal_Frame"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    frame series
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and converting the label corresponding to the at least one object into the label of the specific object.2. The 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system according to claim 1, wherein the foreground identifying module further uses a Gaussian smoothing matrix to perform a smoothing operation on the difference between the pixel value of each pixel position of the current processing frame and that of the background model frame.3. The 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system according to claim 1, wherein when a pixel position of the current processing frame is of the background attribute, the foreground identifying module performs a filtering operation to update the pixel value of a corresponding pixel position of the background model frame.4. The 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system according to claim 1, wherein the foreground identifying module further generates a successive frame difference information of each pixel position according to a difference between the pixel value of each pixel position of the current processing frame and that of a previous frame.5. The 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system according to claim 4, wherein when a pixel position of the current processing frame is of the foreground attribute, the foreground identifying module further generates a foreground accumulated time information of the pixel position according to the successive frame difference information and a pixel value hold time information of the pixel position, and determines whether the pixel position has to be changed to the background attribute according to whether the foreground accumulated time information is greater than a foreground lasting time threshold.6. The 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system according to claim 4, wherein when a pixel position of the current processing frame is of the foreground attribute, the foreground identifying module further compares the pixel value of the pixel position with those of corresponding pixel positions in a plurality of background samples, and determines whether the pixel position has to be changed to the background attribute according to whether a match degree between the pixel position and the corresponding pixel positions in the background samples is greater than a preset threshold.7. The 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system according to claim 1, wherein when a pixel position in the current foreground frame is of the foreground attribute, and the plurality of surrounding pixel positions of the pixel position in the current foreground frame are all of the background attribute, the object grouping module sets a minimal label that has not been used to the pixel position; when a pixel position in the current foreground frame is of the foreground attribute, and the plurality of surrounding pixel positions of the pixel position are all of the foreground attribute and have the same label, the object grouping module sets the same label to the pixel position; when a pixel position in the current foreground frame is of the foreground attribute, and the plurality of surrounding pixel positions of the pixel position are all of the foreground attribute and have at least two different labels, the object grouping module sets a minimal one between the at least two labels to the pixel position.8. The 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system according to claim 1, wherein the object grouping module further determines whether two objects have to be combined according to a boundary information of the two objects in the current foreground frame.9. The 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system according to claim 1, further comprising an object occlusion resolving module, which determines whether there is object occlusion in the current foreground frame according to a moving trajectory and an edge feature of at least one specific object in the 
<a href="https://en.wikipedia.org/wiki/Fatal_Frame"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    frame series
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.10. The 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system according to claim 9, wherein the object occlusion resolving module splits at least two objects that are of a staggered case object in the current foreground frame according to the moving trajectory, the edge feature and an average area of the at least one specific object.11. An 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method, which is performed by an 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system, comprising the steps of:using a foreground identifying module to determine an attribute information of each pixel position of a current processing frame of a 
<a href="https://en.wikipedia.org/wiki/Fatal_Frame"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    frame series
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to a difference between a pixel value of each pixel position of the current processing frame and that of a 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 frame, so as to generate a current foreground frame, wherein the attribute information is of a foreground attribute or a background attribute;using an object grouping module to set a label to each said pixel position according to the attribute information of a plurality of surrounding pixel positions of each said pixel position, and connect a plurality of adjacent said pixel positions with the same label to form at least one object; andusing an 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 module to determine the at least one object being a specific object according to whether the pixel positions corresponding to the at least one object in the current foreground frame are at least partially overlapped with the pixel positions corresponding to the specific object in a previous foreground frame, and convert the label corresponding to the at least one object into the label of the specific object.12. The object tracking method according to claim 11, further comprising the step of: enabling the foreground identifying module to use a Gaussian smoothing matrix to perform a smoothing operation on the difference between the pixel value of each pixel position of the current processing frame and that of the background model frame.13. The object tracking method according to claim 11, further comprising the step of: when a pixel position of the current processing frame is of the background attribute, using the foreground identifying module to perform a filtering operation to update the pixel value of a corresponding pixel position of the background model frame.14. The object tracking method according to claim 11, further comprising the step of: using the foreground identifying module to generate a successive frame difference information of each pixel position according to a difference between the pixel value of each pixel position of the current processing frame and that of a previous frame.15. The object tracking method according to claim 14, further comprising the step of: when a pixel position of the current processing frame is of the foreground attribute, using the foreground identifying module to generate a foreground accumulated time information of the pixel position according to the successive frame difference information and a pixel value hold time information of the pixel position, and determine whether the pixel position has to be changed to the background attribute according to whether the foreground accumulated time information is greater than a foreground lasting time threshold.16. The object tracking method according to claim 14, further comprising the step of: when a pixel position of the current processing frame is of the foreground attribute, using the foreground identifying module to compare the pixel value of the pixel position with those of corresponding pixel positions in a plurality of background samples, and determine whether the pixel position has to be changed to the background attribute according to whether a match degree between the pixel position and the corresponding pixel positions in the background samples is greater than a preset threshold.17. The object tracking method according to claim 11, further comprising the step of: when a pixel position in the current foreground frame is of the foreground attribute, and the plurality of surrounding pixel positions of the pixel position in the current foreground frame are all of the background attribute, using the object grouping module to set a minimal label that has not been used to the pixel position; when a pixel position in the current foreground frame is of the foreground attribute, and the plurality of surrounding pixel positions of the pixel position are all of the foreground attribute and have the same label, using the object grouping module to set the same label to the pixel position; and when a pixel position in the current foreground frame is of the foreground attribute, and the plurality of surrounding pixel positions of the pixel position are all of the foreground attribute and have at least two different labels, using the object grouping module to set a minimal one between the at least two labels to the pixel position.18. The object tracking method according to claim 11, further comprising the step of: using the object grouping module to determine whether two objects have to be combined according to a boundary information of the two objects in the current foreground frame.19. The object tracking method according to claim 11, further comprising the step of: using an object occlusion resolving module to determine whether there is object occlusion in the current foreground frame according to a moving trajectory and an edge feature of at least one specific object in the 
<a href="https://en.wikipedia.org/wiki/Fatal_Frame"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    frame series
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.20. The object tracking method according to claim 19, further comprising the step of: using the object occlusion resolving module to split at least two objects that are of a staggered case object in the current foreground frame according to the moving trajectory, the edge feature and an average area of the at least one specific object._______________MEASURING A PROPERTY OF A TRAJECTORY OF A BALL_____20190829_____XMLs/xml/ipa190829.xml_____US-20190266735-A1 : US-16348715 : AU-2016904594 : WO-PCT/AU2017/051236-00_____G06T0007254000A method for determining whether a goal is achieved by a trajectory of a ball using a 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 comprises capturing a sequence of video frames of the ball with a camera of the 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
; detecting the ball in at least three of the video frames; computing a trajectory of the ball using the detections of the ball; detecting a goal image in at least one of the video frames; computing whether the trajectory of the ball achieves intersection or similar with a goal plane computed from the goal im—age according to a goal criterion._____d:FIELD OF THE INVENTIONThe present invention relates to a method of measuring a property of a trajectory of a ball with a 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.BACKGROUND
<a href="https://en.wikipedia.org/wiki/Mobile_computing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Mobile computer
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 devices, including smartphones, 
<a href="https://en.wikipedia.org/wiki/Tablet_computer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    tablet computers
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the like are widely used. Most people now own a 
<a href="https://en.wikipedia.org/wiki/Smartphone"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    smart phone
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. Commonly, these 
<a href="https://en.wikipedia.org/wiki/Mobile_computing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 devices have an in-built camera and can be loaded with customised software, commonly referred to as an App.Sports are a common pass time and a source of competition between players, not just during the playing of a sports game, but on how well a player's skills compare with another player. Association football, also known as soccer, is a type of football widely played around the world. Players of soccer, even on the same team, will often be competitive about how accurate they can kick the soccer ball, how fast they can kick or how well they can deceive a goalkeeper into thinking they are kicking to one side of the goals, but in actuality kick to the other side of the goals.The present invention relates to ball games such as soccer, tennis, table tennis, basketball, baseball or golf, and using a 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to determine a property of a kicked, batted or thrown ball. The present invention also relates to use of the 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to determine if a goal is achieved.US Patent Application 20140300733 focuses on detecting the speed of the ball from a side view. While useful for some sports, the side view tracking of a ball is not useful in others.US Patent Application 20140300745 again focuses on detecting the speed of the ball between two time spaced frames, by measuring distance travelled by the ball over the time to determine the speed.U.S. Pat. No. 9,275,470 tracks a ball by tracking the image on the ball.US Patent Application 20140301598 focuses on tracking a ball based on detecting the ball in 2D pixel space, converting the detected ball into 3D space and then determining a characteristic of the ball. However, this does not account for false positive detections of the ball.A reference to a 
<a href="https://en.wikipedia.org/wiki/Prior_art"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    prior art
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 document is not intended to be an admission that such a 
<a href="https://en.wikipedia.org/wiki/Prior_art"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    prior art
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 document forms part of the common general knowledge of a person skilled in the art of the invention in any jurisdiction.In this specification the terms “comprising” or “comprises” are used inclusively and not exclusively or exhaustively.SUMMARY OF THE INVENTIONAccording to an aspect of the present invention there is provided a method for determining a property of a trajectory of a ball with a 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, comprising:capturing a sequence of video frames of the ball with a camera of the 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
;finding candidates of the ball in at least three of the video frames; eliminating false positive detections of the ball, comprising fitting ball candidates to a curve trajectory in which ball candidates not sufficiently representing the curve trajectory are eliminated as ball candidates;computing a property of the trajectory of the ball using travel of the ball through the curve trajectory.In an embodiment the eliminating step comprises eliminating outlier candidates of the ball, leaving remaining candidate balls to which the fitting occurs. In an embodiment eliminating outlier candidate balls comprises conducting a random sample consensus iterative analysis across the at least three frames to eliminate unlikely candidates of the position of the ball in the frames. In an embodiment the eliminating step comprises eliminating ball candidates that do not have an appropriate sequential change in the size of a circle fitted to the perimeter of the found ball candidates.In an embodiment the curve is fitted to candidates of the ball for video frames in which the ball is determined to be moving.In an embodiment the method further comprises identifying a reference object in the video frames. Preferably the 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is stationary, so that the reference object is stationary in the video images. In an embodiment the reference object is a soccer goal. In an embodiment the type of reference object is predetermined according to the type of sport in which the ball is used. For example, if a soccer goal is identified then a soccer ball is sought to be identified, however if a basketball ring is identified then a basketball is sought to be identified.In an embodiment the fitting step comprises finding a centre of each candidate ball as a 2-dimensional coordinate in pixel space.In an embodiment the fitting step comprises converting the centre of each candidate ball into a 3 dimensional coordinate in space.In an embodiment the fitting step comprises fitting a quadratic curve to at least one cardinal plane through the 3 dimensional 
<a href="https://en.wikipedia.org/wiki/Vector_space"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    coordinate space
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. In an embodiment the fitting step comprises fitting a quadratic curve to each of the three cardinal planes through the 3 dimensional coordinate space.In an embodiment the fitting of the trajectory curve comprises constraining movement of the ball through the vertical planes according to acceleration due to gravity.In an embodiment the camera is positioned substantially rearward of the direction of travel of the ball.In an embodiment finding ball candidates comprises converting each of the at least three video frames into greyscale. In an embodiment finding ball candidates comprises determining a difference image between consecutive ones of the at least three video frames. In an embodiment finding ball candidates comprises converting the difference images into a binary image. In an embodiment finding ball candidates comprises applying a function to remove groups of adjacent pixels in each of the at least three video frames that are too small. In an embodiment finding ball candidates comprises applying a function to remove groups of adjacent pixels in each of the at least three video frames that are too big. In an embodiment finding ball candidates comprises calculating a gradient vector field. In an embodiment finding ball candidates comprises shape matching. In an embodiment the shape matching comprises checking that the gradient direction in the gradient vector field is a smooth circle. In an embodiment the shape matching comprises applying a Hough transform.In an embodiment finding ball candidates comprises eliminating ball candidates outside of an area of interest.In an embodiment the method further comprises computing whether the trajectory of the ball achieves intersection or similar with a goal image in the video frames according to a goal criterion.In an embodiment the property computed comprises one or more of:The speed of the ballThe force with which the ball was kickedThe angle at which the ball was kickedThe flight time and distance of the ballThe spin asserted on the ballAccording to an aspect of the present invention there is provided a method for determining whether a goal is achieved by a trajectory of a ball using a 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, comprising:capturing a sequence of video frames of the ball with a camera of the 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
;detecting the ball in at least three of the video frames;computing a trajectory of the ball using the detections of the ball; detecting a goal image in at least one of the video frames;computing whether the trajectory of the ball achieves intersection or similar with a goal plane computed from the goal image according to a goal criterion.In an embodiment detecting the goal image comprises determining the position of a goal in the at least one of the video images. In an embodiment computing the goal plane comprises determining a plane in 3 
<a href="https://en.wikipedia.org/wiki/Dimensional_analysis"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    dimensional space
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 which coincides with the position of the goal in the at least one of the video images. In an embodiment detecting the goal image comprises defining a shape of the goal in the determined plane. In an embodiment computing the trajectory of the ball comprises fitting a curve to the detected ball in the at least three video frames. In an embodiment computing whether the trajectory of the ball achieves intersection or similar with the goal image comprises computing whether the curve fitted to the trajectory of the ball intersects with the inside of the defined shape.In an embodiment the goal criterion comprises one or more of achieving a minimum speed of the ball; achieving placement of the ball in a particular position in the goal image; achieving a particular spin on the ball.In an embodiment the goal criterion comprises a difficulty input.In an embodiment the goal criterion determines whether the kick of the ball would be a goal or not according to the difficulty input.In an embodiment the goal plane is mapped to a probability density.In an embodiment the probability density is determined by characteristics assigned to a virtual goalkeeper. A 
<a href="https://en.wikipedia.org/wiki/Thibaut_Courtois"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    virtual goalkeeper
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may be a generic goalkeeper, or a personality goalkeeper. In an embodiment the characteristics comprise height, arm span, speed and reaction time. In an embodiment the goal plane is mapped to a probability density according to an equation of a curve derived from the characteristics of height and arm span.In an embodiment a probability is selected form the probability density according to the placement of the ball. Placement is determined by position of crossing the goal plane. In an embodiment the selected probability density is modified according to characteristics of the kick, such as for example 
<a href="https://en.wikipedia.org/wiki/Speed-ball"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    ball speed
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and path deviation. Path deviation is determined by ball spin.The method may be performed on a 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, such as a smartphone, 
<a href="https://en.wikipedia.org/wiki/Tablet_computer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    tablet computer
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 or 
<a href="https://en.wikipedia.org/wiki/Windows_Media_Player"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile media player
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.The method steps can, for example, be performed by a 
<a href="https://en.wikipedia.org/wiki/Computer_program"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer program
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 (such as an app) installed on the 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. Alternatively, the 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may be equipped with the claimed functionality.The method can use the camera of the 
<a href="https://en.wikipedia.org/wiki/Mobile_computing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device to take a sequence of video frames of, for example, a kick of a soccer ball. It is contemplated to use the method with other ball sports such as tennis, golf, table tennis, football, basketball, baseball and the like. Also, instead of a kick, the ball may be batted, for example, as a tennis ball is hit; or thrown, for example, as a basketball is thrown.The sequence of video frames may be taken directly by use of the app, which performs the method steps. Alternatively the sequence may be taken with a default video application of the 
<a href="https://en.wikipedia.org/wiki/Mobile_computing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device, stored in a memory of the mobile 
<a href="https://en.wikipedia.org/wiki/Computer_hardware"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and subsequently provided to an app for analysis.One or more parts of the finding, fitting, detecting, or computing steps may be performed on a 
<a href="https://en.wikipedia.org/wiki/Computer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 remote from the 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 under the control of the 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.Some of the ball detection techniques described in US 20140300745 and/or US 20140301598 may be used, the contents of both is incorporated herein by reference.According to a further aspect of the present invention there is a 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 configured to perform a method described herein.According to a still further aspect of the present invention there is a 
<a href="https://en.wikipedia.org/wiki/Computer_program"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer program
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 which comprises instructions for controlling a processor and a camera of a 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to perform a method described herein.BRIEF DESCRIPTION OF THE DRAWINGSEmbodiments of the present invention are described in the following detailed description by example only, with reference to the following drawings:FIG. 1: is a front view of an example 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 on which embodiments of the present invention may be implemented.FIG. 2: is a side view of the 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of FIG. 1.FIG. 3: is a schematic representation of the beginning of recording a sequence of video frames of a kick to a ball towards a soccer goal.FIG. 4: is a schematic 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
<a href="https://en.wikipedia.org/wiki/Component"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    of components
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of FIG. 1.FIG. 5: is a schematic 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of an application for execution on the 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.FIG. 6: is a flow chart of a method of an embodiment of the present invention.FIG. 7: is an example of a screen view on the 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of FIG. 1 in which the ball is identified before being kicked.FIG. 8: is an example of a screen view on the 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of FIG. 1 in which the ball has been kicked showing a schematic representation of detected instances of the trajectory of the ball.FIG. 9: is an example of a screen view on the 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of FIG. 1 in which the trajectory of FIG. 8 is shown along with a curve fitted to the detected trajectory of the ball.FIG. 10: is a schematic representation of a computer generated recreation of the kick of the ball.FIG. 11: is a 
<a href="https://en.wikipedia.org/wiki/Schematic"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    schematic diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a first type of probability distribution applied to a goal area.FIG. 12: is a 
<a href="https://en.wikipedia.org/wiki/Schematic"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    schematic diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a second type of probability distribution applied to a goal area.FIG. 13: is a 
<a href="https://en.wikipedia.org/wiki/Schematic"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    schematic diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 depicting defeating an obstacle (in this example a player wall).FIG. 14: is a flow chart of a method of determining whether a goal criteria is met._____c:1. A method for determining whether a goal is achieved by a trajectory of a ball using a 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, comprising:capturing a sequence of video frames of the ball with a camera of the 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
;detecting the ball in at least three of the video frames;computing a trajectory of the ball using the detections of the ball;detecting a goal structure image in at least one of the video frames; andcomputing whether the trajectory of the ball achieves intersection or similar with a goal plane computed from the goal structure image according to a goal criterion.2. A method according to claim 1, wherein detecting the goal image comprises determining the position of a goal in the at least one of the video images.3. A method according to claim 1, wherein computing whether the trajectory of the ball achieves intersection or similar with the goal image comprises computing whether the curve fitted to the trajectory of the ball intersects with the inside of the defined shape.4. A method according to claim 1, wherein the goal criterion comprises one or more of achieving a minimum speed of the ball, achieving placement of the ball in a particular position in the goal image, or achieving a particular spin on the ball.5. A method according to claim 1, wherein the goal criterion comprises a difficulty input, wherein the goal criterion is defeating a virtual obstacle to score a goal, and the difficulty input is the difficulty of defeating the virtual obstacle, wherein the goal criterion determines whether the kick of the ball would be a goal or not according to the difficulty input.6. A method according to claim 1, wherein the goal criterion determines whether the kick of the ball would be a goal or not according to the difficulty input computed trajectory is a curved 3D flight path of the ball.7. A method according to claim 1, wherein the goal plane is mapped to a probability density.8. A method according to claim 1, wherein the probability density is determined by characteristics assigned to a virtual goalkeeper.9. A method according to claim 1, wherein the goal plane is mapped to a probability density according to an equation of a curve derived from the characteristics of height and arm span of the goalkeeper.10. A method according to claim 1, wherein a probability is selected form the probability density according to the placement of the ball.11. A method according to claim 1, wherein the selected probability density is modified according to characteristics of the kick.12. A method for determining a property of a trajectory of a ball with a 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, comprising:capturing a sequence of video frames of the ball with a camera of the 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
;finding candidates of the ball in at least three of the video frames;eliminating false positive detections of the ball, comprising fitting ball candidates to a curve trajectory in which ball candidates not sufficiently representing the curve trajectory are eliminated as ball candidates;computing a property of the trajectory of the ball using travel of the ball through the curve trajectory.13. A method according to claim 12, wherein the eliminating step comprises eliminating outlier candidates of the ball, leaving remaining candidate balls to which the fitting occurs, and wherein eliminating outlier candidate balls comprises eliminating ball candidates that do not have an appropriate sequential change in the size of a circle fitted to the perimeter of the found ball candidates.14. (canceled)15. A method according to claim 12, wherein the curve is fitted to candidates of the ball for video frames in which the ball is determined to be moving.16. A method according to claim 12, wherein the method further comprises identifying a reference object in the video frames.17. (canceled)18. A method according to claim 12, wherein finding ball candidates comprises calculating a gradient vector field.19. A method according to claim 18, wherein the shape matching comprises checking that the gradient direction in the gradient vector field is a smooth circle.20. A method according to claim 12, wherein the method further comprises computing whether the trajectory of the ball achieves intersection or similar with a goal structure image in the video frames according to a goal criterion.21. A method according to claim 12, wherein the property computed comprises one or more of:The force with which the ball was kickedThe spin asserted on the ball22. A 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 configured to perform the method of claim 12.23. (canceled)24. A 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 configured to determine a property of a trajectory of a ball, said 
<a href="https://en.wikipedia.org/wiki/Mobile_computing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device comprising:a camera for capturing a sequence of video frames of the ball;a processor module configured to capture a sequence of video frames of the ball with the camera;a processor module configured to detect the ball in at least three of the video frames;a processor module configured to compute a trajectory of the ball using the detections of the ball;a processor module configured to detect a goal image in at least one of the video frames;a processor module configured to compute whether the trajectory of the ball achieves intersection or similar with a goal plane computed from the goal image according to a goal criterion.25. A 
<a href="https://en.wikipedia.org/wiki/Mobile_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 configured to determine whether a goal is achieved by a trajectory of a ball, said 
<a href="https://en.wikipedia.org/wiki/Mobile_computing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile computer
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device comprising:a camera for capturing a sequence of video frames of the ball;a processor module configured to find candidates of the ball in at least three of the video frames;a processor module configured to eliminate false positive detections of the ball, comprising fitting ball candidates to a curve trajectory in which ball candidates not sufficiently representing the curve trajectory are eliminated as ball candidates;a processor module configured to compute a property of the trajectory of the ball using travel of the ball through the curve trajectory.26. A method according to claim 1, wherein the position of the camera is spaced from the goals and the goals are in the field of view of the camera and wherein the position of the camera is such that the ball moves generally away from the camera._______________OBJECT EXTRACTION FROM VIDEO 
<a href="https://en.wikipedia.org/wiki/System_image"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    IMAGES SYSTEM
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 AND METHOD_____20170713_____XMLs/xml/ipa170713.xml_____US-20170200281-A1 : US-15470477 : US-14525181 : US-9639954 : US-15470477_____G06T0007254000 : G06T0007215000 : G06K0009460000 : G06K0009000000 : G06T0007194000A computer implemented method of 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, the method comprising steps a computer is programmed to perform, the steps comprising: receiving a plurality of video images, deriving a plurality of background templates from at least one of the received video images, calculating a plurality of differences from an individual one of the received video images, each one of the differences being calculated between the individual video image and a respective and different one of the background templates, and extracting an object of interest from the individual video image, using a rule applied on the calculated differences._____d:CROSS-REFERENCE TO RELATED APPLICATIONSThis application is a continuation of U.S. patent application Ser. No. 14/525,181, filed Oct. 27, 2014, which is hereby incorporated in its entirety including all tables, figures, and claims.FIELD AND BACKGROUND OF THE INVENTIONThe present invention relates to 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and, more particularly, but not exclusively to extracting objects of interest from 
<a href="https://en.wikipedia.org/wiki/Image"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video images
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 captured during a sport event.In recent years, the use of 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and 
<a href="https://en.wikipedia.org/wiki/Computer_vision"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer vision
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 has been gaining more and more popularity in a variety of fields and industries. Some known industrial applications of 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and 
<a href="https://en.wikipedia.org/wiki/Computer_vision"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer vision
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 include, for example, security surveillance systems, operational management systems (say in a retail industry environment), tactical battlefield systems, etc.The extraction of objects of interest from video images is an aspect of 
<a href="https://en.wikipedia.org/wiki/Video_content_analysis"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video analysis
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.One of the techniques widely used in the fields of 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and 
<a href="https://en.wikipedia.org/wiki/Computer_vision"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer vision
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is background subtraction.Background subtraction is a technique in which an image's foreground is extracted for further processing, usually for recognition of objects of interest.Generally, an image's foreground is made of regions of the image, which are occupied by objects of interest (humans, cars, text, etc.). After a stage of image preprocessing (which may include image noise removal, morphology based analysis, etc.), 
<a href="https://en.wikipedia.org/wiki/Localization"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object localization
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may be required, which object localization may make use of background subtraction.Background subtraction is widely used for detecting moving objects (say cars or pedestrians) in videos, from static cameras, the rationale being one of detecting the moving objects from the difference between the current frame and a reference background template, also referred to as “background image” or “
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
”, which is made of static objects such as a building or a 
<a href="https://en.wikipedia.org/wiki/Traffic_light"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    traffic light
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 positioned at a road intersection.Objection extraction by background subtraction is often done if the image in question is a part of a 
<a href="https://en.wikipedia.org/wiki/Streaming_media"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video stream
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. Background subtraction provides important cues for numerous applications in 
<a href="https://en.wikipedia.org/wiki/Computer_vision"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer vision
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, for example surveillance tracking or human poses estimation.SUMMARY OF THE INVENTIONAccording to one aspect of the present invention there is provided a computer implemented method of 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, the method comprising steps a computer is programmed to perform, the steps comprising: receiving a plurality of video images, deriving a plurality of background templates from at least one of the received video images, calculating a plurality of differences from an individual one of the received video images, each one of the differences being calculated between the individual video image and a respective and different one of the background templates, and extracting an object of interest from the individual video image, using a rule applied on the calculated differences.According to a second aspect of the present invention there is provided an apparatus for 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, the apparatus comprising: a computer, a 
<a href="https://en.wikipedia.org/wiki/Image"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video image
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 receiver, implemented on the computer, configured to receive a plurality of video images, a background template deriver, in communication with the 
<a href="https://en.wikipedia.org/wiki/Image"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video image
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 receiver, configured to derive a plurality of background templates from at least one of the received video images, a difference calculator, in communication with the background template deriver, configured to calculate a plurality of differences from an individual one of the received video images, each one of the differences being calculated between the individual video image and a respective and different one of the background templates, and an object extractor, in communication with the difference calculator, configured to extract an object of interest from the individual video image, using a rule applied on the calculated differences.According to a third aspect of the present invention there is provided a non-transitory 
<a href="https://en.wikipedia.org/wiki/Machine-readable_medium"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer readable
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 medium storing computer executable instructions for performing steps of 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, the steps comprising: receiving a plurality of video image, deriving a plurality of background templates from at least one of the received video images, calculating a plurality of differences from an individual one of the received video images, each one of the differences being calculated between the individual video image and a respective and different one of the background templates, and extracting an object of interest from the individual video image, using a rule applied on the calculated differences.Unless otherwise defined, all technical and scientific terms used herein have the same meaning as commonly understood by one of ordinary skill in the art to which this invention belongs.The materials, methods, and examples provided herein are illustrative only and not intended to be limiting. Implementation of the method and system of the present invention involves performing or completing certain selected tasks or steps manually, automatically, or a combination thereof.Moreover, according to actual instrumentation and equipment of preferred embodiments of the method and system of the present invention, several selected steps could be implemented by hardware or by software on any 
<a href="https://en.wikipedia.org/wiki/Operating_system"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    operating system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of any firmware or a combination thereof.For example, as hardware, selected steps of the invention could be implemented as a chip or a circuit. As software, selected steps of the invention could be implemented as a plurality of software instructions being executed by a computer using any suitable 
<a href="https://en.wikipedia.org/wiki/Operating_system"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    operating system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. In any case, selected steps of the method and system of the invention could be described as being performed by a 
<a href="https://en.wikipedia.org/wiki/Data_processing_(disambiguation)"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    data processor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, such as a 
<a href="https://en.wikipedia.org/wiki/Computing_platform"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computing platform
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for executing a plurality of instructions.BRIEF DESCRIPTION OF THE DRAWINGSThe invention is herein described, by way of example only, with reference to the accompanying drawings. With specific reference now to the drawings in detail, it is stressed that the particulars shown are by way of example and for purposes of illustrative discussion of the preferred embodiments of the present invention only, and are presented in order to provide what is believed to be the most useful and readily understood description of the principles and conceptual aspects of the invention. The description taken with the drawings making apparent to those skilled in the art how the several forms of the invention may be embodied in practice.In the drawings:FIG. 1 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 schematically illustrating an exemplary apparatus for 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, according to an exemplary embodiment of the present invention.FIG. 2 is a simplified flowchart schematically illustrating a first exemplary method for 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, according to an exemplary embodiment of the present invention.FIG. 3 is a simplified flowchart schematically illustrating a second exemplary method for 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, according to an exemplary embodiment of the present invention.FIGS. 4A-4H are simplified block diagrams schematically illustrating a first implementation scenario, according to an exemplary embodiment of the present invention.FIG. 4A illustrates a player who stands in a left position next to one or more trees and a cloud.FIG. 4B illustrates a player who stands in a right position next to one or more trees and a cloud.FIG. 4C is a background template containing one or more trees and a cloud. It does not include the image of a player.FIG. 4D is a background template containing one or more trees and a cloud. It also includes the image of a player in the left position.FIG. 4E illustrates a player standing in a left position.FIG. 4F illustrates a player who stands next to one or more trees and the sun coming out from behind a cloud.FIG. 4G is a background template containing one or more trees and a cloud.FIG. 4H illustrates a player who stands next to one or more trees, with the sun coming out from behind a cloud.FIG. 5 is a simplified flowchart schematically illustrating a third exemplary method for 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, according to an exemplary embodiment of the present invention.FIGS. 6A-6O are simplified block diagrams schematically illustrating a second implementation scenario, according to an exemplary embodiment of the present invention.FIG. 6A illustrates a player who stands in a left position next to one or more trees and a cloud.FIG. 6B illustrates a player stands in a right position next to one or more trees and a cloud.FIG. 6C illustrates a player who stands in a left position next to one or more trees and a cloud.FIG. 6D is a background template containing one or more trees and a cloud. It does not include the image of a player.FIG. 6E is a background template containing one or more trees and a cloud. It also includes the image of a player in the right position.FIG. 6F illustrates a player standing in a left position.FIG. 6G is a background template containing one or more trees and a cloud. It also includes the image of a player in the right position and a player in the left position.FIG. 6H illustrates a player who stands in a left position.FIG. 6I illustrates a player who stands in a left position next to one or more trees and the sun.FIG. 6J illustrates a player who stands in a right position next to one or more trees and the sun.FIG. 6K is a background template containing one or more trees and a cloud. It does not include the image of a player.FIG. 6L illustrates a player who stands in a left position next to one or more trees and the sun.FIG. 6M illustrates a player who stands in a right position next to one or more trees and the sun.FIG. 6N is a background template containing one or more trees and a cloud. It also includes the image of a player in a right position and a player in a left position.FIG. 6O shows a player standing in a right position.FIG. 7 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 schematically illustrating an exemplary 
<a href="https://en.wikipedia.org/wiki/Machine-readable_medium"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer readable
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 medium storing computer executable instructions for performing steps of 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, according to an exemplary embodiment of the present invention._____c:1. A computer implemented method of 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, the method comprising steps a computer is programmed to perform, the steps comprising:receiving a plurality of video images;deriving a plurality of background templates from at least one of the received video images;calculating a plurality of differences from an individual one of the received video images, each one of the differences being calculated between the individual video image and a respective and different one of the background templates; andextracting an object of interest from the individual video image, using a rule applied on the calculated differences.2. The method of claim 1, further comprising selecting the applied rule among a plurality of predefined rules.3. The method of claim 1, further comprising allowing a user to select the applied rule.4. The method of claim 1, further comprising selecting the applied rule, according to circumstances of capturing of the received video images.5. The method of claim 1, further comprising selecting the applied rule, according to a characteristic pertaining to the object of interest.7. The method of claim 1, further comprising selecting the applied rule, according to a characteristic pertaining to a background of the received video images.8. The method of claim 1, wherein said deriving of the background templates is based on a rule selected among a plurality of predefined rules.9. The method of claim 1, further comprising allowing a user to select a rule for said deriving of the background templates, wherein said deriving is based on the rule selected by the user.10. The method of claim 1, wherein said deriving of the background templates is based on a rule selected according to circumstances of capturing of the received video images.11. The method of claim 1, wherein said deriving of the background templates is based on a rule selected according to a characteristic pertaining to the object of interest.12. The method of claim 1, wherein said deriving of the background templates is based on a rule selected according to a characteristic pertaining to a background of the received video images.13. The method of claim 1, further comprising deriving each one of at least two of the background templates, using a respective and different one of a plurality of background calculation methods.14. The method of claim 1, further comprising deriving each one of at least two of the background templates, using a respective and at least partially different subset of the received video images.15. The method of claim 1, further comprising deriving each one of at least two of the background templates, using a respective and at least partially less recent subset of the received video images.16. The method of claim 1, further comprising deriving each one of at least two of the background templates, using a respective and different frequency of sampling of the received video images.17. The method of claim 1, further comprising deriving each one of at least two of the background templates, using a respective and different in size subset of the received video images.18. The method of claim 1, further comprising updating each one of at least two of the background templates, with a respective and different update rate.19. Apparatus for 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, the apparatus comprising:a computer;a 
<a href="https://en.wikipedia.org/wiki/Image"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video image
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 receiver, implemented on the computer, configured to receive a plurality of video images;a background template deriver, in communication with said 
<a href="https://en.wikipedia.org/wiki/Image"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video image
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 receiver, configured to derive a plurality of background templates from at least one of the received video images;a difference calculator, in communication with said background template deriver, configured to calculate a plurality of differences from an individual one of the received video images, each one of the differences being calculated between the individual 
<a href="https://en.wikipedia.org/wiki/Image"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video image
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and a respective and different one of the background templates; andan object extractor, in communication with said difference calculator, configured to extract an object of interest from the individual video image, using a rule applied on the calculated differences.20. A non-transitory 
<a href="https://en.wikipedia.org/wiki/Machine-readable_medium"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer readable
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 medium storing computer executable instructions for performing steps of 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, the steps comprising:receiving a plurality of video image;deriving a plurality of background templates from at least one of the received video images;calculating a plurality of differences from an individual one of the received video images, each one of the differences being calculated between the individual video image and a respective and different one of the background templates; andextracting an object of interest from the individual video image, using a rule applied on the calculated differences._______________
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    IMAGE PROCESSING
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 METHOD AND INFORMATION PROCESSING APPARATUS_____20190516_____XMLs/xml/ipa190516.xml_____US-20190147604-A1 : US-16166298 : JP-2017-218636_____G06T0007254000 : G06T0007292000An 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus includes a processor that detects positions of ball candidates from a plurality of time-series image frames. The processor adds a position of a second ball candidate to a second image frame subsequent to a first image frame based on a position of a first ball candidate and movement definition information. The first ball candidate is detected from the first image frame. The movement definition information defines a characteristic of a movement of a ball. The processor generates a plurality of trajectory candidates by combining a plurality of ball candidates detected from image frames of different times. The processor evaluates the plurality of trajectory candidates to determine a ball trajectory. The processor interpolates, when the ball trajectory is interrupted, between a starting point and an ending point of the interruption with a trajectory of a first person who moves from the starting point to the ending point._____d:CROSS-REFERENCE TO RELATED APPLICATIONThis application is based upon and claims the benefit of priority of the prior Japanese Patent Application No. 2017-218636, filed on Nov. 13, 2017, the entire contents of which are incorporated herein by reference.FIELDThe embodiments discussed herein are related to an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method and an 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus.BACKGROUNDThe information and 
<a href="https://en.wikipedia.org/wiki/Telecommunication"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    communication technology
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 (ICT) has recently been introduced in the field of sports. For example, there is a technique which captures images of a game and generates statistical information based on the captured images. The statistical information refers to information that combines game performances of a team or an individual player.Here, in order to generate the statistical information, the movements of a player and a ball are both used as input information. As for a technique of tracking the movement of a person, there is, for example, a technique which connects images of a person appearing on multiple cameras in sequence to generate tracking information, based on statistical information premising that the movement speed or direction of a person is somewhat constant. Further, there is a technique which tracks a person by using a template.In addition, as for a technique of tracking the movement of a ball, there is, for example, a technique which detects a ball by a difference image between a background image and an image including a ball and tracks the ball by using a vector related to the ball among images. Further, there is a technique which tracks a ball by detecting the ball from a camera image through an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 on the premise that, for example, a reflecting material is used for the ball.Related technologies are disclosed in, for example, Japanese Laid-
<a href="https://en.wikipedia.org/wiki/Patentleft"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    open Patent
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 Publication No. 2016-099941, Japanese Laid-
<a href="https://en.wikipedia.org/wiki/Patentleft"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    open Patent
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 Publication No. 2004-046647, Japanese Laid-
<a href="https://en.wikipedia.org/wiki/Patentleft"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    open Patent
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 Publication No. 2009-143722, Japanese Laid-open Patent Publication No. 2004-096402, U. S. Patent Publication No. 2005/0254686, and Japanese Laid-
<a href="https://en.wikipedia.org/wiki/Patentleft"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    open Patent
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 Publication No. 2001-273500.SUMMARYAccording to an aspect of the present invention, provided is an 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus including a memory and a processor coupled to the memory. The processor is configured to detect positions of ball candidates from a plurality of time-series image frames. The processor is configured to add a position of a second ball candidate to a second image frame subsequent to a first image frame based on a position of a first ball candidate and movement definition information. The first and second image frames are included in the plurality of image frames. The first ball candidate is detected from the first image frame. The movement definition information defines a characteristic of a movement of a ball. The processor is configured to generate a plurality of trajectory candidates by combining a plurality of ball candidates detected from image frames of different times with each other. The processor is configured to evaluate the plurality of trajectory candidates to determine a ball trajectory. The processor is configured to interpolate, when the ball trajectory is interrupted, between a starting point and an ending point of the interruption with a trajectory of a first person who moves from the starting point to the ending point.The object and advantages of the invention will be realized and attained by means of the elements and combinations particularly pointed out in the claims. It is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory and are not restrictive of the invention, as claimed.BRIEF DESCRIPTION OF DRAWINGSFIG. 1 is a view illustrating an example of a system according to Embodiment 1 of the present disclosure;FIG. 2 is a view illustrating an example of image capturing ranges of cameras;FIG. 3 is a view illustrating a configuration of an 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to Embodiment 1;FIG. 4 is a view illustrating an example of a 
<a href="https://en.wikipedia.org/wiki/Data_structure"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    data structure
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a buffer according to Embodiment 1;FIG. 5 is a view illustrating an example of a 
<a href="https://en.wikipedia.org/wiki/Data_structure"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    data structure
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a conversion table according to Embodiment 1;FIG. 6 is a view illustrating an example of a 
<a href="https://en.wikipedia.org/wiki/Data_structure"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    data structure
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a second conversion table;FIG. 7 is a view for describing a relationship between an image frame 
<a href="https://en.wikipedia.org/wiki/Coordinate_system"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    coordinate system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and a global 
<a href="https://en.wikipedia.org/wiki/Coordinate_system"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    coordinate system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
;FIG. 8 is a view illustrating an example of a 
<a href="https://en.wikipedia.org/wiki/Data_structure"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    data structure
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a 
<a href="https://en.wikipedia.org/wiki/Alan_Ball_Jr."><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    ball management
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 table according to Embodiment 1;FIG. 9 is a view illustrating an example of a 
<a href="https://en.wikipedia.org/wiki/Data_structure"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    data structure
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a player management table according to Embodiment 1;FIG. 10 is a view illustrating an example of a 
<a href="https://en.wikipedia.org/wiki/Data_structure"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    data structure
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of movement definition information according to Embodiment 1;FIG. 11 is a view illustrating an example of a 
<a href="https://en.wikipedia.org/wiki/Data_structure"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    data structure
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a trajectory candidate management table according to Embodiment 1;FIG. 12 is a view illustrating an example of a 
<a href="https://en.wikipedia.org/wiki/Data_structure"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    data structure
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a trajectory management table according to Embodiment 1;FIG. 13 is a view illustrating an example of a 
<a href="https://en.wikipedia.org/wiki/Data_structure"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    data structure
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of 
<a href="https://en.wikipedia.org/wiki/Fisher_information"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    statistical information
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to Embodiment 1;FIG. 14 is a view (part 1) for describing a process performed by a 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    ball detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 unit according to Embodiment 1;FIG. 15 is a view (part 2) for describing the process performed by the 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    ball detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 unit according to Embodiment 1;FIG. 16 is a view (part 3) for describing the process performed by the 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    ball detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 unit according to Embodiment 1;FIG. 17 is a view for describing a process performed by a generation unit according to Embodiment 1;FIG. 18 is a view (part 1) for describing a process performed by a trajectory determination unit according to Embodiment 1;FIG. 19 is a view (part 2) for describing the process performed by the trajectory determination unit according to Embodiment 1;FIG. 20 is a flowchart illustrating a procedure of processes performed by the 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to Embodiment 1;FIG. 21 is a view illustrating a configuration of an 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to Embodiment 2;FIG. 22 is a view for describing a process performed by a trajectory determination unit according to Embodiment 2;FIG. 23 is a view illustrating a configuration of an 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to Embodiment 3;FIG. 24 is a view for describing an example of a landmark;FIG. 25 is a view illustrating an example of a 
<a href="https://en.wikipedia.org/wiki/Data_structure"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    data structure
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of landmark information;FIG. 26 is a view for describing a process performed by a trajectory determination unit according to Embodiment 3;FIG. 27 is a view illustrating a configuration of an 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to Embodiment 4;FIG. 28 is a view for describing a process performed by a trajectory determination unit according to Embodiment 4;FIG. 29 is a flowchart illustrating a procedure of processes performed by the 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to Embodiment 4; andFIG. 30 is a view illustrating an example of a hardware configuration._____c:1. A non-transitory computer-readable 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    recording medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 having stored therein a program that causes a computer to execute a process, the process comprising:detecting positions of ball candidates from a plurality of time-series image frames;adding a position of a second ball candidate to a second image frame subsequent to a first image frame based on a position of a first ball candidate and movement definition information, the first and second image frames being included in the plurality of image frames, the first ball candidate being detected from the first image frame, the movement definition information defining a characteristic of a movement of a ball;generating a plurality of trajectory candidates by combining a plurality of ball candidates detected from image frames of different times with each other;evaluating the plurality of trajectory candidates to determine a ball trajectory; andinterpolating, when the ball trajectory is interrupted, between a starting point and an ending point of the interruption with a trajectory of a first person who moves from the starting point to the ending point.2. The non-transitory computer-readable 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    recording medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to claim 1, the process further comprising:estimating the position of the second ball candidate based on the position of the first ball candidate and the movement definition information; andadding the estimated position of the second ball candidate to the second image frame when no ball candidate is present in a predetermined range defined based on the estimated position of the second ball candidate.3. The non-transitory computer-readable 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    recording medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to claim 2, the process further comprising:assigning lost information to a third ball candidate on the second frame image, the third ball candidate not being included in the predetermined range; anddetermining the ball trajectory based on the lost information included in the trajectory candidates.4. The non-transitory computer-readable 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    recording medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to claim 3, the process further comprising:counting a number of ball candidates assigned with the lost information with respect to each of the plurality of trajectory candidates; anddetermining the ball trajectory from trajectory candidates having the counted number less than a predetermined 
<a href="https://en.wikipedia.org/wiki/Critical_value"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    threshold value
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.5. The non-transitory computer-readable 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    recording medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to claim 1, the process further comprising:comparing the determined ball trajectory and the plurality of trajectory candidates with each other to generate a new trajectory by connecting the determined ball trajectory and one of the plurality of trajectory candidates to each other.6. The non-transitory computer-readable 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    recording medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to claim 1, the process further comprising:evaluating, to determine the ball trajectory, the plurality of trajectory candidates by further using positional information of a landmark on a field where the first person plays a game.7. The non-transitory computer-readable 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    recording medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to claim 1, the process further comprising:extending the ball trajectory when a second trajectory of a second person is present between a starting point of the ball trajectory and an ending point of one of the plurality of trajectory candidates, by connecting the ball trajectory with the second trajectory and the one of the plurality of trajectory candidates.8. The non-transitory computer-readable 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    recording medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to claim 1, the process further comprising:calculating a distance of a ball dribbled by the first person or a number of times that the first player passes the ball based on the ball trajectory and the trajectory of the first person.9. An 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus comprising:a memory; anda processor coupled to the memory and the processor configured to:detect positions of ball candidates from a plurality of time-series image frames;add a position of a second ball candidate to a second image frame subsequent to a first image frame based on a position of a first ball candidate and movement definition information, the first and second image frames being included in the plurality of image frames, the first ball candidate being detected from the first image frame, the movement definition information defining a characteristic of a movement of a ball;generate a plurality of trajectory candidates by combining a plurality of ball candidates detected from image frames of different times with each other;evaluate the plurality of trajectory candidates to determine a ball trajectory; andinterpolate, when the ball trajectory is interrupted, between a starting point and an ending point of the interruption with a trajectory of a first person who moves from the starting point to the ending point.10. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 9, wherein the processor is further configured to:estimate the position of the second ball candidate based on the position of the first ball candidate and the movement definition information; andadd the estimated position of the second ball candidate to the second image frame when no ball candidate is present in a predetermined range defined based on the estimated position of the second ball candidate.11. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 10, wherein the processor is further configured to:assign lost information to a third ball candidate on the second frame image, the third ball candidate not being included in the predetermined range; anddetermine the ball trajectory based on the lost information included in the trajectory candidates.12. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 11, wherein the processor is further configured to:count a number of ball candidates assigned with the lost information with respect to each of the plurality of trajectory candidates; anddetermine the ball trajectory from trajectory candidates having the counted number less than a predetermined 
<a href="https://en.wikipedia.org/wiki/Critical_value"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    threshold value
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.13. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 9, wherein the processor is further configured to:compare the determined ball trajectory and the plurality of trajectory candidates with each other to generate a new trajectory by connecting the determined ball trajectory and one of the plurality of trajectory candidates to each other.14. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 9, wherein the processor is further configured to:evaluate, to determine the ball trajectory, the plurality of trajectory candidates by further using positional information of a landmark on a field where the first person plays a game.15. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 9, wherein the processor is further configured to:extend the ball trajectory when a second trajectory of a second person is present between a starting point of the ball trajectory and an ending point of one of the plurality of trajectory candidates, by connecting the ball trajectory with the second trajectory and the one of the plurality of trajectory candidates.16. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 9, wherein the processor is further configured to:calculate a distance of a ball dribbled by the first person or a number of times that the first player passes the ball based on the ball trajectory and the trajectory of the first person.17. An 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method comprising:detecting, by a computer, positions of ball candidates from a plurality of time-series image frames;adding a position of a second ball candidate to a second image frame subsequent to a first image frame based on a position of a first ball candidate and movement definition information, the first and second image frames being included in the plurality of image frames, the first ball candidate being detected from the first image frame, the movement definition information defining a characteristic of a movement of a ball;generating a plurality of trajectory candidates by combining a plurality of ball candidates detected from image frames of different times with each other;evaluating the plurality of trajectory candidates to determine a ball trajectory; andinterpolating, when the ball trajectory is interrupted, between a starting point and an ending point of the interruption with a trajectory of a first person who moves from the starting point to the ending point.18. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method according to claim 17, further comprising:estimating the position of the second ball candidate based on the position of the first ball candidate and the movement definition information; andadding the estimated position of the second ball candidate to the second image frame when no ball candidate is present in a predetermined range defined based on the estimated position of the second ball candidate.19. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method according to claim 18, further comprising:assigning lost information to a third ball candidate on the second frame image, the third ball candidate not being included in the predetermined range; anddetermining the ball trajectory based on the lost information included in the trajectory candidates.20. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method according to claim 19, further comprising:counting a number of ball candidates assigned with the lost information with respect to each of the plurality of trajectory candidates; anddetermining the ball trajectory from trajectory candidates having the counted number less than a predetermined 
<a href="https://en.wikipedia.org/wiki/Critical_value"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    threshold value
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
._______________REAL-TIME DETECTON OF PERIODIC MOTION SYSTEMS AND METHODS_____20191128_____XMLs/xml/ipa191128.xml_____US-20190362507-A1 : US-16538680 : US-PCT/US2018/019543 : US-16538680 : US-62463273_____G06T0007254000 : G06T0005200000 : G06F0017180000Provided are systems and methods for detecting periodic movement in a 
<a href="https://en.wikipedia.org/wiki/Streaming_media"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video stream
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. A system includes an imaging module configured to capture video of a scene and a 
<a href="https://en.wikipedia.org/wiki/Logic_gate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    logic device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 configured to communicate with the imaging module. The logic device is configured to receive a 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the scene from the imaging module, where the received video sequence comprises one or more video regions that are pixel-wise consistent between successive frames of the received video sequence. The 
<a href="https://en.wikipedia.org/wiki/Logic_gate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    logic device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is configured to determine a periodicity deviation with respect to at least one of the one or more video regions based, at least in part, on the at least one video region. The 
<a href="https://en.wikipedia.org/wiki/Logic_gate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    logic device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may issue an alert based, at least in part, on the determined periodicity deviation._____d:CROSS REFERENCE TO RELATED APPLICATIONSThis application is a continuation of International Patent Application No. PCT/US2018/019543 filed Feb. 24, 2018 and entitled “
<a href="https://en.wikipedia.org/wiki/Real-time"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    REAL TIME
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 DETECTION OF PERIODIC MOTION SYSTEMS AND METHOD,” which is incorporated herein by reference in its entirety.International Patent Application No. PCT/US2018/019543 filed Feb. 24, 2018 claims priority to and the benefit of U.S. Provisional Patent Application. No. 62/463,273 filed Feb. 24, 2017 and entitled “REAL-TIME DETECTION OF PERIODIC MOTION SYSTEMS AND METHODS,” which is hereby incorporated by reference in its entirety.TECHNICAL FIELDOne or more embodiments of the invention relate generally to detection of movement, and more particularly, for example, to near real-time detection of periodic movement in a 
<a href="https://en.wikipedia.org/wiki/Streaming_media"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video stream
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.BACKGROUNDVideo surveillance has made considerable advancements over the past few decades as the proliferation of 
<a href="https://en.wikipedia.org/wiki/Digital_camera"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    digital cameras
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the internet of things have evolved. A majority of businesses are now monitored by security cameras, and people are increasingly installing cameras in and around their homes to monitor their property and family. A majority of these systems go unmonitored in 
<a href="https://en.wikipedia.org/wiki/Real-time_(disambiguation)"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    real time
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and are primarily used to investigate after-the-fact observed aberrations in a user's daily routine. This process is time intensive and exceedingly inefficient.In more recent years, 
<a href="https://en.wikipedia.org/wiki/Closed-circuit_television"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video surveillance
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 systems have implemented various forms of 
<a href="https://en.wikipedia.org/wiki/Motion_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 recording and alerts so that the review process time is more focused and efficient. Integrating motion detection has allowed alerts to be detected and sent to users when 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    the system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 detects movement captured by one of the cameras in a surveillance system. However, contemporary alert detection systems suffer from many problems.Of primary concern, by detecting and triggering an alert based on arbitrary movement in the image capture region, many false alarms are reported. For example, a tree blowing in the wind in an image capture region could repeatedly trigger an unwanted alert. After a few too many false alarms, a user may decide to terminate the 
<a href="https://en.wikipedia.org/wiki/Motion_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 feature or simply ignore the repeated alerts. Some systems have sought to remedy this problem by allowing a user to select portions of the camera's viewing area and only triggering an alert when motion is detected in those user selected 
<a href="https://en.wikipedia.org/wiki/Motion_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 areas, but the user risks not being alerted to potentially triggering events that are outside those designated areas. Moreover, the same false alert problems exist within each selected 
<a href="https://en.wikipedia.org/wiki/Motion_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 areas; namely that any arbitrary movement will trigger an alert. Other conventional methods of preventing false alerts include requiring motion over a period of time or providing a black out period after an alert, but both of these methods risk missing important triggering events. Thus, conventional 
<a href="https://en.wikipedia.org/wiki/Motion_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 systems remain highly prone to false alerts (and users deactivating or ignoring alerts due to the relatively poor alert reliability) and false negative alerts.Accordingly, there exists a need in the art for a reliable and near real-time detection methodology that can reduce risk of false detections and false negative detections in a 
<a href="https://en.wikipedia.org/wiki/Streaming_media"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video stream
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.SUMMARYProvided is a methodology for detecting periodic movement in a 
<a href="https://en.wikipedia.org/wiki/Streaming_media"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video stream
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. Embodiments provide for an efficient, adaptive method of evaluating periodic movement that can be performed in near real-time and that can generate corresponding alerts for distribution to one or more monitoring users or for further processing. Embodiments provide reliable detection of and differentiation between different types of periodic motion, such as detection of a human waving their hands or arms to signal for help, as differentiated from a pets wagging tail, as described in detail herein.In one embodiment, a system includes an imaging module configured to capture video of a scene and a 
<a href="https://en.wikipedia.org/wiki/Logic_gate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    logic device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 configured to communicate with the imaging module. The 
<a href="https://en.wikipedia.org/wiki/Logic_gate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    logic device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may be configured to receive a 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the scene from the imaging module, where the received video sequence comprises one or more video regions that are pixel-wise consistent between successive frames of the received 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
; determine a periodicity deviation with respect to at least one of the one or more video regions based, at least in part, on the at least one video region; and issue an alert based, at least in part, on the determined periodicity deviation.In another embodiment, a method includes receiving a 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a scene from an imaging module, where the received video sequence comprises one or more video regions that are pixel-wise consistent between successive frames of the received 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
; determining a periodicity deviation with respect to at least one of the one or more video regions based, at least in part, on the at least one video region; and issuing an alert based, at least in part, on the determined periodicity deviation.Embodiments of the invention and their advantages are best understood by referring to the detailed description that follows. It should be appreciated that like reference numerals are used to identify like elements illustrated in one or more of the figures.BRIEF DESCRIPTION OF THE DRAWINGSFIG. 1 illustrates a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of an imaging system configured to detect periodic motion in a scene, in accordance with an embodiment of the disclosure.FIG. 2 illustrates a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a controller configured to perform periodic motion detection, in accordance with an embodiment of the disclosure.FIG. 3 illustrates a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a preprocessing block to facilitate periodic motion detection, in accordance with an embodiment of the disclosure.FIG. 4 illustrates a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a periodic motion detection block to determine periodicity deviations in a 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, in accordance with an embodiment of the disclosure.FIG. 5 illustrates a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of an alert block to issue an alert based on a periodicity deviation in a 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, in accordance with an embodiment of the disclosure.FIG. 6 illustrates a comparison of a first 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 including a global temporal trend against a second 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 with the global temporal trend removed by de-trending, in accordance with an embodiment of the disclosure.FIG. 7 illustrates fifty consecutive frames of an infrared video sequence arranged in frame series that portray a person waving both arms as a form of periodic motion, in accordance with an embodiment of the disclosure.FIG. 8 illustrates a 
<a href="https://en.wikipedia.org/wiki/Flow_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    flow diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a process to provide periodic motion detection, in accordance with an embodiment of the disclosure.FIG. 9 illustrates a 
<a href="https://en.wikipedia.org/wiki/Flow_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    flow diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a process to provide periodic motion detection, in accordance with an embodiment of the disclosure._____c:1. A system comprising:an imaging module configured to capture video of a scene; anda 
<a href="https://en.wikipedia.org/wiki/Logic_gate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    logic device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 configured to communicate with the imaging module, wherein the 
<a href="https://en.wikipedia.org/wiki/Logic_gate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    logic device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is configured to:receive a 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the scene from the imaging module, wherein the received video sequence comprises one or more video regions that are pixel-wise consistent between successive frames of the received 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
;determine at least one periodicity deviation corresponding to at least one of the one or more video regions based, at least in part, on the at least one video region; andissue an alert based, at least in part, on the determined at least one periodicity deviation.2. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 1, wherein the 
<a href="https://en.wikipedia.org/wiki/Logic_gate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    logic device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is configured to:determine a spatial average of the received 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
; andde-trend the received video sequence by subtracting the determined spatial average from each frame of the received 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, prior to the determining the periodicity deviation.3. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 1, wherein the 
<a href="https://en.wikipedia.org/wiki/Logic_gate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    logic device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is configured to:apply a 
<a href="https://en.wikipedia.org/wiki/Low-pass_filter"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    low pass filter
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to the received 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, prior to the determining the periodicity deviation.4. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 3, wherein:the low pass filter comprises a linear low pass filter; andthe linear low pass filter is implemented by a separable two dimensional 
<a href="https://en.wikipedia.org/wiki/Daniel_Bernoulli"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Bernoulli filter
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.5. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 1, wherein:the one or more video regions comprise a plurality of rectangular video regions;each rectangular video region at least partially overlaps at least one other rectangular video region; andeach rectangular video region comprises a height or width between 5 pixels and one half a corresponding full height or width of a frame of the received 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.6. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 1, wherein:the one or more video regions comprise a plurality of overlapping rectangular video regions; andthe plurality of overlapping rectangular video regions comprises a corresponding plurality of positions and/or sizes configured to completely cover each frame of the received 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.7. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 1, wherein:the determining the periodicity deviation comprises determining a discretized, grey level translation invariant, normalized periodicity deviation corresponding to the at least one video region based, at least in part, on the at least one video region and an identified period; andthe temporal length of the received 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is at least twice the identified period.8. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 1, wherein the one or more video regions comprise a plurality of video regions, the at least one periodicity deviation comprises a plurality of periodicity deviations corresponding respectively to the plurality of video regions, and the 
<a href="https://en.wikipedia.org/wiki/Logic_gate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    logic device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is configured to:determine the plurality of periodicity deviations corresponding respectively to the plurality of video regions based, at least in part, on the corresponding respective video regions and one or more identified periods or an identified range of periods; andissue the alert based, at least in part, on the determined plurality of periodicity deviations, the one or more identified periods or the identified range of periods, and a periodicity deviation threshold.9. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 1, wherein the at least one periodicity deviation comprises first and second periodicity deviations corresponding respectively to first and second identified periods associated with a biological periodic motion, and wherein the 
<a href="https://en.wikipedia.org/wiki/Logic_gate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    logic device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is configured to:determine the first periodicity deviation corresponding to the first identified period is greater than or equal to a periodicity deviation threshold corresponding to the biological periodic motion;determine a first probability associated with the first identified period and the biological periodic motion is greater than a probability threshold associated with the biological periodic motion;determine the second periodicity deviation corresponding to the second identified period is less than the periodicity deviation threshold corresponding to the biological periodic motion; andissue the alert based, at least in part, on the second periodicity deviation, to indicate a presence of the biological periodic motion in the 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.10. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 9, wherein:the 
<a href="https://en.wikipedia.org/wiki/Logic_gate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    logic device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is configured to update a probability table associated with a set of identified periods and the biological periodic motion;the set of identified periods comprises the first and second identified periods;the probability table comprises at least the first probability associated with the first identified period and the biological periodic motion and a second probability associated with the second identified period and the biological periodic motion; andthe updating the probability table comprises increasing the second probability and decreasing at least the first probability in the probability table.11. A method comprising:receiving a 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a scene from an imaging module, wherein the received video sequence comprises one or more video regions that are pixel-wise consistent between successive frames of the received 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
;determining a periodicity deviation with respect to at least one of the one or more video regions based, at least in part, on the at least one video region; andissuing an alert based, at least in part, on the determined periodicity deviation.12. The method of claim 11, further comprising:determining a spatial average of the received 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
; andde-trending the received video sequence by subtracting the determined spatial average from each frame of the received 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, prior to the determining the periodicity deviation.13. The method of claim 11, further comprising:applying a 
<a href="https://en.wikipedia.org/wiki/Low-pass_filter"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    low pass filter
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to the received 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, prior to the determining the periodicity deviation.14. The method of claim 13, wherein:the low pass filter comprises a linear low pass filter; andthe linear low pass filter is implemented by a separable two dimensional 
<a href="https://en.wikipedia.org/wiki/Daniel_Bernoulli"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Bernoulli filter
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.15. The method of claim 11, wherein:the one or more video regions comprise a plurality of rectangular video regions;each rectangular video region at least partially overlaps at least one other rectangular video region; andeach rectangular video region comprises a height or width between 5 pixels and one half a corresponding full height or width of a frame of the received 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.16. The method of claim 11, wherein:the one or more video regions comprise a plurality of rectangular video regions; andthe plurality of rectangular video regions comprises a corresponding plurality of positions and/or sizes configured to completely cover each frame of the received 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.17. The method of claim 11, wherein:the determining the periodicity deviation comprises determining a discretized, translation invariant, normalized periodicity deviation corresponding to the at least one video region based, at least in part, on the at least one video region and an identified period; andthe temporal length of the received 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is at least twice the identified period.18. The method of claim 11, wherein the one or more video regions comprise a plurality of video regions and the at least one periodicity deviation comprises a plurality of periodicity deviations corresponding respectively to the plurality of video regions, the method further comprising:determining the plurality of periodicity deviations corresponding respectively to the plurality of video regions based, at least in part, on the corresponding respective video regions and one or more identified periods or an identified range of periods; andissuing the alert based, at least in part, on the determined plurality of periodicity deviations, the one or more identified periods or the identified range of periods, and a periodicity deviation threshold.19. The method of claim 11, wherein the at least one periodicity deviation comprises first and second periodicity deviations corresponding to respective first and second identified periods associated with a biological periodic motion, the method further comprising:determining the first periodicity deviation corresponding to the first identified period is greater than or equal to a periodicity deviation threshold corresponding to the biological periodic motion;determining a first probability associated with the first identified period and the biological periodic motion is greater than a probability threshold associated with the biological periodic motion;determining the second periodicity deviation corresponding to the second identified period is less than the periodicity deviation threshold corresponding to the biological periodic motion; andissuing the alert based, at least in part, on the second periodicity deviation, to indicate a presence of the biological periodic motion in the 
<a href="https://en.wikipedia.org/wiki/Video_editing_software"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video sequence
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.20. The method of claim 19, wherein:the method comprises updating a probability table associated with a set of identified periods and the biological periodic motion;the set of identified periods comprises at least the first and second identified periods;the probability table comprises at least the first probability associated with the first identified period and the biological periodic motion and a second probability associated with the second identified period and the biological periodic motion; andthe updating the probability table comprises increasing the second probability and decreasing at least the first probability in the probability table._______________A BUILDING 
<a href="https://en.wikipedia.org/wiki/Management_system"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    MANAGEMENT SYSTEM
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 USING OBJECT DETECTION AND TRACKING IN A LARGE SPACE WITH A LOW RESOLUTION SENSOR_____20190124_____XMLs/xml/ipa190124.xml_____US-20190026908-A1 : US-16070945 : US-62280942 : WO-PCT/US2017/014016-00_____G06T0007254000 : G06T0007277000A method of operating an object detection and tracking system includes the step of estimating (202) a current background of a current frame of sensor data generated by a sensor based on a previous frame of sensor data by a computer-based processor. The method further includes estimating (204) a foreground of the current frame of sensor data by comparing the current frame of sensor data to the current background, and detecting (212) an object using a sensor-specific object model._____d:BACKGROUNDThe present disclosure relates to 
<a href="https://en.wikipedia.org/wiki/Building_management"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    building management
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 systems, and more particularly, to object detection and tracking in a large area using a 
<a href="https://en.wikipedia.org/wiki/Sensor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    low resolution sensor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.Infrared detectors used for intrusion and presence detection are typically limited to pixel counts of about four-by-four (4×4) elements to stay reasonable in terms of cost and performance. Even with advances in MEMS, the pixel counts remain less than approximately one hundred-by-one hundred (100×100). The manufacturing process for these low cost detectors does not scale well in terms of cost as pixel count increases. Additionally, the physical size of an infrared 
<a href="https://en.wikipedia.org/wiki/Focal_seizure"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    focal plan
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
e array is large compared to the same pixel count for, as one example, complementary metal oxide silicon (CMOS) visible sensors because of the longer wavelength. As such, one-by-one (1×1) to four-by-four (4×4) pyroelectric elements are commonplace as, for example, occupancy detectors, but even in sizes up to approximately one hundred-by-one hundred (100×100) they are not able to count with the fidelity needed to achieve more efficiently controlled heating, ventilation, and air conditioning (HVAC) systems and lighting. Yet further, 
<a href="https://en.wikipedia.org/wiki/Energy_consumption"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    energy consumption
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of infrared 
<a href="https://en.wikipedia.org/wiki/Cardinal_point_(optics)"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    focal plane
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 arrays becomes larger than desired for arrays having sufficient pixels to meet fidelity needs when supporting other systems such as HVAC and lighting.Typical state-of-the-art presence detection may use a two-element passive infrared (PR) sensor. These sensors typically include faceted lens designs and may include masks and variable detection thresholds to achieve useable performance. Unfortunately, such PIR sensors may have difficulty in distinguishing people from other heat sources (e.g., animals, HVAC operation, etc.), may not be able to localize or track the detected object, and may not be able to count the number of objects.Typical state-of-the-art algorithms for people detection, classification, tracking and counting have been developed in the field of 
<a href="https://en.wikipedia.org/wiki/Computer_vision"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer vision
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. For instance, there are state-of-the-art object detection algorithms for people detection and tracking including Support Vector Machines (SVM) on Histogram of Oriented Gradient (HOG) features, and discriminatively trained Deformable Part Models (DPM). Unfortunately, these algorithms are designed to work on visible spectrum, multi-color video with many hundreds or thousands of pixels on target. It is desirable to design detector systems and associated algorithms for infrared video and/or video with relatively few pixels on target (i.e., tens to a few hundreds of pixels). It is further desirable to develop cost effective detector systems and methods that perform occupancy detection and people counting with improved fidelity and reduced 
<a href="https://en.wikipedia.org/wiki/Energy_consumption"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    energy consumption
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to, for example, support HVAC and lighting systems.SUMMARYA method of operating an object detection and tracking system according to one, non-limiting, embodiment of the present disclosure includes estimating a current background of a current frame of sensor data generated by a sensor and based on a previous frame of sensor data by a computer-based processor; estimating a foreground of the current frame of sensor data by comparing the current frame of sensor data to the current background; and detecting an object using a sensor-specific 
<a href="https://en.wikipedia.org/wiki/Object_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.Additionally to the foregoing embodiment, the sensor is an absolute 
<a href="https://en.wikipedia.org/wiki/Sound_intensity"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    intensity sensor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 utilizing a chopper.In the alternative or additionally thereto, in the foregoing embodiment, the method includes tracking the object via a Bayesian Estimator, and wherein the sensor-specific 
<a href="https://en.wikipedia.org/wiki/Object_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is a chopped-data 
<a href="https://en.wikipedia.org/wiki/Object_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.In the alternative or additionally thereto, in the foregoing embodiment, the chopped-data 
<a href="https://en.wikipedia.org/wiki/Object_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is a Gaussian Mixture 
<a href="https://en.wikipedia.org/wiki/Object_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.In the alternative or additionally thereto, in the foregoing embodiment, the chopped-data 
<a href="https://en.wikipedia.org/wiki/Object_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is parameterized at least in-part by perspective data.In the alternative or additionally thereto, in the foregoing embodiment, the chopped-data 
<a href="https://en.wikipedia.org/wiki/Object_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is learned by discriminative 
<a href="https://en.wikipedia.org/wiki/Sparse_dictionary_learning"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    dictionary learning
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.In the alternative or additionally thereto, in the foregoing embodiment, the Bayesian Estimator is a Kalman Filter.In the alternative or additionally thereto, in the foregoing embodiment, the Bayesian Estimator is a Particle Filter.In the alternative or additionally thereto, in the foregoing embodiment, the sensor is a relative intensity sensor that does not utilize a chopper.In the alternative or additionally thereto, in the foregoing embodiment, the method includes tracking the object utilizing a Bayesian Estimator, and wherein the object is detected via a sensor-specific 
<a href="https://en.wikipedia.org/wiki/Object_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and a 
<a href="https://en.wikipedia.org/wiki/13_Ghosts"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    ghost filter
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to compensate for characteristics of un-chopped imagery.In the alternative or additionally thereto, in the foregoing embodiment, the 
<a href="https://en.wikipedia.org/wiki/13_Ghosts"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    ghost filter
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is an iterative joint estimation process.In the alternative or additionally thereto, in the foregoing embodiment, the 
<a href="https://en.wikipedia.org/wiki/13_Ghosts"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    ghost filter
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is a space adaptive shape suppression process.In the alternative or additionally thereto, in the foregoing embodiment, the Bayesian Estimator is a Kalman Filter.In the alternative or additionally thereto, in the foregoing embodiment, the Bayesian Estimator is a Particle Filter.In the alternative or additionally thereto, in the foregoing embodiment, the sensor-specific 
<a href="https://en.wikipedia.org/wiki/Object_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 utilizes a designed basis.In the alternative or additionally thereto, in the foregoing embodiment, the designed basis comprises one of a Harr basis and a Gabor basis.In the alternative or additionally thereto, in the foregoing embodiment, the designed basis is over-complete.The foregoing features and elements may be combined in various combinations without exclusivity, unless expressly indicated otherwise. These features and elements as well as the operation thereof will become more apparent in light of the following description and the accompanying drawings. However, it should be understood that the following description and drawings are intended to be exemplary in nature and non-limiting.BRIEF DESCRIPTION OF THE DRAWINGSVarious features will become apparent to those skilled in the art from the following detailed description of the disclosed non-limiting embodiments. The drawings that accompany the detailed description can be briefly described as follows:FIG. 1 is a schematic of a 
<a href="https://en.wikipedia.org/wiki/Building_management_system"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    building management system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 utilizing an object detection and tracking (ODT) system of the present disclosure;FIG. 2 is a schematic of one embodiment of the 
<a href="https://en.wikipedia.org/wiki/Building_management_system"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    building management system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 having an ambient 
<a href="https://en.wikipedia.org/wiki/Temperature"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    air temperature
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
<a href="https://en.wikipedia.org/wiki/Control_system"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    control system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 utilizing the ODT system;FIG. 3 is a schematic of the ODT system;FIG. 4 is a 
<a href="https://en.wikipedia.org/wiki/Multiview_orthographic_projection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    plan view
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a 
<a href="https://en.wikipedia.org/wiki/Focal_seizure"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    focal plan
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 array of a remote unit of the ODT system;FIG. 5 is a perspective view of components of the remote unit integrated into a common substrate platform;FIG. 6 is a flow chart of a method of operating a first embodiment of the ODT system utilizing an absolute intensity (chopped) array sensor; andFIG. 7 is a flow chart of a method of operating a second embodiment of the ODT system utilizing a relative intensity (un-chopped) array sensor.FIG. 8 is a diagram of a rotating wheel chopper detector which provides an absolute intensity image.FIG. 9 is a diagram of imagery from an un-chopped detector depicting one possible spurious artefact._____c:1. A method of operating an object detection and tracking system comprising:estimating a current background of a current frame of sensor data generated by a sensor and based on a previous frame of sensor data by a computer-based processor;estimating a foreground of the current frame of sensor data by comparing the current frame of sensor data to the current background; anddetecting an object using a sensor-specific 
<a href="https://en.wikipedia.org/wiki/Object_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.2. The method set forth in claim 1, wherein the sensor is an absolute 
<a href="https://en.wikipedia.org/wiki/Sound_intensity"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    intensity sensor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 utilizing a chopper.3. The method set forth in claim 2 further comprising:tracking the object via a Bayesian Estimator, and wherein the sensor-specific 
<a href="https://en.wikipedia.org/wiki/Object_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is a chopped-data 
<a href="https://en.wikipedia.org/wiki/Object_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.4. The method set forth in claim 3, wherein the chopped-data 
<a href="https://en.wikipedia.org/wiki/Object_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is a Gaussian Mixture 
<a href="https://en.wikipedia.org/wiki/Object_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.5. The method set forth in claim 3, wherein the chopped-data 
<a href="https://en.wikipedia.org/wiki/Object_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is parameterized at least in-part by perspective data.6. The method set forth in claim 3, wherein the chopped-data 
<a href="https://en.wikipedia.org/wiki/Object_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is learned by discriminative 
<a href="https://en.wikipedia.org/wiki/Sparse_dictionary_learning"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    dictionary learning
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.7. The method set forth in claim 3, wherein the Bayesian Estimator is a Kalman Filter.8. The method set forth in claim 3, wherein the Bayesian Estimator is a Particle Filter.9. The method set forth in claim 1, wherein the sensor is a relative intensity sensor that does not utilize a chopper.10. The method set forth in claim 9 further comprising:tracking the object utilizing a Bayesian Estimator, and wherein the object is detected via a sensor-specific 
<a href="https://en.wikipedia.org/wiki/Object_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and a 
<a href="https://en.wikipedia.org/wiki/13_Ghosts"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    ghost filter
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to compensate for characteristics of un-chopped imagery.11. The method set forth in claim 10, wherein the 
<a href="https://en.wikipedia.org/wiki/13_Ghosts"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    ghost filter
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is an iterative joint estimation process.12. The method set forth in claim 10, wherein the 
<a href="https://en.wikipedia.org/wiki/13_Ghosts"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    ghost filter
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is a space adaptive shape suppression process.13. The method set forth in claim 10, wherein the Bayesian Estimator is a Kalman Filter.14. The method set forth in claim 10, wherein the Bayesian Estimator is a Particle Filter.15. The method set forth in claim 9, wherein the sensor-specific 
<a href="https://en.wikipedia.org/wiki/Object_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 utilizes a designed basis.16. The method set forth in claim 13, wherein the designed basis comprises one of a Harr basis and a Gabor basis.17. The method set forth in claim 16, wherein the designed basis is over-complete._______________DETECTING MOTION DRAGGING ARTIFACTS FOR DYNAMIC ADJUSTMENT OF FRAME RATE CONVERSION SETTINGS_____20190613_____XMLs/xml/ipa190613.xml_____US-20190180454-A1 : US-16214632 : US-62597326_____G06T0007254000 : G06T0007110000 : G06T0007194000 : G06T0007400000 : G06T0007130000 : G06T0007155000Motion characteristics related to foreground objects and background regions bordering the foreground objects in images are determined. A 
<a href="https://en.wikipedia.org/wiki/Frame_rate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    frame rate
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 conversion (FRC)-related metadata portion is generated based on the motion characteristics. The FRC-related metadata portion is to be used for determining an optimal FRC operational mode with a downstream device for the images. The images are encoded into a 
<a href="https://en.wikipedia.org/wiki/Streaming_media"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video stream
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. The FRC-related metadata portion is encoded into the video stream as a part of image metadata. The 
<a href="https://en.wikipedia.org/wiki/Streaming_media"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video stream
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is caused to be transmitted to the downstream device._____d:CROSS REFERENCE TO RELATED APPLICATIONSThis application claims priority to U.S. Provisional Patent Application No. 62/597,326, filed on Dec. 11, 2017, the entire contents of which are hereby incorporated by reference.TECHNOLOGYThe present invention relates generally to images. More particularly, an embodiment of the present invention relates to detecting motion dragging artifacts for dynamic adjustment of 
<a href="https://en.wikipedia.org/wiki/Frame_rate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    frame rate
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 conversion settings.BACKGROUNDImage interpolation, which computes a set of plausible interpolated images using two or more adjacent images, has varied applications including but not limited to 
<a href="https://en.wikipedia.org/wiki/Frame_rate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    frame rate
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 conversion (FRC) between different broadcast standards, synthesis of virtual views, animating still images and so on.Some TV manufacturing companies incorporate built-in 
<a href="https://en.wikipedia.org/wiki/Motion_interpolation"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion interpolation
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 technology in their products to perform FRC. A mechanism for FRC can be as simple as merely replicating received images to achieve the desired 
<a href="https://en.wikipedia.org/wiki/Frame_rate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    frame rate
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. For example, a TV running at an image refresh rate of 120 Hz and receiving a 30 Hz image sequence may simply display each image four consecutive times. The advantage of this solution is that the complexity of 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    the system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is very low, at the expense of possibly resulting in 
<a href="https://en.wikipedia.org/wiki/Motion_interpolation"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion judder
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.Complicated systems can be designed for motion interpolation. However, computational costs of such techniques can be quite high, and may even result in motion dragging artifacts, noticeable lags, and so forth, in viewing image sequences involving motions.The approaches described in this section are approaches that could be pursued, but not necessarily approaches that have been previously conceived or pursued. Therefore, unless otherwise indicated, it should not be assumed that any of the approaches described in this section qualify as 
<a href="https://en.wikipedia.org/wiki/Prior_art"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    prior art
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 merely by virtue of their inclusion in this section. Similarly, issues identified with respect to one or more approaches should not assume to have been recognized in any 
<a href="https://en.wikipedia.org/wiki/Prior_art"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    prior art
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 on the basis of this section, unless otherwise indicated.BRIEF DESCRIPTION OF DRAWINGSThe present invention is illustrated by way of example, and not by way of limitation, in the figures of the accompanying drawings and in which like reference numerals refer to similar elements and in which:FIG. 1 illustrates an example process flow that can be used to detect foreground and background objects in media content;FIG. 2A and FIG. 2B illustrate example process flows that can be used to determine whether motion dragging artifacts are likely to be generated in a set of images;FIG. 3A through FIG. 3C illustrate example video encoders and clients;FIG. 4A and FIG. 4B illustrate example process flows; andFIG. 5 illustrates an example hardware platform on which a computer or a 
<a href="https://en.wikipedia.org/wiki/Computer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 as described herein may be implemented._____c:1. A method, comprising:determining one or more motion characteristics related to one or more foreground objects and one or more background regions bordering the one or more foreground objects in one or more images;generating, based at least in part on the one or more motion characteristics related to the one or more foreground objects and the one or more background regions bordering the one or more foreground objects in the one or more images, a frame rate conversion (FRC)-related metadata portion, wherein the FRC-related metadata portion is to be used for determining an optimal FRC operational mode with a downstream device for the one or more images;encoding the one or more images into a 
<a href="https://en.wikipedia.org/wiki/Streaming_media"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video stream
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, wherein the FRC-related metadata portion is encoded into the 
<a href="https://en.wikipedia.org/wiki/Streaming_media"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video stream
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 as a part of image metadata;causing the 
<a href="https://en.wikipedia.org/wiki/Streaming_media"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video stream
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to be transmitted to the downstream device.2. The method of claim 1, wherein the one or more motion characteristics are computed after the one or more foreground objects and the one or more background regions are separated in the one or more images using one or more optical-flow i
<a href="https://en.wikipedia.org/wiki/The_Book_of_Abramelin"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mage analysis
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 methods.3. The method of claim 1, wherein the one or more motion characteristics are computed after the one or more foreground objects and the one or more background regions are separated in the one or more images using one or more non-optical-flow 
<a href="https://en.wikipedia.org/wiki/Image_analysis"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image analysis
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 methods.4. The method of claim 3, wherein the one or more optical-flow 
<a href="https://en.wikipedia.org/wiki/Image_analysis"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image analysis
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 methods comprise one or more of: operations generating one or more optical flows based on image content visually depicted in the one or more images, operations generating and completing edges between the foreground objects and the background regions from one or more optical flows, operations flood filling closed edges of the foreground objects, operations generating one or more binary masks delineating the foreground objects and the background regions, morphological operations performed on edges detected from one or more 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, or morphological operations performed on one or more binary masks directly or indirectly derived from one or more optical flows.5. The method of claim 1, wherein the one or more motion characteristics include one or more of: pixel-based motion vector magnitudes, pixel-block-based motion vector magnitudes, scene-based motion vector magnitudes, pixel-based motion vector directions, pixel-block-based motion vector directions, or scene-based motion vector directions.6. The method of claim 1, wherein the FRC-related metadata portion is generated further based at least in part on texture information determined for the one or more foreground objects and the one or more background regions.7. The method of claim 1, wherein the one or more images belong to a set of images that represent a scene; and wherein the optimal FRC operational mode applies to all images in the set of images that represent the scene.8. The method of claim 1, wherein the 
<a href="https://en.wikipedia.org/wiki/Streaming_media"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video stream
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is encoded with an image sequence representing a first time sequence of images that include the one or more images; wherein the first time sequence of images supports a first image refres
<a href="https://en.wikipedia.org/wiki/Heart_rate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    h rate
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 in normal playing of the time sequence of images; wherein the downstream device supports a second different image refresh rate in normal playing; and wherein the downstream device is to operate the optimal FRC operational mode to generate, based on the one or more images decoded from the 
<a href="https://en.wikipedia.org/wiki/Streaming_media"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video stream
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, additional images for complying with the second image refresh rate.9. The method of claim 1, wherein the one or more images comprise a plurality of spatial regions; wherein the plurality of spatial regions respectively corresponds to a plurality of sets of motion characteristics; wherein each spatial region in the plurality of spatial regions corresponds to a respective set of motion characteristics; wherein the optimal FRC operational mode represents a FRC operational mode optimally selected from a plurality of FRC operational modes for a specific spatial region in the plurality of spatial regions of the one or more images; and wherein the motion metadata portion is to be used to determine a second optimal FRC operational mode with the downstream device that represents a second different FRC operational mode optimally selected from the plurality of FRC operational modes for a second specific spatial region in the plurality of spatial regions of the one or more images.10. The method of claim 9, wherein the plurality of FRC operational modes comprises two or more FRC operational modes indicating different levels of image interpolation.11. The method of claim 1, wherein at least one of the one or more motion characteristics related to the one or more foreground objects and the one or more background regions bordering the one or more foreground objects in the one or more images is determined based on 
<a href="https://en.wikipedia.org/wiki/Motion_vector"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion vectors
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 in one or more images, and wherein the 
<a href="https://en.wikipedia.org/wiki/Motion_vector"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion vectors
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 are already pre-computed by an upstream device.12. The method of claim 1, wherein the FRC-related metadata portion includes one or more probability values to indicate whether motion dragging is likely to occur in derived images interpolated from the one or more images.13. The method of claim 1, wherein each of the one or more probability values is computed based at least in part on one or more of: a difference between an overall motion vector magnitude of a foreground object and a background region bordering the foreground object as determined for the one or more images; a difference between an overall motion vector direction of the foreground object and the background region as determined for the one or more images; a texture of the foreground object as determined for the one or more images; or a texture of the background region.14. The method of claim 1, wherein the one or more probability values comprises a scene-based probability value to indicate whether a scene that includes in the one or more images is likely to have motion dragging with 
<a href="https://en.wikipedia.org/wiki/Image_scaling"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image interpolation
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and wherein the scene-based probability is computed based at least in part on one or more of: a difference between an overall motion vector magnitude of a foreground object and a background region bordering the foreground object in the scene, a difference between an overall motion vector direction of the foreground object and the background region in the scene, a texture of the foreground object in the scene, or a texture of the background region in the scene.15. The method of claim 1, wherein the one or more probability values comprises a scene-based probability value to indicate whether a scene that includes in the one or more images is likely to have motion dragging with 
<a href="https://en.wikipedia.org/wiki/Image_scaling"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image interpolation
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and wherein the scene-based probability is computed based at least in part on a percentage of images in the scene that are determined to be likely to have motion dragging with 
<a href="https://en.wikipedia.org/wiki/Image_scaling"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image interpolation
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.16. A method, comprising:decoding, from a 
<a href="https://en.wikipedia.org/wiki/Streaming_media"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video stream
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, one or more images and a 
<a href="https://en.wikipedia.org/wiki/Frame_rate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    frame rate
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 conversion (FRC)-related metadata portion, wherein the FRC-related metadata portion is generated by an upstream device for the one or more images based at least in part on one or more motion characteristics related to one or more foreground objects and one or more background regions bordering the one or more foreground objects in the one or more images, wherein the one or more motion characteristics are computed after the one or more foreground objects and the one or more background regions are separated based on image content visually depicted in one or more images;using the FRC-related metadata portion to determine an optimal FRC operational mode for the one or more images;operating the optimal FRC operational mode to generate, based on the one or more images, zero or more additional images in addition to the one or more images;causing the one or more images and the zero or more additional images to be rendered on a 
<a href="https://en.wikipedia.org/wiki/Display_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    display device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.17. The method of claim 16, wherein the FRC-related metadata portion includes one or more probability values to indicate whether motion dragging is likely to occur in derived images interpolated from the one or more images.18. The method of claim 16, wherein the optimal FRC operational mode represents a specific FRC operational mode selected from a plurality of FRC operational modes for the one or more images.19. The method of claim 18, wherein the plurality of FRC operational modes comprises two or more FRC operational modes indicating different levels of image interpolation.20. The method of claim 16, wherein the FRC-related metadata portion for the one or more images indicates avoiding generating the one or more additional images using 
<a href="https://en.wikipedia.org/wiki/Image_scaling"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image interpolation
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the one or more images in the optimal FRC operational mode.21. The method of claim 16, wherein the FRC-related metadata portion for the one or more images indicates generating the one or more additional images using 
<a href="https://en.wikipedia.org/wiki/Image_scaling"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image interpolation
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the one or more images in the optimal FRC operational mode.22. The method of claim 16, further comprising changing to a different FRC operational mode at a scene cut separating two adjacent scenes.23. The method of claim 16, further comprising determining whether to perform image interpolation with respect to the one or more images based at least in part on user preferences of a user to which the one or more images are to be rendered._______________SURVEILLANCE METHOD AND 
<a href="https://en.wikipedia.org/wiki/Computer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    COMPUTING DEVICE
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 USING THE SAME_____20190131_____XMLs/xml/ipa190131.xml_____US-20190035092-A1 : US-15661064_____G06T0007254000 : G06T0007246000A 
<a href="https://en.wikipedia.org/wiki/Computer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is able to detect one or more motion events based on two consecutive images, such as a first image and a second image. In the detection process, the 
<a href="https://en.wikipedia.org/wiki/Computer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 assigns identifiers to difference blocks retrieved from a plurality of first blocks of the first image, then defines a scanning window and moves the scanning window on a preset route over the first image. A new identical identifier is assigned for difference blocks within a current image subarea which falls into the scanning window. After a scanning period is completed, the 
<a href="https://en.wikipedia.org/wiki/Computer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 determines the happening of a motion event according to sufficient pixel similarities found in one of new identifiers._____d:FIELDThe subject matter herein generally relates to 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 technologies, and more particularly to a surveillance method and a 
<a href="https://en.wikipedia.org/wiki/Computer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 using the same.BACKGROUNDWith the development of science and technology, 
<a href="https://en.wikipedia.org/wiki/Motion_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 technologies are used in more situations, such as 
<a href="https://en.wikipedia.org/wiki/Home_security"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    home security
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, community security, or field birdwatching.However, 
<a href="https://en.wikipedia.org/wiki/Motion_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 based on the traditional technology needs large amount of computation, thereby greatly increasing workload of a reference 
<a href="https://en.wikipedia.org/wiki/Computer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. Therefore, it's necessary to provide a method for detecting a motion of a target object (e.g., human body) using fewer computation.BRIEF DESCRIPTION OF THE DRAWINGSImplementations of the present technology will be described, by way of example only, with reference to the attached figures, wherein:FIG. 1 illustrates a diagram showing a distribution of difference pixels in a first image;FIG. 2 illustrates a diagram showing a distribution of difference blocks in a first image;FIG. 3 illustrates a flowchart of an exemplary embodiment of a surveillance method;FIG. 4 illustrates a flowchart of an exemplary embodiment of step S10 in flowchart of FIG. 3;FIG. 5 illustrates a diagram showing a distribution of difference pixels in a part of the first image of FIG. 1, wherein FIG. 5 is the state of the part of the first image of FIG. 1 after being enlarged.FIG. 6 illustrates a 
<a href="https://en.wikipedia.org/wiki/Schematic"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    schematic diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 showing a distribution of identifiers in difference blocks in the part of the first image of FIG. 5;FIGS. 7-10 illustrate 
<a href="https://en.wikipedia.org/wiki/Schematic"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    schematic diagrams
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of an exemplary embodiment of processes of reassigning new identifiers in the difference blocks in the part of the first image of FIG. 5, during a scanning 
<a href="https://en.wikipedia.org/wiki/Period"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    period of
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method of FIG. 3;FIG. 11 illustrates a flowchart of an exemplary embodiment of step S10 of method in FIG. 3;FIG. 12 illustrates a 
<a href="https://en.wikipedia.org/wiki/Schematic"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    schematic diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 showing a distribution of new identifiers in the difference blocks in the part of the first image of FIG. 5, after the scanning period is completed in method of FIG. 3; andFIG. 13 illustrates an exemplary embodiment of functional modules of a 
<a href="https://en.wikipedia.org/wiki/Computer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 applying the method of FIG. 3._____c:1. A 
<a href="https://en.wikipedia.org/wiki/Computer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 comprising:at least one processor;a non-transitory 
<a href="https://en.wikipedia.org/wiki/Computer_data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    storage system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 coupled to the at least one processor and configured to store one or more programs to be executed by the at least one processor, the one or more programs including instructions for:retrieving a plurality of difference blocks from a plurality of first blocks of a first image by comparing the first image with a second image;assigning identifiers to the difference blocks, wherein adjacent difference blocks are assigned with an identical identifier;defining a scanning window and moving the scanning window on a preset route over the first image, reassigning a new identical identifier to difference blocks within a current image subarea which is falling into the scanning window, wherein the new identical identifier is selected from current identifiers of the difference blocks within the current image subarea according a preset rule;selecting a target identifier associating with a target object from the new identifiers and determining whether the amount of difference blocks associating with the target identifier exceeds a first preset value; andoutputting a motion event of the target object upon the condition that the amount of difference blocks associating with the target identifier exceeds the first preset value.2. The 
<a href="https://en.wikipedia.org/wiki/Computer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 1, the retrieving instruction further comprises:comparing each pixel of the first image with a corresponding pixel of the second image;retrieving difference pixels of the first image, wherein a difference between each difference pixel and corresponding pixel of the second image is greater than a second preset value; andretrieving the difference blocks from the first blocks, wherein the number of difference pixels of each difference block is greater than a third preset value.3. The 
<a href="https://en.wikipedia.org/wiki/Computer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 1, the moving instructions further comprise:(A). moving the scanning window from left to right based on a preset increment, until the scanning window touches a right edge of the first image;(B), resetting the scanning window on a left side edge of the first image and moving the scanning window down a preset increment;repeating (A) and (B) until a scanning period is completed.4. The 
<a href="https://en.wikipedia.org/wiki/Computer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 1, the reassigning instructions further comprise:selecting a smallest one of the current identifiers of the difference blocks within the current image subarea as the new identical identifier; andreassigning the new identical identifier to the difference blocks within the current image subarea.5. The 
<a href="https://en.wikipedia.org/wiki/Computer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computing device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 1, wherein the determining instruction further comprises:counting amounts, each of which indicates the number of difference blocks associating with corresponding one of the new identifiers; andselecting one new identifier with a maximum amount as the target identifiers, and determining whether the maximum amount exceeds the first preset value.6. A surveillance method comprising:retrieving a plurality of difference blocks from a plurality of first blocks of a first image by comparing the first image with a second image;assigning identifiers to the difference blocks, wherein adjacent difference blocks are assigned with an identical identifier;defining a scanning window and moving the scanning window on a preset route over the first image, reassigning a new identical identifier to difference blocks within a current image subarea which is falling into the scanning window, wherein the new identical identifier is selected from current identifiers of the difference blocks within the current image subarea according a preset rule;selecting a target identifier associating with a target object from the new identifiers and determining whether the amount of difference blocks associating with the target identifier exceeds a first preset value; andoutputting a motion event of the target object upon the condition that the amount of difference blocks associating with the target identifier exceeds the first preset value.7. The method of claim 6, the retrieving instruction further comprises:comparing each pixel of the first image with a corresponding pixel of the second image;retrieving difference pixels of the first image, wherein a difference between each difference pixel and corresponding pixel of the second image is greater than a second preset value; andretrieving the difference blocks from the first blocks, wherein the number of difference pixels of each difference block is greater than a third preset value.8. The method of claim 6, the moving instructions further comprise:(A). moving the scanning window from left to right based on a preset increment, until the scanning window touches a right edge of the first image;(B), resetting the scanning window on a left side edge of the first image and moving the scanning window down a preset increment;repeating (A) and (B) until a scanning period is completed.9. The method of claim 6, the reassigning instructions further comprise:selecting a smallest one of the current identifiers of the difference blocks within the current image subarea as the new identical identifier; andreassigning the new identical identifier to the difference blocks within the current image subarea.10. The method of claim 6, wherein the determining step further comprises:counting amounts, each of which indicates the number of difference blocks associating with corresponding one of the new identifiers; andselecting one new identifier with a maximum amount as the target identifiers, and determining whether the maximum amount exceeds the first preset value.11. A non-transitory 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    storage medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 storing 
<a href="https://en.wikipedia.org/wiki/Executable"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    executable program
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 instructions which, when executed by a processing system, cause the processing system to perform a method comprising:retrieving a plurality of difference blocks from a plurality of first blocks of a first image by comparing the first image with a second image;assigning identifiers to the difference blocks, wherein adjacent difference blocks are assigned with an identical identifier;defining a scanning window and moving the scanning window on a preset route over the first image, and when the scanning window is moved to an image subarea of the first image, reassigning a new identical identifier to difference blocks within the image subarea of the first image, wherein the new identical identifier is selected from current identifiers of the difference blocks within the image subarea of the first image according a preset rule;selecting a target identifier associating with a target object from the new identifiers and determining whether the amount of difference blocks associating with the target identifier exceeds a first preset value; andoutputting a motion event of the target object upon the condition that the amount of difference blocks associating with the target identifier exceeds the first preset value.12. The medium of claim 11, the retrieving instruction further comprises:comparing each pixel of the first image with a corresponding pixel of the second image;retrieving difference pixels of the first image, wherein a difference between each difference pixel and corresponding pixel of the second image is greater than a second preset value; andretrieving the difference blocks from the first blocks, wherein the number of difference pixels of each difference block is greater than a third preset value.13. The medium of claim 11, the moving instructions further comprise:(A). moving the scanning window from left to right based on a preset increment, until the scanning window touches a right edge of the first image;(B), resetting the scanning window on a left side edge of the first image and moving the scanning window down a preset increment;repeating (A) and (B) until a scanning period is completed.14. The medium of claim 11, the reassigning instructions further comprise:selecting a smallest one of the current identifiers of the difference blocks within the current image subarea as the new identical identifier; andreassigning the new identical identifier to the difference blocks within the current image subarea.15. The medium of claim 11, wherein the determining step further comprises:counting amounts, each of which indicates the number of difference blocks associating with corresponding one of the new identifiers; andselecting one new identifier with a maximum amount as the target identifiers, and determining whether the maximum amount exceeds the first preset value._______________OBJECT DISPLACEMENT DETECTION METHOD FOR DETECTING OBJECT DISPLACEMENT BY MEANS OF DIFFERENCE IMAGE DOTS_____20190207_____XMLs/xml/ipa190207.xml_____US-20190043206-A1 : US-15987876 : CN-201710657081.6_____G06T0007254000 : G06T0011200000 : G08B0013196000An object displacement detection method includes capturing n images of an object for obtaining n sets of image dots, where the object corresponds to an ith set of image dots in an ith image of the n images; performing (n−1) difference calculations using the n sets of image dots to obtain (n−1) sets of difference image dots, where a jth set of difference image dots of the (n−1) sets of difference image dots is generated by performing a jth difference calculation of the (n−1) difference calculations using a (j+1)th set of image dots and a jth set of image dots of the n sets of the image dots; and determining the object has displaced when a sum of numbers of the (n−1) sets of difference image dots reaches a first threshold._____d:BACKGROUND OF THE INVENTION1. Field of the InventionThe invention relates to an object displacement detection method, and more particularly, an object displacement detection method used for detecting object displacement by means of difference image dots.2. Description of the 
<a href="https://en.wikipedia.org/wiki/Prior_art"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Prior Art
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
Conventional security monitoring method can detect a large movement of an object. For example, a person who is walking or running can be detected. However, it is relatively difficult to detect a small movement of an object. For example, when an unknown person enters a monitored area, it is difficult for a conventional security method to detect the person's movement if the person intentionally keeps still. Taking a human body as an example, slight vibrations may still occur due to breathing or inevitable slight body movement even if the human body intentionally stays still. However, the probability of false alarms will greatly increase if sending an alarm for every detected slight vibration in a monitored area. Hence, most detected slight vibrations are filtered out as 
<a href="https://en.wikipedia.org/wiki/Background_noise"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background noise
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and the slight vibrations which should be alerted (e.g. the slight body vibrations of an intruder without consent) are likely to be neglected. By means of a specific method such as 
<a href="https://en.wikipedia.org/wiki/Eulerian_path"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Eulerian algorithm
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, transient changes of the position of an object in an image can be amplified to visualize the above mentioned slight movement. However, this sort of method relates to complex calculations and i
<a href="https://en.wikipedia.org/wiki/Mage:_The_Ascension"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mage processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 programs, and excessive resource of hardware and software will therefore be consumed. Hence, a better solution is still required in the field.SUMMARY OF THE INVENTIONAn embodiment provides an object displacement detection method. The method includes capturing n images of an object for obtaining n sets of image dots, where the object corresponds to an ith set of image dots in an ith image of the n images; performing (n−1) difference calculations using the n sets of image dots to obtain (n−1) sets of difference image dots, where a jth set of difference image dots of the (n−1) sets of difference image dots is generated by performing a jth difference calculation of the (n−1) difference calculations using a (j+1)th set of image dots and a jth set of image dots of the n sets of the image dots; and determining the object has displaced when a sum of numbers of the (n−1) sets of difference image dots reaches a first threshold.Another embodiment provides an object displacement detection system for detecting displacement of an object. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 includes an image capture unit, a memory and a processor. The image capture unit is used to capture n images of the object for obtaining n sets of image dots, where the object corresponds to an ith set of image dots in an ith image of the n images. The memory is coupled to the image capture unit and used to store then images and a first threshold. The processor is coupled to the memory and the image capture unit and used to perform (n−1) difference calculations using the n sets of image dots to obtain (n−1) sets of difference image dots and determine whether the object has displaced, where a jth set of difference image dots of the (n−1) sets of difference image dots is generated by the processor by performing a jth difference calculation of the (n−1) difference calculations using a (j+1)th set of image dots and a jth set of image dots of the n sets of the image dots, and the processor determines that the object has displaced when a sum of numbers of the (n−1) sets of difference image dots reaches a first threshold.These and other objectives of the present invention will no doubt become obvious to those of ordinary skill in the art after reading the following detailed description of the preferred embodiment that is illustrated in the various figures and drawings.BRIEF DESCRIPTION OF THE DRAWINGSFIG. 1 illustrates that a plurality of images of an object are captured according to an embodiment.FIG. 2 illustrates that a difference calculation is performed using two images according to an embodiment.FIG. 3 illustrates that an area is displayed on a visual display according to a plurality of sets of difference image dots according another embodiment.FIG. 4 illustrates an object displacement detection system according to an embodiment._____c:1. An object displacement detection method comprising:capturing n images of an object for obtaining n sets of image dots, wherein the object corresponds to an ith set of image dots in an ith image of the n images;performing (n−1) difference calculations using the n sets of image dots to obtain (n−1) sets of difference image dots, wherein a jth set of difference image dots of the (n−1) sets of difference image dots is generated by performing a jth difference calculation of the (n−1) difference calculations using a (j+1)th set of image dots and a jth set of image dots of the n sets of the image dots; anddetermining the object has displaced when a sum of numbers of the (n−1) sets of difference image dots reaches a first threshold;wherein i, n and j are positive integers, i≤n, and j+1≤n.2. The method of claim 1, wherein the n images are captured during a first 
<a href="https://en.wikipedia.org/wiki/Time"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time interval
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and the method further comprises:capturing m images during a second 
<a href="https://en.wikipedia.org/wiki/Time"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time interval
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 following the first 
<a href="https://en.wikipedia.org/wiki/Time"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time interval
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for obtaining m sets of image dots, wherein the object corresponds to a pth set of image dots in a pth image of the m images;performing (m−1) difference calculations using the m sets of image dots to obtain (m−1) sets of difference image dots, wherein a qth set of difference image dots of the (m−1) sets of difference image dots is generated by performing a qth difference calculation of the (m−1) difference calculations using a (q+1)th set of image dots and a qth set of image dots of the m sets of the image dots; anddetermining the object has not displaced during the second 
<a href="https://en.wikipedia.org/wiki/Time"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time interval
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 when a sum of numbers of the (m−1) sets of difference image dots fails to reach a second threshold;wherein p, m and q are positive integers, p≤m, and q+1≤m.3. The method of claim 1, further comprising:displaying at least one visual pattern on a visual display according to the (n−1) sets of difference image dots.4. The method of claim 3, wherein the visual pattern is displayed by displaying the (n−1) sets of difference image dots on the visual display.5. The method of claim 3, wherein displaying the at least one visual pattern on the visual display according to the (n−1) sets of difference image dots comprises:displaying k sets of difference image dots of the (n−1) sets of difference image dots on the visual display during a first 
<a href="https://en.wikipedia.org/wiki/Time"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time interval
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
; andeliminating the k sets of difference image dots from the visual display after the first 
<a href="https://en.wikipedia.org/wiki/Time"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time interval
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 has elapsed;wherein k is a positive integer and k≤(n−1).6. The method of claim 3, wherein displaying the at least one visual pattern on the visual display according to the (n−1) sets of difference image dots comprises:displaying an area on the visual display according to the (n−1) sets of difference image dots wherein the area highlights the object.7. The method of claim 3, wherein displaying the at least one visual pattern on the visual display according to the (n−1) sets of difference image dots comprises:displaying an area on the visual display according to k sets of difference image dots of the (n−1) sets of difference image dots during a first 
<a href="https://en.wikipedia.org/wiki/Time"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time interval
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, wherein the area highlights the object; andeliminating the area from the visual display after the first 
<a href="https://en.wikipedia.org/wiki/Time"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time interval
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 has elapsed;wherein k is a positive integer and k≤(n−1).8. The method of claim 3, wherein the n images are captured during a first 
<a href="https://en.wikipedia.org/wiki/Time"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time interval
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and the method further comprises:capturing m images during a second 
<a href="https://en.wikipedia.org/wiki/Time"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time interval
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 following the first 
<a href="https://en.wikipedia.org/wiki/Time"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time interval
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for obtaining m sets of image dots, wherein the object corresponds to a pth set of image dots in a pth image of the m images;performing (m−1) difference calculations using the m sets of image dots to obtain (m−1) sets of difference image dots, wherein a qth set of difference image dots of the (m−1) sets of difference image dots is generated by performing a qth difference calculation of the (m−1) difference calculations using a (q+1)th set of image dots and a qth set of image dots of the m sets of the image dots;determining the object has not displaced during the second 
<a href="https://en.wikipedia.org/wiki/Time"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time interval
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 when a sum of numbers of the (m−1) sets of difference image dots fails to reach a second threshold; andeliminating the visual pattern from the visual display after determining the object has not displaced during the second 
<a href="https://en.wikipedia.org/wiki/Time"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time interval
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
;wherein p, m and q are positive integers, p≤m, and q+1≤m.9. An object displacement detection system for detecting displacement of an object, comprising:an image capture unit configured to capture n images of the object for obtaining n sets of image dots, wherein the object corresponds to an ith set of image dots in an ith image of the n images;a memory coupled to the image capture unit and configured to store the n images and a first threshold; anda processor coupled to the memory and the image capture unit and configured to perform (n−1) difference calculations using the n sets of image dots to obtain (n−1) sets of difference image dots and determine whether the object has displaced, wherein a jth set of difference image dots of the (n−1) sets of difference image dots is generated by the processor by performing a jth difference calculation of the (n−1) difference calculations using a (j+1)th set of image dots and a jth set of image dots of the n sets of the image dots, and the processor determines that the object has displaced when a sum of numbers of the (n−1) sets of difference image dots reaches a first threshold;wherein i, n and j are positive integers, i≤n, and j+1≤n.10. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 9 further comprising:a 
<a href="https://en.wikipedia.org/wiki/Display_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    display device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 coupled to the memory and the processor wherein the processor controls the 
<a href="https://en.wikipedia.org/wiki/Display_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    display device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to display at least one visual pattern according to the (n−1) sets of difference image dots.11. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 9, wherein:the image capture unit is further configured to capture m images during a 
<a href="https://en.wikipedia.org/wiki/Time"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time interval
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for obtaining m sets of image dots corresponding to the object, wherein the object corresponds to a pth set of image dots in a pth image of the m images;the memory is further configured to store the 
<a href="https://en.wikipedia.org/wiki/Time"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time interval
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, the m images and a second threshold;the processor is further configured to perform (m−1) difference calculations using the m sets of image dots to obtain (m−1) sets of difference image dots, wherein a qth set of difference image dots of the (m−1) sets of difference image dots is generated by performing a qth difference calculation of the (m−1) difference calculations using a (q+1)th set of image dots and a qth set of image dots of the m sets of the image dots; and the processor determines that the object has not displaced during the 
<a href="https://en.wikipedia.org/wiki/Time"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time interval
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 when a sum of numbers of the (m−1) sets of difference image dots fails to reach a second threshold;wherein p, m and q are positive integers, p≤m and q+1≤m.12. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 11, further comprising:a 
<a href="https://en.wikipedia.org/wiki/Display_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    display device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 coupled to the memory and the processor;wherein the processor controls the 
<a href="https://en.wikipedia.org/wiki/Display_device"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    display device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to display the at least one visual pattern when the processor determines the object has displaced according to the (n−1) sets of difference image dots, and the processor controls the display device to eliminate the at least one visual pattern when the processor determines the object has not displaced during the 
<a href="https://en.wikipedia.org/wiki/Time"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time interval
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to the (m−1) sets of difference image dots._______________LEARNING RIGIDITY OF DYNAMIC SCENES FOR THREE-DIMENSIONAL SCENE FLOW ESTIMATION_____20190221_____XMLs/xml/ipa190221.xml_____US-20190057509-A1 : US-16052528 : US-62546442_____G06T0007254000 : G06T0007900000 : G06T0007500000 : G06N0003080000 : G06N0005040000 : G06T0003000000 : G06T0007700000 : G06T0007600000 : G06T0007110000 : G06T0007194000A 
<a href="https://en.wikipedia.org/wiki/Artificial_neural_network"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    neural network model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 receives color data for a sequence of images corresponding to a dynamic scene in three-dimensional (3D) space. Motion of objects in the image sequence results from a combination of a 
<a href="https://en.wikipedia.org/wiki/High_Dynamic_Range_(photography_technique)"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    dynamic camera
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 orientation and motion or a change in the shape of an object in the 3D space. The 
<a href="https://en.wikipedia.org/wiki/Artificial_neural_network"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    neural network model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 generates two components that are used to produce a 3D 
<a href="https://en.wikipedia.org/wiki/Motion_field"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion field
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 representing the dynamic (non-rigid) part of the scene. The two components are information identifying dynamic and static portions of each image and the camera orientation. The dynamic portions of each image contain motion in the 3D space that is independent of the camera orientation. In other words, the motion in the 3D space (estimated 3D scene flow data) is separated from the motion of the camera._____d:CLAIM OF PRIORITYThis application claims the benefit of U.S. Provisional Application No. 62/546,442 (Attorney Docket No. NVIDP1184+/17SC0167US01) titled “Learning-Based 3D 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Scene Flow Estimation
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 with RGBD Images,” filed Aug. 16, 2017, the entire contents of which are incorporated herein by reference.FIELD OF THE INVENTIONThe present invention relates to three-dimensional (3D) flow estimation for images, and more particularly to learning rigidity of dynamic scenes by a 
<a href="https://en.wikipedia.org/wiki/Neural_network"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    neural network
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.BACKGROUNDThe estimation of 3D motion from images is a fundamental 
<a href="https://en.wikipedia.org/wiki/Computer_vision"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer vision
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 problem, and key to many applications such as 
<a href="https://en.wikipedia.org/wiki/Robotics"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    robot manipulation
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, dynamic scene reconstruction, autonomous driving, 
<a href="https://en.wikipedia.org/wiki/Activity_recognition"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    action recognition
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and 
<a href="https://en.wikipedia.org/wiki/Video_content_analysis"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video analysis
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. The task of estimating 3D motion is commonly referred as 3D 
<a href="https://en.wikipedia.org/wiki/Motion_field"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion field
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 or scene flow estimation. 3D 
<a href="https://en.wikipedia.org/wiki/Motion_field"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion field
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 estimation in a 
<a href="https://en.wikipedia.org/wiki/Environment_variable"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    dynamic environment
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is, however, a challenging and still 
<a href="https://en.wikipedia.org/wiki/Open_problem"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    open problem
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 when the scene is observed from different viewpoints and the number of moving objects (either rigid or non-rigid) or movement of a single object in each image is large. The difficulty is mainly because the disambiguation of camera motion (ego-motion) from object motion requires the correct identification of rigid static structure of a scene. Existing approaches suffer from demanding computational expenses and often fail when a scene includes multiple moving objects in the foreground. There is a need for addressing these issues and/or other issues associated with the 
<a href="https://en.wikipedia.org/wiki/Prior_art"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    prior art
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.SUMMARYA method, 
<a href="https://en.wikipedia.org/wiki/Machine-readable_medium"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer readable
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 medium, and system are disclosed for learning rigidity of dynamic scenes by a 
<a href="https://en.wikipedia.org/wiki/Neural_network"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    neural network
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. Color data are received for a sequence of images corresponding to a dynamic scene in three-dimensional (3D) space including a first image and a second image, where the first image is captured from a first viewpoint and the second image is captured from a second viewpoint. The color data are processed by layers of a 
<a href="https://en.wikipedia.org/wiki/Artificial_neural_network"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    neural network model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to generate segmentation data indicating a portion of the second image where a first object changes position or shape relative the first object in the first image.BRIEF DESCRIPTION OF THE DRAWINGSFIG. 1A illustrates 2D 
<a href="https://en.wikipedia.org/wiki/Flow_Motion"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for a static scene resulting from camera motion, in accordance with an embodiment.FIG. 1B illustrates 2D motion flow resulting from scene motion for a static camera, in accordance with an embodiment.FIGS. 1C and 1D illustrate 2D motion flow resulting from scene motion and camera motion, in accordance with an embodiment.FIG. 1E illustrates a flowchart of a method for generating a rigidity mask for 3D 
<a href="https://en.wikipedia.org/wiki/Motion_field"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion field
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 estimation, in accordance with an embodiment.FIG. 2A illustrates a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a 3D 
<a href="https://en.wikipedia.org/wiki/Motion_field"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion field estimation
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system, in accordance with an embodiment.FIG. 2B illustrates a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the rigidity transform 
<a href="https://en.wikipedia.org/wiki/Artificial_neural_network"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    neural network model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from FIG. 2A, in accordance with an embodiment.FIG. 2C illustrates another 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the rigidity transform 
<a href="https://en.wikipedia.org/wiki/Artificial_neural_network"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    neural network model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from FIG. 2A, in accordance with an embodiment.FIG. 2D illustrates a flowchart of a method for training the rigidity transform 
<a href="https://en.wikipedia.org/wiki/Artificial_neural_network"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    neural network model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, in accordance with an embodiment.FIG. 3 illustrates a 
<a href="https://en.wikipedia.org/wiki/Parallel_computing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    parallel processing unit
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, in accordance with an embodiment.FIG. 4A illustrates a 
<a href="https://en.wikipedia.org/wiki/Process"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    general processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 cluster within the 
<a href="https://en.wikipedia.org/wiki/Parallel_computing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    parallel processing unit
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of FIG. 3, in accordance with an embodiment.FIG. 4B illustrates a memory partition unit of the 
<a href="https://en.wikipedia.org/wiki/Parallel_computing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    parallel processing unit
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of FIG. 3, in accordance with an embodiment.FIG. 5A illustrates the streaming multi-processor of FIG. 4A, in accordance with an embodiment.FIG. 5B is a 
<a href="https://en.wikipedia.org/wiki/Concept_map"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    conceptual diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a processing system implemented using the PPU of FIG. 3, in accordance with an embodiment.FIG. 5C illustrates an exemplary system in which the various architecture and/or functionality of the various previous embodiments may be implemented._____c:1. A computer-implemented method, comprising:receiving color data for a sequence of images corresponding to a dynamic scene in three-dimensional (3D) space including a first image and a second image, wherein the first image is captured from a first viewpoint and the second image is captured from a second viewpoint; andprocessing the color data by layers of a 
<a href="https://en.wikipedia.org/wiki/Artificial_neural_network"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    neural network model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to generate segmentation data indicating a portion of the second image where a first object changes position or shape relative the first object in the first image.2. The computer-implemented method of claim 1, further comprising processing the color data by the layers of the 
<a href="https://en.wikipedia.org/wiki/Artificial_neural_network"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    neural network model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to produce a pose of the second viewpoint, the pose including a position and orientation in the 
<a href="https://en.wikipedia.org/wiki/Three-dimensional_space"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    3D space
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.3. The computer-implemented method of claim 2, further comprising:warping the pose to generate 2D viewpoint 
<a href="https://en.wikipedia.org/wiki/Flow_Motion"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 data for the second image; andsubtracting the 2D viewpoint motion flow data from two-dimensional 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 data for the sequence of images to produce estimated projected 3D scene flow data for the second image.4. The computer-implemented method of claim 2, further comprising refining the pose based on two-dimensional 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 data for the sequence of images.5. The computer-implemented method of claim 1, further comprising refining the segmentation data based on two-dimensional 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 data for the sequence of images.6. The computer-implemented method of claim 1, further comprising:receiving 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for the sequence of images; andprocessing the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 with the color data to generate the segmentation data.7. The computer-implemented method of claim 1, further comprising:processing the sequence of images to extract depth data; andprocessing the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 with the color data to generate the segmentation data.8. The computer-implemented method of claim 1, further comprising training the 
<a href="https://en.wikipedia.org/wiki/Artificial_neural_network"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    neural network model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 using a dataset including a first image sequence for 
<a href="https://en.wikipedia.org/wiki/Viewpoints"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    viewpoint motion
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and a static scene, a second image sequence for scene motion and a static viewpoint, and a third image sequence for simultaneous viewpoint motion and scene motion.9. The computer-implemented method of claim 8, wherein a portion of the dataset includes a real background scene and synthetic foreground objects.10. The computer-implemented method of claim 1, wherein the segmentation data is a mask comprising a single bit for each pixel in the second image.11. The computer-implemented method of claim 1, wherein the layers of the 
<a href="https://en.wikipedia.org/wiki/Artificial_neural_network"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    neural network model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 comprise one or more convolutional layers followed by one or more deconvolutional layers.12. A system, comprising:a processing unit configured to:receive color data for a sequence of images corresponding to a dynamic scene in three-dimensional (3D) space including a first image and a second image, wherein the first image is captured from a first viewpoint and the second image is captured from a second viewpoint; andprocess the color data by layers of a 
<a href="https://en.wikipedia.org/wiki/Artificial_neural_network"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    neural network model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to generate segmentation data indicating a portion of the second image where a first object changes position or shape relative the first object in the first image.13. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 12, wherein the processing unit is further configured to process the color data by the layers of the 
<a href="https://en.wikipedia.org/wiki/Artificial_neural_network"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    neural network model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to produce a pose of the second viewpoint, the pose including a position and orientation in the 3D space.14. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 13, wherein the processing unit is further configured to:warp the pose to generate 2D viewpoint 
<a href="https://en.wikipedia.org/wiki/Flow_Motion"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 data for the second image; andsubtract the 2D viewpoint motion flow data from two-dimensional 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 data for the sequence of images to produce estimated projected 3D scene flow data for the second image.15. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 14, wherein the processing unit is further configured to refine the pose based on two-dimensional 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 data for the sequence of images.16. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 12, wherein the processing unit is further configured to refine the segmentation data based on two-dimensional 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 data for the sequence of images.17. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 12, wherein the processing unit is further configured to:receive 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for the sequence of images; andprocess the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 with the color data to generate the segmentation data.18. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 12, wherein the processing unit is further configured to train the 
<a href="https://en.wikipedia.org/wiki/Artificial_neural_network"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    neural network model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 using a dataset including a first image sequence for 
<a href="https://en.wikipedia.org/wiki/Viewpoints"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    viewpoint motion
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and a static scene, a second image sequence for scene motion and a static viewpoint, and a third image sequence for simultaneous viewpoint motion and scene motion.19. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 12, wherein the segmentation data is a mask comprising a single bit for each pixel in the second image.20. A non-transitory, computer-readable 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    storage medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 storing instructions that, when executed by a processing unit, cause the processing unit to:receive color data for a sequence of images corresponding to a dynamic scene in three-dimensional (3D) space including a first image and a second image, wherein the first image is captured from a first viewpoint and the second image is captured from a second viewpoint; andprocess the color data by layers of a 
<a href="https://en.wikipedia.org/wiki/Artificial_neural_network"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    neural network model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to generate segmentation data indicating a portion of the second image where a first object changes position or shape relative the first object in the first image._______________
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    IMAGE PROCESSING
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 APPARATUS, 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    IMAGE PROCESSING
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 METHOD, AND STORAGE MEDIUM_____20190801_____XMLs/xml/ipa190801.xml_____US-20190236791-A1 : US-16260806 : JP-2018-014188_____G06T0007254000 : G06T0007215000 : G08B0013196000 : H04N0005232000 : H04N0005272000An 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus includes a detection unit configured to detect a first region in which a foreground object is present with respect to a plurality of captured images, a retention unit configured to retain a first background image, a generation unit configured to generate a second background image based on portions of each of the plurality of captured images which are not detected as a first region, and an output unit configured to select one of the first background image and the second background image based on a property of the second background image, and configured to output, based on the selected background image and the first region, an image in which the foreground object is obscured._____d:BACKGROUND OF THE INVENTIONField of the InventionAspects of the present invention generally relate to an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus, an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method, and a 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    storage medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 each of which hides a specific object, such as a person, from a captured image so as to protect privacy.Description of the Related ArtIn recent years, the importance of privacy protection for an individual whose image is captured by a monitoring camera has been increasing. Therefore, there is a technique to use a background image so as to detect a region in which to protect privacy. For example, there is a method of previously acquiring, as a background image, an image captured at timing when no foreground is shown in the image, comparing a processing target image with the background image, and performing concealment processing on a specific region of the processing target image based on a result of comparison, thus protecting privacy. A technique discussed in Japanese Patent Application Laid-Open No. 2016-115214 detects a human body or moving object included in a captured image and performs processing in such a way as to update a background image based on the detection thereof, thus increasing the accuracy of concealment processing.In the technique of acquiring, as a background image, an image captured at timing when no foreground is shown in the image and using the background image in a fixed manner, if, for example, a large change occurs in the luminance of an image capturing environment, a comparison with the background image may sometimes cause a region to be excessively extracted as a foreground. On the other hand, in the technique discussed in Japanese Patent Application Laid-Open No. 2016-115214, while regions other than human body or moving object regions included in an image capturing range are combined to successively update a background image, depending on an environment in which image capturing is performed, a region which is always determined to be a moving object region occurs, so that the background image may enter a hole-like defective state. Here, a hole-like defective state is a state where the moving object region is removed from the background image. It may be unfavorable to use such a hole-like defective image as a background image for 
<a href="https://en.wikipedia.org/wiki/Foreground_and_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    foreground extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 processing or privacy protection processing.SUMMARY OF THE INVENTIONAccording to an aspect of the present invention, an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus includes a detection unit configured to detect a first region in which a foreground object is present with respect to a plurality of captured images, a retention unit configured to retain a first background image, a generation unit configured to generate a second background image based on portions of each of the plurality of captured images which are not detected as a first region, and an output unit configured to select one of the first background image and the second background image based on a property of the second background image, and configured to output, based on the selected background image and the first region, an image in which the foreground object is obscured.Further features of the present invention will become apparent from the following description of exemplary embodiments with reference to the attached drawings.BRIEF DESCRIPTION OF THE DRAWINGSFIG. 1A is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating a hardware configuration of each apparatus of an 
<a href="https://en.wikipedia.org/wiki/Image_processor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and FIG. 1B is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating a schematic configuration of the 
<a href="https://en.wikipedia.org/wiki/Image_processor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.FIG. 2 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating a functional configuration of each apparatus.FIGS. 3A, 3B, 3C, and 3D are 
<a href="https://en.wikipedia.org/wiki/Schematic"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    schematic diagrams
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating an outline of generation processing for a background image.FIG. 4 is a flowchart illustrating the generation processing for a background image.FIG. 5A is a flowchart illustrating generation processing for a privacy protection image, and FIG. 5B is a diagram illustrating an example of a 
<a href="https://en.wikipedia.org/wiki/User_interface"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    user interface
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
._____c:1. An 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus comprising:a detection unit configured to detect a first region in which a foreground object is present with respect to a plurality of captured images;a retention unit configured to retain a first background image;a generation unit configured to generate a second background image based on portions of each of the plurality of captured images which are not detected as a first region; andan output unit configured to select one of the first background image and the second background image based on a property of the second background image, and configured to output, based on the selected background image and the first region, an image in which the foreground object is obscured.2. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the property of the second background image used by the output unit to select one of the first background image and the second background image is a proportion of a number of pixels of the second background image to a number of pixels of the captured image3. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the property of the second background image used by the output unit to select one of the first background image and the second background image is an area ratio of the second background image to an angular field of the captured image.4. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the detection unit is configured to detect an object having a predetermined attribute as the foreground object.5. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the detection unit is configured to detect a foreground object included in the plurality of captured images based on the first background image.6. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the first region is defined as a rectangular region which includes the foreground object.7. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1,wherein the detection unit is configured to detect the first region in each of a first captured image and a second captured image, andwherein the generation unit is configured to generate the second background image using, with respect to a second region in which the first region of the first captured image and the first region of the second captured image overlap, pixel values of a region corresponding to the second region included in the second captured image.8. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1,wherein the detection unit is configured to detect a first region in each of a first captured image and a second captured image, andwherein the generation unit is configured to generate the second background image based on, with respect to a second region in which the first region of the first captured image and the first region of the second captured image overlap, both pixel values of a region corresponding to the second region included in the first captured image and pixel values of a region corresponding to the second region included in the second captured image.9. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the generation unit is configured to determine whether to use a further captured image for composite generation of the second background image based on, with respect to the second background image generated based on the plurality of images, a score determined based on a proportion of a region in which pixel values are not acquired from the plurality of images.10. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the generation unit is configured to update the generated second background image with use of a newly captured image.11. The 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, further comprising an acquisition unit configured to acquire the plurality of captured images.12. An 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method comprising:detecting a first region in which a foreground object is present with respect to a plurality of captured images;retaining a first background image;generating a second background image based on portions of each of the plurality of captured images which are not detected as a first region; andselecting one of the first background image and the second background image based on a property of the second background image, and outputting, based on the selected background image and the detected first region, an image in which the foreground object is obscured.13. A non-transitory computer-readable 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    storage medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 storing computer-executable instructions that, when executed by a computer, cause the computer to perform a method comprising:detecting a first region in which a foreground object is present with respect to a plurality of captured images;retaining a first background image;generating a second background image based on portions of each of the plurality of captured images which are not detected as a first region; andselecting one of the first background image and the second background image based on a property of the second background image, and outputting, based on the selected background image and the detected first region, an image in which the foreground object is obscured._______________DETECTION SYSTEM, DETECTION METHOD, AND 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    PROGRAM STORAGE MEDIUM
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
_____20170629_____XMLs/xml/ipa170629.xml_____US-20170186179-A1 : US-15315413 : JP-2014-115207 : WO-PCT/JP2015/002775-00_____G06T0007254000 : G06T0007215000A detection system which detects a 
<a href="https://en.wikipedia.org/wiki/Mobile_object"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile object
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 includes: an image input unit for receiving an input of a plurality of image frames having different capturing times; an inter-background model distance calculation unit for calculating differences between a first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 generated based on an image frame at the time of processing, a second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 in which an influence of an image frame at the time of processing is smaller than that of the first background model, and a third background model in which an influence of an image frame at the time of processing is smaller than that of the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
; and a 
<a href="https://en.wikipedia.org/wiki/Lidar"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 unit for detecting a first region in an image frame._____d:TECHNICAL FIELDSome aspects of the present invention relate to an 
<a href="https://en.wikipedia.org/wiki/Image_processor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method, and a 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    program storage medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.BACKGROUND ARTIn recent years, in the application of video surveillance or the like, needs for detecting and tracking a 
<a href="https://en.wikipedia.org/wiki/Mobile_object"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile object
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 such as a person or 
<a href="https://en.wikipedia.org/wiki/Vehicle"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    a vehicle
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 are increasing. With such increasing needs, many techniques for detecting a 
<a href="https://en.wikipedia.org/wiki/Hardlink"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile objec
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
t and tracking the detected 
<a href="https://en.wikipedia.org/wiki/Mobile_object"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile object
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 have been proposed. A mobile object herein is not limited to an object which continues to move among objects appeared on an image, and also includes an object which “temporarily stops” (also referred to as “rests” or “loiters”). In other words, a 
<a href="https://en.wikipedia.org/wiki/Mobile_object"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile object
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 generally means an object appeared on an image except a portion regarded as a background. For example, a person or 
<a href="https://en.wikipedia.org/wiki/Vehicle"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    a vehicle
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 which is a common target to be monitored by video surveillance is moving not all the time, but has a state of resting such as temporarily stopping or parking. For this reason, it is important in applications such as 
<a href="https://en.wikipedia.org/wiki/Closed-circuit_television"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video surveillance
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 that an object can be detected even when the object temporarily stops.As a method of detecting a 
<a href="https://en.wikipedia.org/wiki/Mobile_object"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile object
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, a 
<a href="https://en.wikipedia.org/wiki/Difference_and_Repetition"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background difference
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method is known (see, for example, Non Patent Literature 1 and Non Patent Literature 2). The background difference method is a method in which an image stored as a background is compared with an image captured by a camera to extract a region having a difference as a 
<a href="https://en.wikipedia.org/wiki/Mobile_object"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile object
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. Here, when the 
<a href="https://en.wikipedia.org/wiki/Mobile_object"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile object
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is detected by using a 
<a href="https://en.wikipedia.org/wiki/Difference_and_Repetition"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background difference
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, an accurate background extraction is required at the time of analysis. This is because, when data at the start of measurement is simply used as a background fixedly, many error detections occur, caused by influence of a change of the background due to an environmental change such as a change of illumination. Accordingly, in order to avoid such problems, usually, a background at the time of analysis is performed by a method such as calculating a mean value for each pixel from images observed within the latest time period. For example, Non Patent Literature 1 discloses a method of applying a background difference method while performing an update of a background successively.On the other hand, there is also a technique in which only an object which temporarily rests such as a left object or a person who loiters for a predetermined time is extracted (see, for example, Patent Literature 1). Patent Literature 1 discloses a method in which a motion in a scene is analyzed by a plurality of background models having different time spans. In the method, a long-term background model which is analyzed using a long time range and a short-term background model which is analyzed using a short time range are generated. When a 
<a href="https://en.wikipedia.org/wiki/Mobile_object"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile object
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is not detected by the 
<a href="https://en.wikipedia.org/wiki/Difference_and_Repetition"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background difference
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 based on the short-term 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and is detected by the background difference based on the long-term background model for a predetermined times, the 
<a href="https://en.wikipedia.org/wiki/Mobile_object"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile object
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is then detected as being a temporarily stationary object.CITATION LISTPatent Literature[PTL 1] Patent No. 5058010Non Patent Literature[NPL 1] KAWABATA ATSUSHI, TANIFUJI SHINYA, MOROOKA YASUO. “An 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Image Extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 Method for 
<a href="https://en.wikipedia.org/wiki/Moving_object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Moving Object
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
”, 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Information Processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 Society of Japan, vol.28, no.4, pp.395-402, 1987[NPL 2] 
<a href="https://en.wikipedia.org/wiki/Gordon_C._Stauffer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    C. Stauffer
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and W. E. L. Grimson, “Adaptive background mixture models for real-time tracking”, Proceedings of CVPR, vol.2, pp. 246-252, 1999SUMMARY OF INVENTIONTechnical ProblemAs described in Non Patent Literature 1, a case in which a 
<a href="https://en.wikipedia.org/wiki/Mobile_object"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile object
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 such as a person or 
<a href="https://en.wikipedia.org/wiki/Vehicle"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    a vehicle
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 stays for a longer time than a time span for analyzing a background image in a method of extracting a difference between a successively updated background image and an image to be analyzed will be considered. In this case, there is a problem that the 
<a href="https://en.wikipedia.org/wiki/Mobile_object"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile object
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 cannot be detected since it is determined as a portion of a background image. On the other hand, when a time span for analyzing is increased for detecting a temporarily stationary object, the analysis is likely to be influenced by a change of a background due to an external noise such as illumination fluctuation, and therefore, there arises a problem that a temporary change of a background image other than the stationary object is often erroneously detected.Patent Literature 1 aims at detecting a temporarily stationary object on the assumption that a 
<a href="https://en.wikipedia.org/wiki/Difference_and_Repetition"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background difference
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 based on a long-term background model can express a true background at the time of obtaining an observed image. For this reason, it has been difficult to sufficiently suppress error detections in an environment in which a background gradually changes such as illumination fluctuation since there is a large difference from a true background at the time of obtaining an observed image in a long-term background model.Some aspects of the present invention have been made in view of the above-described problems, and an object of the present invention is provide a detection system, a detection method, and a 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    program storage medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 which can preferably detect a 
<a href="https://en.wikipedia.org/wiki/Mobile_object"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile object
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.Solution to ProblemA detection system of the present invention includes: input means for receiving an input of a plurality of image frames having different capturing times;calculation means for calculating differences between a first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 generated based on an image frame at the time of processing, a second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 in which an influence of an image frame at the time of processing is sma
<a href="https://en.wikipedia.org/wiki/Hulkoff"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    ller than
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 that of the first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and a third background model in which an influence of an image frame at the time of processing is smaller than that of the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
; anddetect means for detecting a first region in an image frame in which a difference between the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the third 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is not less than a first threshold, and a difference between the first background model and the third 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is not less than second threshold times a difference between the first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.A detection method of the present invention by a computer, includes:receiving an input of a plurality of image frames having different capturing times;calculating differences between a first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 generated based on an image frame at the time of processing, a second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 in which an influence of an image frame at the time of processing is smaller than that of the first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and a third background model in which an influence of an image frame at the time of processing is smaller than that of the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
; anddetecting a first region in an image frame in which a difference between the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the third 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is not less than a first threshold, and a difference between the first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the third back
<a href="https://en.wikipedia.org/wiki/Two-ray_ground-reflection_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    ground model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is not less than second threshold times a difference between the first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.A 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    program storage medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the present invention for storing a program causing a computer to executea processing of receiving an input of a plurality of image frames having different capturing times;a processing of calculating differences between a first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 generated based on an image frame at the time of processing, a second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 in which an influence of an image frame at the time of processing is smaller than that of the first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and a third background model in which an influence of an image frame at the time of processing is smaller than that of the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
; anda processing of detecting a first region in an image frame in which a difference between the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the third 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is not less than a first threshold, and a difference between the first background model and the third 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is not less than second threshold times a difference between the first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.In the present invention, a “unit”, “means”, “apparatus”, or a “system” does not simply means a physical means, and also includes a software realizing a function of the “unit”, “means”, “apparatus”, or “system”. A function of one “unit”, “means”, “apparatus”, or “system” may be realized by two or more physical means or apparatuses, or two or more functions of a “unit”, “means”, “apparatus”, or a “system” may be realized by one physical means or apparatus.Advantageous Effects of InventionAccording to the present invention, a detection system, a detection method, and a 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    program storage medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 which can preferably detect a 
<a href="https://en.wikipedia.org/wiki/Mobile_object"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    mobile object
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 can be provided.BRIEF DESCRIPTION OF DRAWINGSFIG. 1 is a diagram illustrating a relationship between a 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and an input image frame.FIG. 2 is a 
<a href="https://en.wikipedia.org/wiki/Functional_block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    functional block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 schematically illustrating a detection system according to a first example embodiment.FIG. 3 is a flow chart illustrating a processing flow of a detection system illustrated in FIG. 2.FIG. 4 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating a hardware configuration which can implement a detection system shown in FIG. 2.FIG. 5 is a 
<a href="https://en.wikipedia.org/wiki/Functional_block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    functional block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 schematically illustrating a detection system according to a second example embodiment._____c:1. A detection system comprising:one or more processors acting as input unit configured to receive an input of a plurality of image frames having different capturing times;the one or more processors acting as calculation unit configured to calculate differences between a first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 generated based on an image frame at the time of processing, a second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 in which an influence of an image frame at the time of processing is smaller than that of the first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and a third background model in which an influence of an image frame at the time of processing is smaller than that of the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
; andthe one or more processors acting as a detect unit configured to detect a first region in an image frame in which a difference between the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the third 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is not less than a first threshold, and a difference between the first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the third 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is not less than second threshold times a difference between the first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.2. The detection system according to claim 1, whereinthe first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and the third background model have a different time span of capturing of image frames to be considered.3. The detection system according to claim 1, whereinThe one or more processors acting as detect unit detects a second region in the image frame in which a difference between the first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is not less than a third threshold.4. The detection system according to claim 1, the one or more processors further acting as output unit configured to output the first region and the second region by discriminating them.5. The detection system according to claim 1, whereinan influence of an image frame at the processing time in the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is changeable.6. The detection system according to claim 5, whereinan influence that the first region in an image frame at the processing time has on the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is smaller than an influence that other regions have on the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.7. A detection method by a computer, comprising:receiving an input of a plurality of image frames having different capturing times;calculating differences between a first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 generated based on an image frame at the time of processing, a second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 in which an influence of an image frame at the time of processing is smaller than that of the first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and a third background model in which an influence of an image frame at the time of processing is smaller than that of the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
; anddetecting a first region in an image frame in which a difference between the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the third 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is not less than a first threshold, and a difference between the first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the third back
<a href="https://en.wikipedia.org/wiki/Two-ray_ground-reflection_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    ground model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is not less than second threshold times a difference between the first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.8. A computer-readable non-transitory 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    storage medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for storing a program causing a computer to execute function of:receiving an input of a plurality of image frames having different capturing times;calculating differences between a first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 generated based on an image frame at the time of processing, a second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 in which an influence of an image frame at the time of processing is smaller than that of the first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, and a third background model in which an influence of an image frame at the time of processing is smaller than that of the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
; anddetecting a first region in an image frame in which a difference between the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the third 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is not less than a first threshold, and a difference between the first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the third back
<a href="https://en.wikipedia.org/wiki/Two-ray_ground-reflection_model"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    ground model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is not less than second threshold times a difference between the first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.9. The detection system according to claim 2, whereinthe one or more processors acting as detect unit detects a second region in the image frame in which a difference between the first 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is not less than a third threshold.10. The detection system according to claim 2, the one or more processors further acting as output unit configured to output the first region and the second region by discriminating them.11. The detection system according to claim 3, the one or more processors further acting as output unit configured to output the first region and the second region by discriminating them.12. The detection system according to claim 2, whereinan influence of an image frame at the processing time in the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is changeable.13. The detection system according to claim 3, whereinan influence of an image frame at the processing time in the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is changeable.14. The detection system according to claim 4, whereinan influence of an image frame at the processing time in the second 
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is changeable._______________OBJECT DETECTION DEVICE AND OBJECT DETECTION METHOD_____20180719_____XMLs/xml/ipa180719.xml_____US-20180204333-A1 : US-15744026 : WO-PCT/JP2015/083880-00_____G06T0007254000 : G06K0009000000 : G06T0007110000An 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device includes: an optical flow calculator to calculate an optical flow between images captured by the image capturer at different times; an evaluation value calculator to divide the image captured by the image capturer into areas, and calculate, for each divided area, an evaluation value by using the optical flows of pixels belonging to the divided area, the evaluation value indicating a measure of a possibility that the divided area is an object area representing part or whole of the object to be detected; and an area determinator to determine an area in an image, in which the object to be detected exists, by comparing the evaluation value of each divided area calculated by the evaluation value calculator with a 
<a href="https://en.wikipedia.org/wiki/Critical_value"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    threshold value
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
._____d:TECHNICAL FIELDThe present invention relates to an 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device and an object detection method for detecting an area in an image, in which an object to be detected exists.BACKGROUND ARTProcessing of detecting an object, such as a person or a car, from an image captured by a camera is an important technology applied to a vision sensor for robot or vehicle, an image monitoring system, or the like.In detecting a desired object, discrimination processing, such as 
<a href="https://en.wikipedia.org/wiki/Pattern_recognition"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    pattern recognition
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 processing based on 
<a href="https://en.wikipedia.org/wiki/Machine_learning"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    machine learning
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, is often used.Specifically, a window that indicates a local area having an appropriate size is cut out from frames of images continuously captured by a camera, discrimination processing, such as 
<a href="https://en.wikipedia.org/wiki/Pattern_recognition"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    pattern recognition
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 processing, for the image in the window is executed, and it is determined whether an object exists within the window. By these processing, an area in the image in which the object to be detected exists is detected.It is known that the discrimination processing, such as the 
<a href="https://en.wikipedia.org/wiki/Pattern_recognition"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    pattern recognition
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 processing, has a large amount of operation. In general, a position where an object exists in each frame of an image and a size of the object are unknown. Therefore, the discrimination processing, such as the 
<a href="https://en.wikipedia.org/wiki/Pattern_recognition"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    pattern recognition
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 processing, is continuously executed while slightly changing the size and the position of the window.For the reason above, an enormous number of times of discrimination processing is needed per frame, resulting in an enormous operation amount.In Patent Literature 1 below, in order to decrease the number of times of discrimination processing to reduce the operation amount, an 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device is disclosed, which detects in advance pixels indicating large luminance change in a time direction as an object area and exclusively uses the detected object area as a target for the discrimination processing.CITATION LISTPatent Literature 1JP 2007-18324 A (paragraph [0008], FIG. 1)SUMMARY OF INVENTIONSince the conventional object detection device is configured as described above, the processing time to detect the object can be shortened by reducing the operation amount. However, such operation is premised on the condition where the camera remains stationary when capturing an image. The conventional object detection device cannot be applied to an image captured by a camera that is moving at the time of capture, like a camera mounted in a moving body such as a robot or an automobile, or a hand-held camera. Therefore, there is a problem that an area in an image, in which an object to be detected exists, cannot be accurately detected from the image which is captured while moving.The present invention has been made to solve the above-described problem, and an object thereof is to provide an 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device and an object detection method, each being capable of accurately detecting an area in which an object to be detected exists, even from an image captured while moving.An 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device according to the present invention is provided with an image capturer to continuously capture images; an optical flow calculator to calculate an optical flow between images captured by the image capturer at different times; and an object detector to detect, by using the 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 calculated by the 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 calculator, an area in an image, in which an object to be detected exists.According to the present invention, the 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 calculator to calculating an optical flow between images captured by an image capturer at different times is provided, and the object detector detects an area in an image in which an object to be detected exists by using the 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 calculated by the 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 calculator. Therefore, there is an effect to accurately detect the area in which the object to be detected exists from an image captured by the image capturer while moving.BRIEF DESCRIPTION OF DRAWINGSFIG. 1 is a structural diagram illustrating an object detection device according to Embodiment 1 of the present invention.FIG. 2 is a hardware structural diagram illustrating the object detection device according to the Embodiment 1 of the present invention.FIG. 3 is a hardware structural diagram, where an optical flow calculator 2 and an object detector 3 are realized by a computer.FIG. 4 is a flowchart illustrating processing contents of the optical flow calculator 2 and the object detector 3.FIG. 5 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating a calculation example of optical flows and an aggregation example of optical flows in a spatial direction.FIG. 6 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating an example in which spatial sets of optical flows are aggregated in a time direction.FIG. 7 is a structural diagram illustrating an object detection device according to Embodiment 2 of the present invention.FIG. 8 is a hardware structural diagram illustrating the object detection device according to the Embodiment 2 of the present invention.FIG. 9 is a flowchart illustrating processing contents of an area determinator 6 in an object detector 3.FIG. 10 is a flowchart illustrating processing contents of an area corrector 7.FIG. 11 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating a setting example of 
<a href="https://en.wikipedia.org/wiki/Search_engine"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    a search
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 window performed by the area determinator 6.FIG. 12 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating relation between search windows and objects to be detected.FIG. 13 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating processing of the area determinator 6 for determining whether an object exists in 
<a href="https://en.wikipedia.org/wiki/Search_engine"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    a search
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 window while shifting a position of the search window.FIG. 14 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating an example of a case where a size and a position of an object to be detected cannot be accurately grasped even by reference to longitudinal/lateral sizes and positional coordinates of a rectangle output by the area determinator 6.FIG. 15 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating a setting example of 
<a href="https://en.wikipedia.org/wiki/Search_engine"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    a search
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 window by the area corrector 7.FIG. 16 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating an example of an edge image.FIG. 17 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating relation between search windows and edge density.FIG. 18 is a structural diagram illustrating an object detection device according to Embodiment 3 of the present invention.FIG. 19 is a hardware structural diagram illustrating the object detection device according to the Embodiment 3 of the present invention.FIG. 20 is a flowchart illustrating processing contents of an optical flow predictor 36.FIG. 21 is a flowchart illustrating processing contents of an evaluation value calculator 37 and an area determinator 38 in an object detector 3.FIG. 22 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating an example of optical flows obtained by the 
<a href="https://en.wikipedia.org/wiki/Laminar_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow predictor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 36, which depends on motion of a camera 11 and a spatial shape captured by the camera 11.FIG. 23 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating an example of optical flows calculated by an optical flow calculator 2.FIG. 24 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating a difference between optical flows output by the 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 calculator 2 and optical flows output by the 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 predictor 36._____c:1-11. (canceled)12. An 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device comprising:an image capturer to continuously capture images;an optical flow calculator to calculate, for each pixel constituting the image, an optical flow between images captured by the image capturer at different times; andan object detector to aggregate the optical flows calculated by the 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 calculator in a spatial direction and a time direction and detect an area in an image, in which an object to be detected exists, by using difference absolute values of arbitrary two optical flows,wherein the object detector includes:an evaluation value calculator todivide the image captured by the image capturer into one or more areas,calculate, for each divided area, a total of difference absolute values of angles of the optical flows and a total of difference absolute values of lengths of the optical flows of the pixels belonging to a corresponding divided area, andcalculate, for each divided area, an evaluation value of a corresponding divided area by using the total of difference absolute values of the angles and the total of difference absolute values of the lengths, the evaluation value indicating a measure of a possibility that the corresponding divided area is an object area representing part or whole of the object to be detected; andan area determinator to determine an area in an image, in which the object to be detected exists, by comparing the evaluation value of each divided area calculated by the evaluation value calculator with a 
<a href="https://en.wikipedia.org/wiki/Critical_value"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    threshold value
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.13. The 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device according to claim 12, further comprising an area corrector to correct the area detected by the 
<a href="https://en.wikipedia.org/wiki/Metal_detector"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detector
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
,wherein the area connection unit is configured tocalculate an image characteristic amount in the area detected by the object detector and an image characteristic amount in an area obtained by changing a position and a size of the detected area, and compare the calculated image characteristic amounts, andselect, as a corrected area for the area detected by the object detector, any one among the area detected by the object detector and the changed area on a basis of a comparison result of the image characteristic amounts.14. An 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device comprising:an image capturer to continuously capture images;an optical flow calculator to calculate, for each pixel constituting the image, an optical flow between images captured by the image capturer at different times; andan object detector to aggregate the optical flows calculated by the 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 calculator in a spatial direction and a time direction and detect an area in an image, in which an object to be detected exists, by using difference absolute values of arbitrary two optical flows,wherein the object detector includes:an evaluation value calculator todivide the image captured by the image capturer into one or more areas,calculate, for each divided area, a total of difference absolute values of angles of the optical flows and a total of difference absolute values of lengths of the optical flows of the pixels belonging to a corresponding divided area, andcalculate, for each divided area, an evaluation value of a corresponding divided area by using the total of difference absolute values of the angles and the total of difference absolute values of the lengths, the evaluation value indicating a measure of a possibility that the corresponding divided area is an object area representing part or whole of the object to be detected; andan area determinator to determine an area in an image, in which the object to be detected exists, by searching for an area in which the evaluation value of each divided area calculated by the evaluation value calculator indicates a maximum in the image.15. An 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method comprising:continuously capturing images;calculating, for each pixel constituting the image, an optical flow between images captured at different times; andaggregating the calculated optical flows in a spatial direction and a time direction and detecting an area in an image in which an object to be detected exists, by using difference absolute values of arbitrary two optical flows,dividing the captured image into one or more areas,calculating, for each divided area, a total of difference absolute values of angles of the optical flows and a total of difference absolute values of lengths of the optical flows of the pixels belonging to a corresponding divided area,calculating, for each divided area, an evaluation value of a corresponding divided area by using the total of difference absolute values of the angles and the total of difference absolute values of the lengths, the evaluation value indicating a measure of a possibility that the corresponding divided area is an object area representing part or whole of the object to be detected; anddetermining an area in an image, in which the object to be detected exists, by comparing the evaluation value of each divided area with a 
<a href="https://en.wikipedia.org/wiki/Critical_value"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    threshold value
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.16. An 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method comprising:continuously capturing images;calculating, for each pixel constituting the image, an optical flow between images captured at different times; andaggregating the calculated optical flows in a spatial direction and a time direction and detecting an area in an image in which an object to be detected exists, by using difference absolute values of arbitrary two optical flows,dividing the captured image into one or more areas,calculating, for each divided area, a total of difference absolute values of angles of the optical flows and a total of difference absolute values of lengths of the optical flows of the pixels belonging to a corresponding divided area,calculating, for each divided area, an evaluation value of a corresponding divided area by using the total of difference absolute values of the angles and the total of difference absolute values of the lengths, the evaluation value indicating a measure of a possibility that the corresponding divided area is an object area representing part or whole of the object to be detected; anddetermining an area in an image, in which the object to be detected exists, by searching for an area in which the evaluation value of each divided area indicates a maximum in the image.17. The 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device according to claim 14, further comprising an area corrector to correct the area detected by the 
<a href="https://en.wikipedia.org/wiki/Metal_detector"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detector
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
,wherein the area connection unit is configured tocalculate an image characteristic amount in the area detected by the object detector and an image characteristic amount in an area obtained by changing a position and a size of the detected area, and compare the calculated image characteristic amounts, andselect, as a corrected area for the area detected by the object detector, any one among the area detected by the object detector and the changed area on a basis of a comparison result of the image characteristic amounts._______________OBJECT EXTRACTION FROM VIDEO 
<a href="https://en.wikipedia.org/wiki/System_image"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    IMAGES SYSTEM
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 AND METHOD_____20180726_____XMLs/xml/ipa180726.xml_____US-20180211397-A1 : US-15934787 : US-15470477 : US-9959632 : US-15934787 : US-14525181 : US-9639954 : US-15470477_____G06T0007254000 : G06K0009460000 : G06T0007194000 : G06T0007215000 : G06K0009000000A computer implemented method of 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, the method comprising steps a computer is programmed to perform, the steps comprising: receiving a plurality of video images, deriving a plurality of background templates from at least one of the received video images, calculating a plurality of differences from an individual one of the received video images, each one of the differences being calculated between the individual video image and a respective and different one of the background templates, and extracting an object of interest from the individual video image, using a rule applied on the calculated differences._____d:CROSS-REFERENCE TO RELATED APPLICATIONSThis application is a continuation of U.S. patent application Ser. No. 15/470,477, filed Mar. 27, 2017, which is a continuation of U.S. patent application Ser. No. 14/525,181, filed Oct. 27, 2014, now U.S. Pat. No. 9,639,954, issued May 2, 2017, each of which is hereby incorporated in its entirety including all tables, figures, and claims.FIELD AND BACKGROUND OF THE INVENTIONThe present invention relates to 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and, more particularly, but not exclusively to extracting objects of interest from 
<a href="https://en.wikipedia.org/wiki/Image"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video images
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 captured during a sport event.In recent years, the use of 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and 
<a href="https://en.wikipedia.org/wiki/Computer_vision"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer vision
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 has been gaining more and more popularity in a variety of fields and industries. Some known industrial applications of 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and 
<a href="https://en.wikipedia.org/wiki/Computer_vision"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer vision
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 include, for example, security surveillance systems, operational management systems (say in a retail industry environment), tactical battlefield systems, etc.The extraction of objects of interest from video images is an aspect of 
<a href="https://en.wikipedia.org/wiki/Video_content_analysis"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video analysis
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.One of the techniques widely used in the fields of 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and 
<a href="https://en.wikipedia.org/wiki/Computer_vision"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer vision
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is background subtraction.Background subtraction is a technique in which an image's foreground is extracted for further processing, usually for recognition of objects of interest.Generally, an image's foreground is made of regions of the image, which are occupied by objects of interest (humans, cars, text, etc.). After a stage of image preprocessing (which may include image noise removal, morphology based analysis, etc.), 
<a href="https://en.wikipedia.org/wiki/Localization"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object localization
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may be required, which object localization may make use of background subtraction.Background subtraction is widely used for detecting moving objects (say cars or pedestrians) in videos, from static cameras, the rationale being one of detecting the moving objects from the difference between the current frame and a reference background template, also referred to as “background image” or “
<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    background model
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
”, which is made of static objects such as a building or a 
<a href="https://en.wikipedia.org/wiki/Traffic_light"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    traffic light
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 positioned at a road intersection.Objection extraction by background subtraction is often done if the image in question is a part of a 
<a href="https://en.wikipedia.org/wiki/Streaming_media"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video stream
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. Background subtraction provides important cues for numerous applications in 
<a href="https://en.wikipedia.org/wiki/Computer_vision"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer vision
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, for example surveillance tracking or human poses estimation.SUMMARY OF THE INVENTIONAccording to one aspect of the present invention there is provided a computer implemented method of 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, the method comprising steps a computer is programmed to perform, the steps comprising: receiving a plurality of video images, deriving a plurality of background templates from at least one of the received video images, calculating a plurality of differences from an individual one of the received video images, each one of the differences being calculated between the individual video image and a respective and different one of the background templates, and extracting an object of interest from the individual video image, using a rule applied on the calculated differences.According to a second aspect of the present invention there is provided an apparatus for 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, the apparatus comprising: a computer, a 
<a href="https://en.wikipedia.org/wiki/Image"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video image
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 receiver, implemented on the computer, configured to receive a plurality of video images, a background template deriver, in communication with the 
<a href="https://en.wikipedia.org/wiki/Image"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video image
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 receiver, configured to derive a plurality of background templates from at least one of the received video images, a difference calculator, in communication with the background template deriver, configured to calculate a plurality of differences from an individual one of the received video images, each one of the differences being calculated between the individual video image and a respective and different one of the background templates, and an object extractor, in communication with the difference calculator, configured to extract an object of interest from the individual video image, using a rule applied on the calculated differences.According to a third aspect of the present invention there is provided a non-transitory 
<a href="https://en.wikipedia.org/wiki/Machine-readable_medium"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer readable
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 medium storing computer executable instructions for performing steps of 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, the steps comprising: receiving a plurality of video image, deriving a plurality of background templates from at least one of the received video images, calculating a plurality of differences from an individual one of the received video images, each one of the differences being calculated between the individual video image and a respective and different one of the background templates, and extracting an object of interest from the individual video image, using a rule applied on the calculated differences.Unless otherwise defined, all technical and scientific terms used herein have the same meaning as commonly understood by one of ordinary skill in the art to which this invention belongs.The materials, methods, and examples provided herein are illustrative only and not intended to be limiting. Implementation of the method and system of the present invention involves performing or completing certain selected tasks or steps manually, automatically, or a combination thereof.Moreover, according to actual instrumentation and equipment of preferred embodiments of the method and system of the present invention, several selected steps could be implemented by hardware or by software on any 
<a href="https://en.wikipedia.org/wiki/Operating_system"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    operating system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of any firmware or a combination thereof.For example, as hardware, selected steps of the invention could be implemented as a chip or a circuit. As software, selected steps of the invention could be implemented as a plurality of software instructions being executed by a computer using any suitable 
<a href="https://en.wikipedia.org/wiki/Operating_system"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    operating system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. In any case, selected steps of the method and system of the invention could be described as being performed by a 
<a href="https://en.wikipedia.org/wiki/Data_processing_(disambiguation)"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    data processor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, such as a 
<a href="https://en.wikipedia.org/wiki/Computing_platform"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computing platform
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for executing a plurality of instructions.BRIEF DESCRIPTION OF THE DRAWINGSThe invention is herein described, by way of example only, with reference to the accompanying drawings. With specific reference now to the drawings in detail, it is stressed that the particulars shown are by way of example and for purposes of illustrative discussion of the preferred embodiments of the present invention only, and are presented in order to provide what is believed to be the most useful and readily understood description of the principles and conceptual aspects of the invention. The description taken with the drawings making apparent to those skilled in the art how the several forms of the invention may be embodied in practice.In the drawings:FIG. 1 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 schematically illustrating an exemplary apparatus for 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, according to an exemplary embodiment of the present invention.FIG. 2 is a simplified flowchart schematically illustrating a first exemplary method for 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, according to an exemplary embodiment of the present invention.FIG. 3 is a simplified flowchart schematically illustrating a second exemplary method for 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, according to an exemplary embodiment of the present invention.FIGS. 4A-4H are simplified block diagrams schematically illustrating a first implementation scenario, according to an exemplary embodiment of the present invention.FIG. 4A illustrates a player who stands in a left position next to one or more trees and a cloud.FIG. 4B illustrates a player who stands in a right position next to one or more trees and a cloud.FIG. 4C is a background template containing one or more trees and a cloud. It does not include the image of a player.FIG. 4D is a background template containing one or more trees and a cloud. It also includes the image of a player in the left position.FIG. 4E illustrates a player standing in a left position.FIG. 4F illustrates a player who stands next to one or more trees and the sun coming out from behind a cloud.FIG. 4G is a background template containing one or more trees and a cloud.FIG. 4H illustrates a player who stands next to one or more trees, with the sun coming out from behind a cloud.FIG. 5 is a simplified flowchart schematically illustrating a third exemplary method for 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, according to an exemplary embodiment of the present invention.FIGS. 6A-6O are simplified block diagrams schematically illustrating a second implementation scenario, according to an exemplary embodiment of the present invention.FIG. 6A illustrates a player who stands in a left position next to one or more trees and a cloud.FIG. 6B illustrates a player stands in a right position next to one or more trees and a cloud.FIG. 6C illustrates a player who stands in a left position next to one or more trees and a cloud.FIG. 6D is a background template containing one or more trees and a cloud. It does not include the image of a player.FIG. 6E is a background template containing one or more trees and a cloud. It also includes the image of a player in the right position.FIG. 6F illustrates a player standing in a left position.FIG. 6G is a background template containing one or more trees and a cloud. It also includes the image of a player in the right position and a player in the left position.FIG. 6H illustrates a player who stands in a left position.FIG. 6I illustrates a player who stands in a left position next to one or more trees and the sun.FIG. 6J illustrates a player who stands in a right position next to one or more trees and the sun.FIG. 6K is a background template containing one or more trees and a cloud. It does not include the image of a player.FIG. 6L illustrates a player who stands in a left position next to one or more trees and the sun.FIG. 6M illustrates a player who stands in a right position next to one or more trees and the sun.FIG. 6N is a background template containing one or more trees and a cloud. It also includes the image of a player in a right position and a player in a left position.FIG. 6O shows a player standing in a right position.FIG. 7 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 schematically illustrating an exemplary 
<a href="https://en.wikipedia.org/wiki/Machine-readable_medium"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer readable
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 medium storing computer executable instructions for performing steps of 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from video images, according to an exemplary embodiment of the present invention._____c:1. A computer implemented method of 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from images, the method comprising steps a computer is programmed to perform, the steps comprising:receiving a plurality of background templates and at least one image;calculating a plurality of differences for an individual one of the received at least one image, each one of the differences being calculated between the individual image and a respective one of the received background templates; andextracting an object of interest from the individual image, using a rule applied on the calculated differences.2. The method of claim 1, further comprising selecting the rule among a plurality of predefined rules.3. The method of claim 1, further comprising allowing a user to define the rule.4. The method of claim 1, further comprising selecting the rule according to circumstances of capturing of the received at least one image.5. The method of claim 1, further comprising selecting the rule according to a characteristic pertaining to the object of interest.6. The method of claim 1, further comprising selecting the rule according to a characteristic pertaining to a background of the received at least one image.7. The method of claim 1, wherein each one of at least two of the received background templates is a template derived using a respective and different one of a plurality of background calculation methods.8. The method of claim 1, wherein each one of at least two of the received background templates is a template derived using a respective and at least partially different subset of the received at least one image.9. The method of claim 1, wherein each one of at least two of the received background templates is a template derived using a respective and at least partially less recent subset of the received at least one image.10. The method of claim 1, wherein each one of at least two of the received background templates is a template derived using a respective and different frequency of sampling of the received at least one image.11. The method of claim 1, wherein each one of at least two of the received background templates is a template derived using a respective and different in size subset of the received at least one image.12. The method of claim 1, further comprising updating each one of at least two of the received background templates, with a respective and different update rate.13. Apparatus for 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from images, the apparatus comprising:a computer;a difference calculator, configured to receive a plurality of background templates and at least one image and calculate a plurality of differences for an individual one of the received at least one image, each one of the differences being calculated between the individual image and a respective one of the received background templates; andan object extractor, in communication with said difference calculator, configured to extract an object of interest from the individual image, using a rule applied on the calculated differences.14. A non-transitory 
<a href="https://en.wikipedia.org/wiki/Machine-readable_medium"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer readable
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 medium storing computer executable instructions for performing steps of 
<a href="https://en.wikipedia.org/wiki/Simple_interactive_object_extraction"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object extraction
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 from images, the steps comprising:receiving a plurality of background templates and at least one image;calculating a plurality of differences for an individual one of the received at least one image, each one of the differences being calculated between the individual image and a respective one of the received background templates; andextracting an object of interest from the individual image, using a rule applied on the calculated differences._______________SYSTEM AND METHOD FOR TRACKING MULTIPLE OBJECTS_____20180816_____XMLs/xml/ipa180816.xml_____US-20180232891-A1 : US-15662738 : KR-10-2017-0019427_____G06T0007254000 : G06T0007246000 : G06K0009000000 : G06K0009320000The present invention relates to a system for tracking an object. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 includes an image capturing unit configured to capture a video of a predetermined observation area and output the captured video; and a multi-object tracker configured to output an object-tracking image by tracking multiple objects within an object image which is generated by extracting the objects from each of image frames obtained from the video obtained from the image capturing unit, wherein the multi-object tracker determines whether occlusion of the objects or hijacking occurs while performing multi-object tracking, and when it is determined that at least one of the occlusion and hijacking occurs, the multi-object tracker outputs the object-tracking image corrected by removing the occurring occlusion or hijacking._____d:CROSS-REFERENCE TO RELATED APPLICATIONThis application claims priority to and the benefit of Korean Patent Application No. 10-2017-0019427, filed on Feb. 13, 2017, the disclosure of which is incorporated herein by reference in its entirety.BACKGROUND1. Field of the InventionThe present invention relates to an 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system, and more particularly, to a system and method for tracking multiple objects.2. Discussion of Related ArtIn the sports and security service fields, a service capable of accurately tracking multiple target objects (e.g., people) and providing information about the objects by analyzing images obtained through camera tracking on the target objects is provided.In the case where multiple target objects are tracked to provide the information about the objects, there may be problems of occlusion among objects and hijacking which causes a target object to be changed (i.e., causes another object to be tracked) due to the similarities between the objects.Therefore, there is a need for an approach that prevents an 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 error due to occlusion of objects and hijacking that tracks another target object.SUMMARY OF THE INVENTIONTherefore, the present invention is devised to solve the aforementioned problems, and the objective of the present invention is to provide a system and method for tracking multiple objects, which is capable of providing accurate information about a tracked object by handling occlusion of objects and hijacking which may occur when multiple objects are tracked simultaneously.In one general aspect, there is provided a system for tracking multiple objects including: an image capturing unit configured to capture a video of a predetermined observation area and output the captured video; and a multi-object tracker configured to output an object-tracking image by tracking multiple objects within an object image which is generated by extracting the objects from each of image frames obtained from the video obtained from the image capturing unit, wherein the multi-object tracker determines whether occlusion of the objects or hijacking occurs while performing multi-object tracking, and when it is determined that at least one of the occlusion and the hijacking occurs, the multi-object tracker outputs the object-tracking image corrected by removing the occurring occlusion or hijacking.The multi-object tracker may output a current object-tracking image when it is determined that the occlusion of the objects and the hijacking do not occur.When a bounding box of a target object is detected to be occluded by a bounding box of another object, the multi-object tracker may determine that the occlusion of the objects occurs.When it is determined that the occlusion of the objects occurs, the multi-object tracker may estimate a bounding box and depth of each of the objects and remove objects except for a target object from the object image.The multi-object tracker may determine a depth order of the objects on the basis of the estimated bounding boxes and depths of the objects, recognize at least one of objects in front of and behind the target object on the basis of the determined 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth order
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the objects, and remove the recognized object.When a displacement of a bounding box of a target object is detected to be identical to a displacement of a bounding box of another object for a predetermined time period, the multi-object tracker may determine that the hijacking occurs.When it is determined that the hijacking occurs, the multi-object tracker may remove an object being actually tracked by a bounding box of a target object from the object image so that the bounding box tracks the target object.The multi-object tracker may generate a reference background image by modeling a reference background using a reference background modeling video obtained from the image capturing unit and generate the object image by extracting the objects from each of the image frames on the basis of comparison between the reference background image and each of the image frames obtained from a video for 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 obtained from the image capturing unit.The multi-object tracker may obtain a 
<a href="https://en.wikipedia.org/wiki/Color_difference"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    color difference
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 by comparing colors of the reference background image and each of the image frames, and extract the objects from each of the image frames on the basis of the obtained 
<a href="https://en.wikipedia.org/wiki/Color_difference"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    color difference
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.When the generated object image is an initial object image, the multi-object tracker may allocate a bounding box to each of the objects to initialize multi-object tracking and perform the multi-object tracking.In another general aspect, there is provided a method of tracking multiple objects including: generating a reference background image by modeling a reference background using a reference background modeling video obtained from an image capturing unit; generating an object image by extracting objects from each of image frames on the basis of comparison between the reference background image and each of the image frames obtained from a video for 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 obtained from the image capturing unit; and outputting a current object-tracking image or a corrected object-tracking image according to a result of determination on whether occlusion of the objects or hijacking occurs, while performing multi-object tracking on the basis of the object image.Whether the occlusion of the objects occurs may be determined on the basis of detection of whether a bounding box of a target object is occluded by a bounding box of another object.Whether the hijacking occurs may be determined on the basis of detection of whether a displacement of a bounding box of a target object is identical to a displacement of a bounding box of another object.The outputting of the current object-tracking image may be performed when it is determined that the occlusion of the objects and the hijacking do not occur.When it is determined that the occlusion of the objects occurs, the outputting of the corrected object-tracking image may be performed wherein the object-tracking image may be corrected by removing objects except for a target object from the object image by estimating a bounding box and depth of each of the objects.When it is determined that the hijacking occurs, the outputting of the corrected object-tracking image may be performed wherein the object-tracking image may be corrected by removing an object being actually tracked by a bounding box of a target object from the object image so that the bounding box tracks the target object.When it is determined that the occlusion of the objects and the hijacking occur, the outputting of the corrected object-tracking image may be performed wherein the object-tracking image may be corrected by removing objects except for a target object from the object image by estimating a bounding box and depth of each of the objects and is corrected also by removing an object being actually tracked by a bounding box of the target object from the object image so that the bounding box tracks the target object.The removing of the object other than the target object from the object image by estimating the bounding box and depth of each of the objects may include determining a depth order of the objects on the basis of the estimated bounding boxes and depths of the objects, recognizing at least one of objects in front of and behind the target object on the basis of the determined depth order, and removing the recognized object.The generating of the object image may include obtaining a 
<a href="https://en.wikipedia.org/wiki/Color_difference"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    color difference
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 by comparing colors of the reference background image and each of the image frames and extracting the objects from each of the image frames on the basis of the obtained 
<a href="https://en.wikipedia.org/wiki/Color_difference"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    color difference
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.When initial multi-object tracking is performed, the performing of the multi-object tracking on the basis of the object image may include allocating a bounding box to each of the objects to initialize the multi-object tracking and then performing the multi-object tracking.BRIEF DESCRIPTION OF THE DRAWINGSThe above and other objects, features and advantages of the present invention will become more apparent to those of ordinary skill in the art by describing exemplary embodiments thereof in detail with reference to the accompanying drawings, in which:FIG. 1 is a diagram illustrating a configuration of a system for tracking multiple objects according to an exemplary embodiment of the present invention;FIG. 2 is a flowchart illustrating operations of 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    the system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for tracking multiple objects according to the exemplary embodiment of the present invention;FIGS. 3A and 3B are flowcharts illustrating an operation of outputting an object-tracking image performed by 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    the system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for tracking multiple objects according to the exemplary embodiment of the present invention in detail;FIG. 4A is a picture showing an example of a video input to a multi-object tracker in one embodiment of the present invention;FIG. 4B is a picture showing an example of a reference background image modeled by the multi-object tracker in the embodiment of the present invention;FIG. 4C is a picture showing an example of an object image extracted from the input video by the multi-object tracker in the embodiment of the present invention;FIG. 5 is a picture for describing a method of determining whether hijacking occurs according to an embodiment of the present invention;FIG. 6A is a picture showing an example of an original video input to the multi-object tracker in the embodiment of the present invention;FIG. 6B is a picture showing an image in which hijacking occurs in the embodiment of the present invention;FIG. 6C is a picture showing an image from which the hijacking is removed in the embodiment of the present invention;FIG. 7A shows an example in which occlusion of objects occurs in the embodiment of the present invention;FIG. 7B shows an example in which the occlusion is removed in the embodiment of the present invention;FIG. 8 is a table for comparing center location errors of 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    the system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for tracking multiple objects according to the embodiment of the present invention and a conventional 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system;FIG. 9 is a table for comparing success rates of 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    the system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for tracking multiple objects according to the embodiment of the present invention and the conventional 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system; andFIG. 10 is a table for comparing multi-object tracking times of 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    the system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for tracking multiple objects according to the embodiment of the present invention and the conventional object tracking system._____c:1. A system for tracking multiple objects, comprising:an image capturing unit configured to capture a video of a predetermined observation area and output the captured video; anda multi-object tracker configured to output an object-tracking image by tracking multiple objects within an object image which is generated by extracting the objects from each of image frames obtained from the video obtained from the image capturing unit,wherein the multi-object tracker determines whether occlusion of the objects or hijacking occurs while performing multi-object tracking, and when it is determined that at least one of the occlusion and the hijacking occurs, the multi-object tracker outputs the object-tracking image corrected by removing the occurring occlusion or hijacking.2. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 1, wherein the multi-object tracker outputs a current object-tracking image when it is determined that the occlusion of the objects and the hijacking do not occur.3. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 1, wherein, when a bounding box of a target object is detected to be occluded by a bounding box of another object, the multi-object tracker determines that the occlusion of the objects occurs.4. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 1, wherein, when it is determined that the occlusion of the objects occurs, the multi-object tracker estimates a bounding box and depth of each of the objects and removes objects except for a target object from the object image.5. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 4, wherein the multi-object tracker determines a depth order of the objects on the basis of the estimated bounding boxes and depths of the objects, recognizes at least one of objects in front of and behind the target object on the basis of the determined depth order of the objects, and removes the recognized object.6. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 1, wherein, when a displacement of a bounding box of a target object is detected to be identical to a displacement of a bounding box of another object for a predetermined time period, the multi-object tracker determines that the hijacking occurs.7. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 1, wherein, when it is determined that the hijacking occurs, the multi-object tracker removes an object being actually tracked by a bounding box of a target object from the object image so that the bounding box tracks the target object.8. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 1, wherein the multi-object tracker generates a reference background image by modeling a reference background using a reference background modeling video obtained from the image capturing unit and generates the object image by extracting the objects from each of the image frames on the basis of comparison between the reference background image and each of the image frames obtained from a video for 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 obtained from the image capturing unit.9. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 8, wherein the multi-object tracker obtains a 
<a href="https://en.wikipedia.org/wiki/Color_difference"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    color difference
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 by comparing colors of the reference background image and each of the image frames, and extracts the objects from each of the image frames on the basis of the obtained 
<a href="https://en.wikipedia.org/wiki/Color_difference"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    color difference
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.10. 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    The system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of claim 1, wherein, when the generated object image is an initial object image, the multi-object tracker allocates a bounding box to each of the objects to initialize multi-object tracking and performs the multi-object tracking.11. A method of tracking multiple objects, comprising:generating a reference background image by modeling a reference background using a reference background modeling video obtained from an image capturing unit;generating an object image by extracting objects from each of image frames on the basis of comparison between the reference background image and each of the image frames obtained from a video for 
<a href="https://en.wikipedia.org/wiki/Video_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 obtained from the image capturing unit; andoutputting a current object-tracking image or a corrected object-tracking image according to a result of determination on whether occlusion of the objects or hijacking occurs, while performing multi-object tracking on the basis of the object image.12. The method of claim 11, wherein whether the occlusion of the objects occurs is determined on the basis of detection of whether a bounding box of a target object is occluded by a bounding box of another object.13. The method of claim 11, wherein whether the hijacking occurs is determined on the basis of detection of whether a displacement of a bounding box of a target object is identical to a displacement of a bounding box of another object.14. The method of claim 11, wherein the outputting of the current object-tracking image is performed when it is determined that the occlusion of the objects and the hijacking do not occur.15. The method of claim 11, wherein, when it is determined that the occlusion of the objects occurs, the outputting of the corrected object-tracking image is performed wherein the object-tracking image is corrected by removing objects except for a target object from the object image by estimating a bounding box and depth of each of the objects.16. The method of claim 11, wherein, when it is determined that the hijacking occurs, the outputting of the corrected object-tracking image is performed wherein the object-tracking image is corrected by removing an object being actually tracked by a bounding box of a target object from the object image so that the bounding box tracks the target object.17. The method of claim 11, wherein, when it is determined that the occlusion of the objects and the hijacking occur, the outputting of the corrected object-tracking image is performed wherein the object-tracking image is corrected by removing objects except for a target object from the object image by estimating a bounding box and depth of each of the objects and is corrected also by removing an object being actually tracked by a bounding box of the target object from the object image so that the bounding box tracks the target object.18. The method of claim 15, wherein the removing of the objects except for the target object from the object image by estimating the bounding box and depth of each of the objects includes determining a depth order of the objects on the basis of the estimated bounding boxes and depths of the objects, recognizing at least one of objects in front of and behind the target object on the basis of the determined depth order, and removing the recognized object.19. The method of claim 11, wherein the generating of the object image includes obtaining a 
<a href="https://en.wikipedia.org/wiki/Color_difference"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    color difference
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 by comparing colors of the reference background image and each of the image frames and extracting the objects from each of the image frames on the basis of the obtained 
<a href="https://en.wikipedia.org/wiki/Color_difference"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    color difference
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.20. The method of claim 11, wherein, when initial multi-object tracking is performed, the performing of the multi-object tracking on the basis of the object image includes allocating a bounding box to each of the objects to initialize the multi-object tracking and then performing the multi-object tracking._______________SYSTEM AND METHOD FOR DETECTING MOVING OBJECT IN AN IMAGE_____20180913_____XMLs/xml/ipa180913.xml_____US-20180260964-A1 : US-15917565 : CN-201710137069.2_____G06T0007254000 : G06K0009620000 : G06K0009380000 : G06K0009460000A moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method and apparatus are disclosed. The method may include: obtaining a first image and a second image of a scene; determining a difference of the first image and the second image; performing a binarization operation on the difference of the first image and the second image, to generate a binary image; determining the number of pixels whose values are nonzero in each column of the binary image, to generate a column pixel histogram; and determining whether a moving object is present in the scene based on the column pixel histogram._____d:CROSS-REFERENCE TO RELATED APPLICATIONThis application is based upon and claims priority from Chinese Patent Application No. 201710137069.2, filed on Mar. 9, 2017, the disclosure of which is expressly incorporated herein by reference in its entirety.TECHNICAL FIELDThe present disclosure generally relates to 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 technology, and more specifically to a system and method for detecting moving objects in an image.BACKGROUNDIn the field of 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, the use of 
<a href="https://en.wikipedia.org/wiki/Motion_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 technology can automatically detect motion scenes in videos, which can reduce the cost of manual monitoring and increase monitoring efficiency and effectiveness. As the foundation for motion detection recording technology and an important component of 
<a href="https://en.wikipedia.org/wiki/Elizabeth_Smart"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    smart video
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 cameras, 
<a href="https://en.wikipedia.org/wiki/Motion_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 technology has been embedded into the camera firmware of an increasing number of 
<a href="https://en.wikipedia.org/wiki/Elizabeth_Smart"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    smart video
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 cameras.There are three types of conventional motion detection methods: the optical flow method, the inter-frame difference method, and the background subtraction method. In the 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method, each pixel point in the image is assigned a velocity vector to form an image motion field; at a given moment of the motion, the points in the image have a one-to-one relationship with the points on the three-dimensional object, and this relationship can be obtained from the projection relationship; a 
<a href="https://en.wikipedia.org/wiki/Dynamic_program_analysis"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    dynamic analysis
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 on the image can be performed based on the characteristics of the velocity vector of each pixel point. If no moving object is present in the image, then the 
<a href="https://en.wikipedia.org/wiki/Optical_flow"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    optical flow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 vector would change continuously across the entire image area; when a moving object is present in the image, the target and the image background would move relative to one another, so the velocity vector of the moving object is inevitably different from the velocity vector of the adjacent background area; thus, the moving object and its location are detected. 
<a href="https://en.wikipedia.org/wiki/Image_differencing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Image difference
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 methods are relatively straightforward and easy to implement. Therefore, these methods have now become the most widely used to detect moving targets. There are two types of image difference methods: the background subtraction method and the inter-frame difference method. The background subtraction method compares the current frame against the background reference model in a series of images to detect moving objects, and its performance depends on the background modeling technology used. The inter-frame difference method performs a difference operation on two or three adjacent frames in a series of video images to obtain the contour of a moving target.However, the image difference methods, the background subtraction method and the inter-frame difference method, are highly sensitive to interference factors that cause changes in the scene, for example, lighting, plants swinging in the wind, etc. For example, in a night vision environment, a flying insect may lead to changes in the scene. Since conventional motion detection algorithms are highly sensitive to interference factors that cause changes in the scene, a motion detection alarm is triggered once a change in the scene is detected. However, this type of alarm is not desired; in other words, it is a false positive alarm. Therefore, there is a need to detect and identify moving objects, for example, flying insects, in a night vision environment to avoid false positive alarms.The disclosed methods and systems address one or more of the problems listed above.SUMMARYConsistent with one embodiment of the present disclosure, a method for detecting moving objects in an image is provided. The method may include: obtaining a first image and a second image of a scene; determining a difference of the first image and the second image; performing a binarization operation on the difference of the first image and the second image, to generate a binary image; determining the number of pixels whose values are nonzero in each column of the binary image, to generate a column pixel histogram; and determining whether a moving object is present in the scene based on the column pixel histogram.Consistent with another embodiment of the present disclosure, an apparatus for detecting moving objects in an image is provided. The apparatus includes a memory storing instructions. The apparatus also includes a processor configured to execute the instructions to: obtain a first image and a second image of a scene; determine a difference of the first image and the second image; perform a binarization operation on the difference of the first image and the second image, to generate a binary image; determine the number of pixels whose values are nonzero in each column of the binary image, to generate a column pixel histogram; and determine whether a moving object is present in the scene based on the column pixel histogram.Consistent with 
<a href="https://en.wikipedia.org/wiki/Yet_another"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    yet another
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 embodiment of the present disclosure, a non-transitory computer-readable 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    storage medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 is provided. The medium stores instructions that, when executed by a processor, cause the processor to perform a method for detecting moving objects in an image. The method may include: obtaining a first image and a second image of a scene; determining a difference of the first image and the second image; performing a binarization operation on the difference of the first image and the second image, to generate a binary image; determining the number of pixels whose values are nonzero in each column of the binary image, to generate a column pixel histogram; and determining whether a moving object is present in the scene based on the column pixel histogram.It is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory only and are not restrictive of the invention, as claimed.DESCRIPTION OF DRAWINGSThe accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate embodiments consistent with the present disclosure and, together with the description, serve to explain the principles of the present disclosure.FIG. 1 is a flowchart of a method for detecting moving objects in an image, according to an exemplary embodiment of the present disclosure.FIG. 2 is a 
<a href="https://en.wikipedia.org/wiki/Schematic"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    schematic diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating spatial relationships between pixel p and its adjacent pixels in an erosion operation, according to an exemplary embodiment of the present disclosure.FIG. 3 is a flowchart of a method for detecting moving objects in an image, according to an exemplary embodiment of the present disclosure.FIG. 4 is a 
<a href="https://en.wikipedia.org/wiki/Schematic"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    schematic diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating a binary image, according to an exemplary embodiment of the present disclosure.FIG. 5 is a 
<a href="https://en.wikipedia.org/wiki/Schematic"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    schematic diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating a column pixel histogram, according to an exemplary embodiment of the present disclosure.FIG. 6 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a device for detecting moving objects in an image, according to an exemplary embodiment of the present disclosure.FIG. 7 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a device for detecting moving objects in an image, according to an exemplary embodiment of the present disclosure.FIG. 8 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 device, according to an exemplary embodiment of the present disclosure._____c:1. A method for detecting moving objects in an image, comprising:obtaining a first image and a second image of a scene;determining a difference of the first image and the second image;performing a binarization operation on the difference of the first image and the second image, to generate a binary image;determining, in each column of the binary image, the number of pixels whose values are nonzero, to generate a column pixel histogram; anddetermining whether a moving object is present in the scene based on the column pixel histogram.2. The method according to claim 1, wherein determining whether a moving object is present in the scene based on the column pixel histogram comprises:determining the number of crests in the column pixel histogram; anddetermining whether a moving object is present in the scene based on the number of crests.3. The method according to claim 2, wherein:a y-axis of the column pixel histogram corresponds to the number of pixels whose values are nonzero in each column of the binary image;an x-axis of the column pixel histogram corresponds to the columns of the binary image; anddetermining the number of crests in the column pixel histogram comprises:traversing all columns of the column pixel histogram; andif the column pixel histogram includes a first column whose y-coordinate value is greater than a crest threshold,determining a second column which is separate from the first column by a set distance along the x-axis, andincreasing the number of crests by one when a difference between the y-coordinate value of the first column and a y-coordinate value of the second column is greater than a set difference threshold.4. The method according to claim 3, wherein only one crest is retained within a set range along the x-axis of the column pixel histogram.5. The method according to claim 2, wherein determining whether a moving object is present in the scene based on the number of crests comprises:when the number of crests is less than a minimum number threshold or greater than a maximum number threshold, determining that a moving object is present in the scene.6. The method according to claim 1, wherein:a y-axis of the column pixel histogram corresponds to the number of pixels whose values are nonzero in each column of the binary image; anddetermining whether a moving object is present in the scene based on the column pixel histogram comprises:determining whether a moving object is present in the scene based on an effective column ratio, the effective column ratio being a ratio of the number of columns in the column pixel histogram whose y-coordinate values are greater than a crest threshold to the number of columns in the column pixel histogram whose y-coordinate values are greater than a noise threshold.7. The method according to claim 6, wherein determining whether a moving object is present in the scene based on the effective column ratio comprises:when the effective column ratio is less than a ratio threshold, determining that a moving object is present in the scene.8. The method according to claim 1, wherein determining, in each column of the binary image, the number of pixels whose values are nonzero comprises:performing an erosion operation on the binary image to obtain an eroded image;determining, in the eroded image, the number of pixels whose values are nonzero; andwhen the number of pixels in the eroded image whose values are nonzero is greater than a first set threshold, determining, in each column of the binary image, the number of pixels whose values are nonzero.9. The method according to claim 8, further comprising:when the number of pixels in the eroded image whose values are nonzero is less than or equal to the first set threshold, obtaining new images of the scene to generate the column pixel histogram.10. The method according to claims 1, wherein:determining the difference of the first image and the second image comprises:subtracting pixel values of the first image by pixel values of the second image, to generate difference values; andgenerating a different image using absolute values of the determined difference values; andperforming a binarization operation on the difference of the first image and the second image comprises:in the difference image, setting pixel values less than a threshold to be zero, and setting pixel values greater than or equal to the threshold to be a preset nonzero value.11. The method according to claims 1, wherein the first image and the second image are two adjacent image frames in an 
<a href="https://en.wikipedia.org/wiki/Thermographic_camera"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    infrared video
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.12. An apparatus comprising:a memory storing instructions; anda processor configured to execute the instructions to:obtain a first image and a second image of a scene;determine a difference of the first image and the second image;perform a binarization operation on the difference of the first image and the second image, to generate a binary image;determine the number of pixels whose values are nonzero in each column of the binary image, to generate a column pixel histogram; anddetermine whether a moving object is present in the scene based on the column pixel histogram.13. The apparatus according to claims 12, wherein the processor is further configured to execute the instructions to:determine the number of crests in the column pixel histogram; anddetermine whether a moving object is present in the scene based on the number of crests.14. The apparatus according to claims 13, wherein:a y-axis of the column pixel histogram corresponds to the number of pixels whose values are nonzero in each column of the binary image;an x-axis of the column pixel histogram corresponds to the columns of the binary image; andthe processor is further configured to execute the instructions to:traverse all columns of the column pixel histogram; andif the column pixel histogram includes a first column whose y-coordinate value is greater than a crest threshold,determine a second column which is separate from the first column by a set distance along the x-axis, andincrease the number of crests by one when a difference between the y-coordinate value of the first column and a y-coordinate value of the second column is greater than a set difference threshold.15. The apparatus according to claims 13, wherein in determining whether a moving object is present in the scene based on the number of crests, the processor is further configured to execute the instructions to:when the number of crests is less than a minimum number threshold or greater than a maximum number threshold, determine that a moving object is present in the scene.16. The apparatus according to claims 12, wherein:a y-axis of the column pixel histogram corresponds to the number of pixels whose values are nonzero in each column of the binary image; andthe processor is further configured to execute the instructions to:determine whether a moving object is present in the scene based on an effective column ratio, the effective column ratio being a ratio of the number of columns in the column pixel histogram whose y-coordinate values are greater than a crest threshold to the number of columns in the column pixel histogram whose y-coordinate values are greater than a noise threshold.17. The apparatus according to claims 16, wherein in determining whether a moving object is present in the scene based on the effective column ratio, the processor is further configured to execute the instructions to:when the effective column ratio is less than a ratio threshold, determine that a moving object is present in the scene.18. The apparatus according to claims 12, wherein the processor is further configured to execute the instructions to:perform an erosion operation on the binary image to obtain an eroded image;determine, in the eroded image, the number of pixels whose values are nonzero; andwhen the number of pixels in the eroded image whose values are nonzero is greater than a first set threshold, determine, in each column of the binary image, the number of pixels whose values are nonzero.19. The apparatus according to claims 18, wherein the processor is further configured to execute the instructions to:when the number of pixels in the eroded image whose values are nonzero is less than or equal to the first set threshold, obtain new images of the scene to generate the column pixel histogram.20. A non-transitory computer-readable 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    storage medium
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 storing instructions that, when executed by a processor, cause the processor to perform a moving 
<a href="https://en.wikipedia.org/wiki/Object_detection"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    object detection
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method comprising:obtaining a first image and a second image of a scene;determining a difference of the first image and the second image;performing a binarization operation on the difference of the first image and the second image, to generate a binary image;determining the number of pixels whose values are nonzero in each column of the binary image, to generate a column pixel histogram; anddetermining whether a moving object is present in the scene based on the column pixel histogram._______________APPARATUS AND METHOD FOR EFFICIENT MOTION ESTIMATION_____20171109_____XMLs/xml/ipa171109.xml_____US-20170323454-A1 : US-15586600 : IN-201641015446_____G06T0007254000 : H04N0019523000 : H04N0019159000 : G06T0003400000 : H04N0019139000The architecture shown can perform global search, local search and local sub pixel search in a parallel or in a pipelined mode. All operations are in a 
<a href="https://en.wikipedia.org/wiki/HTTP_Live_Streaming"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    streaming mode
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 without the requirement of external intermediate 
<a href="https://en.wikipedia.org/wiki/Data_storage"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    data storage
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
._____d:CLAIM OF PRIORITYThis application claims priority under 35 U.S.C 119(e) (1) to Indian Provisional Application No. 20164101546 filed May 4, 2016TECHNICAL FIELD OF THE INVENTIONThe technical field of this invention is 
<a href="https://en.wikipedia.org/wiki/Image_compression"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image compression
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.BACKGROUND OF THE INVENTIONIncreasing 
<a href="https://en.wikipedia.org/wiki/Display_resolution"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video resolution
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and frame rates, along with large number of searching and matching operations involved in motion estimation demand very high performance. While high performance can be achieved by increasing hardware throughput and higher 
<a href="https://en.wikipedia.org/wiki/Clock_rate"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    clock frequency
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, it is important to identify and exploit parallelism present in the algorithm in order to efficiently utilize available hardware resources.The 
<a href="https://en.wikipedia.org/wiki/Motion_estimation"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Motion Estimation
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 process involves searching operations which require accessing large amount of reference picture data from memory. Memory bandwidth is an expensive resource which often limits the computational parallelism that can be built in hardware. Further, this large data traffic from the memory leads to large power dissipation.Motion estimation finds best match for each block in current 
<a href="https://en.wikipedia.org/wiki/Film_frame"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video frame
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 among blocks from previously coded frame (s) (called as reference frames). Block size is typically 16×16 pixels.A widely used metric to define the match is—SAD_(Sum Of Absolute Difference in all the pixel values of current block and a reference block).The best match information is indicated by the motion vector: if the current position of a block is (16,16) then 
<a href="https://en.wikipedia.org/wiki/Motion_vector"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion vector
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 (4,1) means the best match lies at position (20,17) in the reference frame.The motion vector can also be in fraction pixel precision: half pixel, quarter pixel etc.Fractional pixels are calculated by interpolating neighboring integer position pixels.A 
<a href="https://en.wikipedia.org/wiki/Motion_estimation"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion estimation
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 algorithm would typically consist of these steps:Stage 1: choosing best among a few predictor 
<a href="https://en.wikipedia.org/wiki/Motion_vector"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion vectors
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
;Stage 2: search around winner of Stage 1;Stage 3, 4: search around winner of Stage 2 and stage 3 respectively;Stage 5: sub-pixel search at interpolated positions.SUMMARY OF THE INVENTIONA 
<a href="https://en.wikipedia.org/wiki/Motion_capture"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    parallel motion estimation
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 architecture is shown that enables efficient utilization of computational resources by making use of the inherent parallelism of the motion estimation algorithms.BRIEF DESCRIPTION OF THE DRAWINGSThese and other aspects of this invention are illustrated in the drawings, in which:FIG. 1 illustrates the organization of a typical 
<a href="https://en.wikipedia.org/wiki/Digital_signal_processor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    digital signal processor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 to which this invention is applicable (
<a href="https://en.wikipedia.org/wiki/Prior_art"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    prior art
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
);FIG. 2 illustrates details of a very long instruction word 
<a href="https://en.wikipedia.org/wiki/Digital_signal"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    digital signal
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
<a href="https://en.wikipedia.org/wiki/Central_processing_unit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    processor core
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 suitable for use in Figure (
<a href="https://en.wikipedia.org/wiki/Prior_art"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    prior art
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
);FIG. 3 illustrates the pipeline stages of the very long instruction word 
<a href="https://en.wikipedia.org/wiki/Digital_signal"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    digital signal
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 processor core illustrated in FIG. 2 (
<a href="https://en.wikipedia.org/wiki/Prior_art"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    prior art
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
);FIG. 4 illustrates the instruction syntax of the very long instruction word 
<a href="https://en.wikipedia.org/wiki/Digital_signal"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    digital signal
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 processor core illustrated in FIG. 2 (
<a href="https://en.wikipedia.org/wiki/Prior_art"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    prior art
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
);FIG. 5 illustrates an overview of the 
<a href="https://en.wikipedia.org/wiki/Data_compression"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    video encoding
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 process of the 
<a href="https://en.wikipedia.org/wiki/Prior_art"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    prior art
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
;FIG. 6 illustrates an overview of the video decoding process of the 
<a href="https://en.wikipedia.org/wiki/Prior_art"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    prior art
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
;FIG. 7 illustrates an overview of the motion estimation engine of this invention; andFIG. 8 illustrates one of the local buffers._____c:1. An apparatus for motion estimation comprising of:a predictor engine connected to a vector SAD engine and a subpel engine;a vector SAD (Sum of Absolute Difference) engine connected to said subpel engine and said predictor engine;a subpel (sub pixel) engine connected to said vector SAD engine, said predictor engine ant to an interpolation 
<a href="https://en.wikipedia.org/wiki/Buffer_solution"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    reference buffer
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
;a skip engine connected to a skip 
<a href="https://en.wikipedia.org/wiki/Data_buffer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    input buffer
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and a, MB (Macro Block) buffer; anda plurality of local buffers.2. The apparatus of claim 1 operable to execute independent operations 
<a href="https://en.wikipedia.org/wiki/Series_and_parallel_circuits"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    in parallel
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.3. The apparatus of claim 1 operable to execute multiple reference frame searches in a pipelined mode.4. The apparatus of claim 1 operable to process a plurality of macro blocks in a pipelined mode.5. The apparatus of claim 1 wherein said local buffers may include an interpolation 
<a href="https://en.wikipedia.org/wiki/Buffer_solution"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    reference buffer
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, a macro block skip buffer and a macro block skip 
<a href="https://en.wikipedia.org/wiki/Data_buffer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    input buffer
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.6. A method of motion estimation wherein global search, local integer search and local sub-pixel search operations are performed 
<a href="https://en.wikipedia.org/wiki/Series_and_parallel_circuits"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    in parallel
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.7. The method of claim 6 wherein said global search, local integer search and local sub-pixel search are performed in a pipelined mode._______________
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    IMAGE PROCESSING
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 APPARATUS, 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    IMAGE PROCESSING
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 METHOD, AND IMAGE PROCESSING SYSTEM_____20171228_____XMLs/xml/ipa171228.xml_____US-20170372485-A1 : US-15542685 : JP-2015-082275 : WO-PCT/JP2016/001897-00_____G06T0007254000 : G06K0009000000An 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus, including circuitry configured to generate or receive a first image of a sequence of images including an object. The circuitry is configured to determine a length of time movement of the object is below a predetermined movement threshold. The circuitry is further configured to identify the object as a target object based on the determined length of time the movement of the object is below the predetermined movement threshold._____d:CROSS REFERENCE TO RELATED APPLICATIONSThis application claims the benefit of Japanese Priority Patent Application JP 2015-082275 filed Apr. 14, 2015, the entire contents of which are incorporated herein by reference.TECHNICAL FIELDThe present disclosure relates to an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus, an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method, and an 
<a href="https://en.wikipedia.org/wiki/Image_processor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.BACKGROUND ARTIn related art, technology for segmenting a region of an object such as a person of a detection target, within a photographed image, has been variously developed.For example, PTL 1 discloses technology that detects moving bodies within an image photographed by a fish-eye lens camera, and respectively segments circumscribed quadrangle regions of each of the detected moving bodies. Further, PTL 2 discloses technology that extracts, based on position information of a partial region extracted with an immediately preceding frame image, and a physical feature amount analyzed from a present frame image, a partial region from each frame image. Further, PTL 3 discloses technology that detects a moving body with a size, an existing time, or a moving speed the largest from among moving bodies extracted from picture data, and segments a region that includes the detected moving body.CITATION LISTPatent LiteraturePTL 1: JP 2001-333422APTL 2: JP 2004-334587APTL 3: JP 2014-222825ASUMMARYTechnical ProblemHowever, in the technology disclosed in PTL 1 to PTL3, there will be cases where the position of a segmented region is restricted. For example, in the technology disclosed in PTL 3, when an object determined once to be a detection target continues to be positioned at the same location, the same location will continue to be set for a long time as a segmented region.Accordingly, the present disclosure proposes a new and improved 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus, 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method, and 
<a href="https://en.wikipedia.org/wiki/Image_processor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, capable of adaptively determining a segmented region for the length of time that an object of a detection target is stopped.Solution to ProblemAccording to an embodiment of the present disclosure, there is provided an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus, including circuitry configured to generate or receive a first image of a sequence of images including an object. The circuitry is configured to determine a length of time movement of the object is below a predetermined movement threshold. The circuitry is further configured to identify the object as a target object based on the determined length of time the movement of the object is below the predetermined movement threshold.According to an embodiment of the present disclosure, there is provided a method of an 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus for identifying a target object. The method includes generating or receiving a first image of a sequence of images including an object. A length of time movement of the object is below the predetermined movement threshold is determined by circuitry of the 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus. Further, the method includes identifying, by the circuitry, the object as the target object based on the determined length of time the movement of the object is below the predetermined movement threshold.According to an embodiment of the present disclosure, there is provided a non-transitory computer-readable medium storing instructions which when executed by a computer cause the computer to perform a method of an 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus for identifying a target object. The method includes generating or receiving a first image of a sequence of images including an object. A length of time movement of the object is below the predetermined movement threshold is determined. The method further includes identifying the object as the target object based on the determined length of time the movement of the object is below the predetermined movement threshold.Advantageous Effects of InventionAccording to an embodiment of the present disclosure such as described above, a segmented region can be adaptively determined for the length of time that an object of a detection target is stopped. Note that, the effect described here is not necessarily limited, and may be any of the effects described within the present disclosure.BRIEF DESCRIPTION OF DRAWINGSFIG. 1 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 that shows a configuration example of an 
<a href="https://en.wikipedia.org/wiki/Image_processor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to an embodiment of the present disclosure.FIG. 2 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 that shows an example of a reduced image 32 generated by a camera 10.FIG. 3 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 that shows an example of a plurality of cropped images 50 generated from a frame image 30.FIG. 4 is a function 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 that shows a configuration of the camera 10 according to a same embodiment.FIG. 5 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 that shows a relationship between the frame image 30 and a cropped region 40.FIG. 6 is a function 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 that shows a configuration of a monitoring terminal 22 according to a same embodiment.FIG. 7 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 that shows a display example of an evaluation standard setting screen according to a same embodiment.FIG. 8 is a function 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 that shows a configuration of a region setting unit 104 according to a same embodiment.FIG. 9 is a flow chart that shows the operations according to a same embodiment.FIG. 10 is a flow chart that shows a part of the operations of a cropped image generation process according to a same embodiment.FIG. 11 is a flow chart that shows a part of the operations of a cropped image generation process according to a same embodiment._____c:1. An 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus, comprising:circuitry configured togenerate or receive a first image of a sequence of images including an object,determine a length of time movement of the object is below a predetermined movement threshold, andidentify the object as a target object based on the determined length of time the movement of the object is below the predetermined movement threshold.2. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the circuitry is configured to:receive or generate the sequence of images, including the first image and a second image captured before the first image, the object being identified as the target object in the second image,determine whether to continue to identify the object as the target object in the first image based on the determined length of time the movement of the object is below the predetermined movement threshold, andidentify the object as the target object when the circuitry determines to continue to identify the object as the target object in the first image.3. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 2, wherein the circuitry is configured to change the target object from the object to a different object included in the first image when the circuitry determines not to continue identifying the object as the target object.4. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the circuitry is configured to:determine whether the length of time the movement of the object is below the predetermined movement threshold exceeds an upper 
<a href="https://en.wikipedia.org/wiki/Time_limit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time limit
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, andidentify the object as the target object when the length of time the movement of the object is below the predetermined movement threshold is less than or equal to the upper 
<a href="https://en.wikipedia.org/wiki/Time_limit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time limit
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.5. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the circuitry is configured to:determine whether the length of time the movement of the object is below the predetermined movement threshold exceeds an upper 
<a href="https://en.wikipedia.org/wiki/Time_limit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time limit
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, andidentify a different object included in the first image as the target object when the length of the time the movement of the object is below the predetermined movement threshold exceeds the upper 
<a href="https://en.wikipedia.org/wiki/Time_limit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time limit
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.6. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, further comprising:an 
<a href="https://en.wikipedia.org/wiki/Image_sensor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image sensor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 configured to capture the sequence of images,including the first image.7. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, whereinthe target object is a cropping target, andthe circuitry is configured to generate a cropped image by cropping the first image based on a position of the object within the first image when the object is identified as the target object.8. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, whereinthe circuitry is configured to transmit a plurality of image streams,the plurality of image streams includes cropped images of different portions of each of the sequence of images, andeach of the different portions corresponds to a different object.9. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 7, wherein the circuitry is configured to:generate a lower resolution version of the first image, andtransmit the cropped image and the lower resolution version of the first image.10. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the circuitry is configured to:identify the object as the target object based on the determined length of time the movement of the object is below the predetermined movement threshold and a length of time the object is identified as the target object.11. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the predetermined movement threshold corresponds to an amount of movement between successive images in the sequence of images.12. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the circuitry is configured to:set an upper 
<a href="https://en.wikipedia.org/wiki/Time_limit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time limit
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 based on a 
<a href="https://en.wikipedia.org/wiki/Input/output"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    user input
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, andidentify the object as the target object based on a comparison of the length of time the movement of the object is below the predetermined movement threshold and the set 
<a href="https://en.wikipedia.org/wiki/Upper-convected_time_derivative"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    upper time
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 limit.13. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the circuitry is configured to:set different 
<a href="https://en.wikipedia.org/wiki/Upper-convected_time_derivative"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    upper time
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 limits for different images, including the first image, in the sequence of images, andidentify the object as the target object in the first image based on a comparison of the length of time the movement of the object is below the predetermined movement threshold and the upper time limit set for the first image.14. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the circuitry is configured to:detect a plurality of objects included in the first image, andidentify a subset of the plurality of objects as target objects in the first image when a number of the plurality of objects exceeds a predetermined maximum number of target objects, the predetermined maximum number being greater than 1.15. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the circuitry is configured to:continue to identify the object as the target object in subsequent successive images of the sequence of images, including the object and captured after the first image, until the length of time the movement of the object is below the predetermined movement threshold exceeds an 
<a href="https://en.wikipedia.org/wiki/Limit_inferior_and_limit_superior"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    upper limit
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
<a href="https://en.wikipedia.org/wiki/Time_limit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time limit
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.16. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the circuitry is configured to:determine a plurality of cropped regions of the first image, each of the cropped images corresponding to a different target object, anddetermine whether a first one of the plurality of cropped regions overlaps with a second one of the plurality of cropped regions.17. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 16, wherein the circuitry is configured to:determine that the first one of the plurality of cropped regions overlaps with the second one of the plurality of cropped regions when an area of an overlapping region for the first one of the plurality of cropped regions and the second one of the plurality of cropped regions exceeds a first predetermined overlap threshold or a distance between the centers of the first one of the plurality of cropped regions and the second one of the plurality of cropped regions exceeds a second predetermined overlap threshold.18. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the length of time the movement of the object is below the predetermined movement threshold indicates a length of time the object has stopped.19. A method of an 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus for identifying a target object, the method comprising:generating or receiving a first image of a sequence of images including an object;determining, by circuitry of the 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus, a length of time movement of the object is below the predetermined movement threshold, andidentifying, by the circuitry, the object as the target object based on the determined length of time the movement of the object is below the predetermined movement threshold.20. A non-transitory computer-readable medium storing instructions which when executed by a computer cause the computer to perform a method of an 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus for identifying a target object, the method comprising:generating or receiving a first image of a sequence of images including an object;determining, a length of time movement of the object is below the predetermined movement threshold, andidentifying the object as the target object based on the determined length of time the movement of the object is below the predetermined movement threshold._______________METHODS OF AND APPARATUSES FOR RECOGNIZING MOTION OF OBJECTS, AND ASSOCIATED SYSTEMS_____20171228_____XMLs/xml/ipa171228.xml_____US-20170372486-A1 : US-15636751 : KR-10-2012-0120883 : US-14064639 : US-9715740 : US-15636751_____G06T0007254000A method of recognizing motion of an object may include periodically obtaining 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a first resolution and two-dimensional data of a second resolution with respect to a scene using an image capturing device, wherein the second resolution is higher than the first resolution; determining a 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region by recognizing a target object in the scene based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, such that the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region corresponds to a portion of a frame and the portion includes the target object; periodically obtaining tracking region data of the second resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region; and/or analyzing the motion of the target object based on the tracking region data._____d:CROSS-REFERENCE TO RELATED APPLICATION(S)This application is a continuation of U.S. application Ser. No. 14/064,639, filed on Oct. 28, 2013, which claims priority from Korean Patent Application No. 10-2012-0120883, filed on Oct. 30, 2012, in the Korean 
<a href="https://en.wikipedia.org/wiki/Intellectual_property"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Intellectual Property
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 Office (KIPO), the entire contents of each of which are incorporated herein by reference.BACKGROUND1. FieldSome example embodiments may relate generally to processing of image data. Some example embodiments may relate to methods of and/or apparatuses for recognizing motion of objects based on 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and/or two-dimensional data.2. Description of Related ArtA two-dimensional 
<a href="https://en.wikipedia.org/wiki/Image_sensor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image sensor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may be used to obtain two-dimensional data and the two-dimensional data may be used to recognize a shape and/or a motion of an object. Particularly the technology for recognizing the motion of a user is developed to support a 
<a href="https://en.wikipedia.org/wiki/User_interface"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    user interface
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. The two-dimensional data for the 
<a href="https://en.wikipedia.org/wiki/Golden_Globe_Award_for_Best_Actor_–_Motion_Picture_Drama"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion recognition
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may include color image data or black and white image data.Alternatively a 
<a href="https://en.wikipedia.org/wiki/Range_imaging"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth sensor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may be used to obtain 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may be used to recognize the shape and/or the motion of the object. The 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for the 
<a href="https://en.wikipedia.org/wiki/Golden_Globe_Award_for_Best_Actor_–_Motion_Picture_Drama"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion recognition
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may include information of a distance to the object from the sensor.In general, the two-dimensional data may be provided with relatively a higher resolution, but it is difficult to distinguish the object from the background based on the two-dimensional data during the data process for the motion recognition. The 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may be provided with relatively a lower resolution and thus it is difficult to discern the complex shape of the object based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 during the data process for the 
<a href="https://en.wikipedia.org/wiki/Golden_Globe_Award_for_Best_Actor_–_Motion_Picture_Drama"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion recognition
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.SUMMARYSome example embodiments of the inventive concept may provide methods of recognizing motion of objects, capable of discerning the motion of the objects efficiently based on 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and/or two-dimensional data.Some example embodiments of the inventive concept may provide apparatuses for recognizing motion of objects, capable of discerning the motion of the objects efficiently based on 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and/or two-dimensional data.Some example embodiments of the inventive concept may provide systems adopting the methods and/or apparatuses of recognizing the motion of the objects.In some example embodiments, a method of recognizing motion of an object may comprise periodically obtaining 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a first resolution and two-dimensional data of a second resolution with respect to a scene using an image capturing device, wherein the second resolution is higher than the first resolution; determining a 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region by recognizing a target object in the scene based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, such that the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region corresponds to a portion of a frame and the portion includes the target object; periodically obtaining tracking region data of the second resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region; and/or analyzing the motion of the target object based on the tracking region data.In some example embodiments, periodically obtaining the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the two-dimensional data may comprise providing the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 corresponding to the frame with a first frame period using depth pixels of the first resolution; and/or providing the two-dimensional data corresponding to the frame with a second frame period using color pixels of the second resolution.In some example embodiments, the method may further comprise synchronizing the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the two-dimensional data to be matched with each other, when the first frame period is different from the second frame period.In some example embodiments, the tracking region data corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region may be provided with the first frame period or the second frame period.In some example embodiments, periodically obtaining the tracking region data of the second resolution may comprise extracting region image data of the second resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region from the two-dimensional data of the second resolution corresponding to the frame; and/or providing the region image data of the second resolution as the tracking region data.In some example embodiments, periodically obtaining the tracking region data of the second resolution may comprise extracting region depth data of the first resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region from the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the first resolution corresponding to the frame; extracting region image data of the second resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region from the two-dimensional data of the second resolution corresponding to the frame; compensating for the region depth data of the first resolution using the region image data of the second resolution to generate region depth data of the second resolution; and/or providing the region depth data of the second resolution as the tracking region data.In some example embodiments, the depth pixels and the color pixels may be arranged in a common pixel array.In some example embodiments, the depth pixels and the color pixels may be arranged respectively in distinct pixel arrays that are spaced apart from each other.In some example embodiments, periodically obtaining the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the two-dimensional data may comprise periodically providing raw data corresponding to the frame using time-of-flight (TOF) depth pixels, the TOF depth pixels operating in response to a plurality of demodulation signals having different phases from each other; and/or calculating the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the first resolution and the two-dimensional data of the second resolution based on the raw data.In some example embodiments, calculating the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the first resolution and the two-dimensional data of the second resolution may comprise providing the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the first resolution by combining every M bits of the raw data, where M is a positive integer equal to or greater than two and/or providing the two-dimensional data of the second resolution by combining every N bits of the raw data, where N is a positive integer equal to or smaller than M.In some example embodiments, the demodulation signals may have phase difference of 0, 90, 180 and 270 degrees, respectively, with respect to transmission light radiated from the image capturing device.In some example embodiments, providing the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the first resolution may comprise providing one 
<a href="https://en.wikipedia.org/wiki/Parity_bit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    bit value
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 based on four bit values of the raw data, the four bit values respectively corresponding to the four demodulation signals having the phase difference of 0, 90, 180 and 270 degrees, respectively.In some example embodiments, providing the two-dimensional data of the second resolution may comprise providing one 
<a href="https://en.wikipedia.org/wiki/Parity_bit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    bit value
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the two-dimensional data by summing two bit values of the raw data, the two bit values respectively corresponding to the two demodulation signals having the phase differences of 0 and 180 degrees; and/or providing another bit value of the two-dimensional data by summing other two bit values of the raw data, the other two bit values respectively corresponding to the two demodulation signals having the phase differences of 90 and 270 degrees.In some example embodiments, periodically obtaining the tracking region data of the second resolution may comprise extracting region depth data of the first resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region from the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the first resolution corresponding to the frame; extracting region image data of the second resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region from the two-dimensional data of the second resolution corresponding to the frame; compensating for the region depth data of the first resolution using the region image data of the second resolution to generate region depth data of the second resolution; and/or providing the region depth data of the second resolution as the tracking region data.In some example embodiments, determining the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region may comprise determining coordinates of a center point of the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region in the frame; and/or determining a size of the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region in the frame.In some example embodiments, the method may further comprise upgrading the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region according to the motion of the target object.In some example embodiments, upgrading the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region may comprise detecting a change of a position of the target object in the scene based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and/or changing coordinates of a center point of the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region in the frame based on the change of the position of the target object in the scene.In some example embodiments, upgrading the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region may comprise detecting a change of distance to the target object based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
; decreasing a size of the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region when the distance to the target object increases; and/or increasing the size of the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region when the distance to the target object decreases.In some example embodiments, an apparatus for recognizing motion of an object may comprise an image capturing device configured to periodically provide 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a first resolution and two-dimensional data of a second resolution with respect to a scene, wherein the second resolution is higher than the first resolution; a motion region tracker configured to determine a 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region by recognizing a target object in the scene based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, such that the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region corresponds to a portion of a frame and the portion includes the target object, and configured to periodically provide tracking region data of the second resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region; and/or a motion analyzer configured to analyze the motion of the target object based on the tracking region data.In some example embodiments, the image capturing device may comprise a pixel array in which depth pixels of the first resolution and color pixels of the second resolution are alternatively arranged, the depth pixels providing the depth data with a first frame period, and the color pixels providing the two-dimensional data with a second frame period.In some example embodiments, the image capturing device may comprise a first pixel array in which depth pixels of the first resolution are arranged, the depth pixels providing the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 with a first frame period; and/or a second pixel array in which color pixels of the second resolution are arranged, the color pixels providing the two-dimensional data with a second frame period.In some example embodiments, the image capturing device may comprise a pixel array in which time-of-flight (TOF) depth pixels are arranged, the TOF depth pixels operating in response to a plurality of demodulation signals having different phases from each other to periodically provide raw data corresponding to the frame.In some example embodiments, the demodulation signals may have phase difference of 0, 90, 180, and 270 degrees, respectively, with respect to transmission light radiated from the image capturing device, and/or one 
<a href="https://en.wikipedia.org/wiki/Parity_bit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    bit value
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the depth data may be provided based on four bit values of the raw data, the four bit values respectively corresponding to the four demodulation signals having the phase difference of 0, 90, 180, and 270 degrees, respectively.In some example embodiments, a system may comprise an image capturing device configured to periodically provide 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a first resolution corresponding to a frame of a scene and two-dimensional data of a second resolution corresponding to the frame, wherein the second resolution is higher than the first resolution; a motion region tracker configured to determine a 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region by recognizing a target object in the scene based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, such that the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region corresponds to a portion of the frame and the portion includes the target object, and configured to periodically provide tracking region data of the second resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region; a motion analyzer configured to analyze motion of the target object based on the tracking region data; and/or a 
<a href="https://en.wikipedia.org/wiki/Remote_control"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    control device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 configured to generate an event corresponding to the motion of the target object based on an analysis result of the 
<a href="https://en.wikipedia.org/wiki/Mass_spectrometry"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion analyzer
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.In some example embodiments, 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    the system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may be a 
<a href="https://en.wikipedia.org/wiki/User_interface"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    user interface
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system that operates by recognizing motion of a user. The target object may include a body of the user or a portion of the body of the user.In some example embodiments, an apparatus for recognizing motion of an object may comprise a first device configured to provide 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for a scene that includes the object at a first resolution and two-dimensional data for the scene at a second resolution; a second device configured to determine a 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region by recognizing the object based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and configured to provide tracking region data of the second resolution corresponding to the motion tracking region; and/or a third device configured to analyze the motion of the object based on the tracking region data. The second resolution may be higher than the first resolution. The 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region may correspond to a portion of a frame. The portion of the frame may include the object.In some example embodiments, the first device may comprise a sensing unit. The sensing unit may comprise a depth pixel array. The depth pixel array may be configured to output depth information.In some example embodiments, the first device may comprise a sensing unit. The sensing unit may comprise a color pixel array. The color pixel array may be configured to output color information.In some example embodiments, the first device may comprise a sensing unit. The sensing unit may comprise a depth pixel array and a color pixel array. The depth pixel array may be configured to output depth information. The color pixel array may be configured to output color information.In some example embodiments, the first device may comprise a sensing unit. The sensing unit may comprise a pixel array. The pixel array may be configured to output depth information, color information, or depth and color information.In some example embodiments, a method for recognizing motion of an object may comprise obtaining 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a first resolution with respect to a scene; obtaining two-dimensional data of a second resolution with respect to the scene; recognizing the object in the scene based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
; tracking the object using a 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region to provide tracking region data of the second resolution; and/or analyzing the motion of the object based on the tracking region data. The second resolution may be higher than the first resolution.In some example embodiments, obtaining the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the first resolution may comprise using a depth pixel array of a sensing unit to output depth information.In some example embodiments, obtaining the two-dimensional data of the second resolution may comprise using a color pixel array of a sensing unit to output color information.In some example embodiments, obtaining the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the first resolution may comprise using a depth pixel array of a sensing unit to output depth information and/or obtaining the two-dimensional data of the second resolution may comprise using a color pixel array of the sensing unit to output color information.In some example embodiments, obtaining the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the first resolution may comprise using a pixel array of a sensing unit to output depth information and/or obtaining the two-dimensional data of the second resolution may comprise using the pixel array of the sensing unit to output color information.BRIEF DESCRIPTION OF THE DRAWINGSThe above and/or other aspects and advantages will become more apparent and more readily appreciated from the following detailed description of example embodiments, taken in conjunction with the accompanying drawings, in which:FIG. 1 is a flowchart illustrating a method of recognizing a motion of an object according to some example embodiments of the inventive concept;FIG. 2 is a diagram illustrating an example of using a system according to some example embodiments of the inventive concept;FIG. 3 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating a system according to some example embodiments of the inventive concept;FIG. 4 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating an example of an image capturing device in a motion recognizing device according to some example embodiments of the inventive concept;FIG. 5 is a diagram illustrating an example of a sensing unit in the image capturing device of FIG. 4;FIG. 6 is a diagram illustrating an example of a pixel array in the sensing unit of FIG. 5;FIG. 7 is a diagram illustrating a frame of an example 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 obtained by an image capturing device;FIG. 8 is a diagram illustrating a frame of an example two-dimensional data obtained by an image capturing device;FIG. 9 is a diagram illustrating an example of a 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region determined according to some example embodiments of the inventive concept;FIG. 10 is a diagram illustrating an example of a sensing unit in the image capturing device of FIG. 4;FIGS. 11A and 11B are diagrams illustrating example pixel arrays in the sensing unit of FIG. 10;FIGS. 12A, 12B, 12C, and 12D are circuit diagrams illustrating some example unit pixels in a pixel array;FIG. 13 is a flowchart illustrating a method of recognizing a motion of an object according to some example embodiments of the inventive concept;FIG. 14 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating an example of a motion region tracker in a motion recognizing device according to some example embodiments of the inventive concept;FIGS. 15A, 15B, and 15C are diagrams illustrating example operations of a synchronizer in the motion region tracker of FIG. 14;FIG. 16 is a diagram for describing tracking region data provided by the motion region tracker of FIG. 14;FIGS. 17A and 17B are diagrams for describing a method of upgrading a 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region according to some example embodiments of the inventive concept;FIG. 18 is a flowchart illustrating a method of recognizing a motion of an object according to some example embodiments of the inventive concept;FIG. 19 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating an example of a motion region tracker in a motion recognizing device according to some example embodiments of the inventive concept;FIG. 20 is a diagram for describing tracking region data provided by the motion region tracker of FIG. 19;FIG. 21 is a flowchart illustrating a method of recognizing a motion of an object according to some example embodiments of the inventive concept;FIG. 22 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating an example of a motion region tracker in a motion recognizing device according to some example embodiments of the inventive concept;FIG. 23 is a diagram illustrating an example of a pixel array included in a 
<a href="https://en.wikipedia.org/wiki/Range_imaging"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth sensor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
;FIG. 24 is a 
<a href="https://en.wikipedia.org/wiki/Circuit_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    circuit diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating example time-of-flight (TOF) depth pixels in the pixel array of FIG. 23;FIG. 25 is a 
<a href="https://en.wikipedia.org/wiki/Timing_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    timing diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating an operation of the TOF pixels of FIG. 24;FIG. 26 is a diagram illustrating an example combination for providing 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
;FIG. 27 is a diagram for describing a method of calculating two-dimensional data based on raw data obtained using depth pixels;FIGS. 28A and 28B are diagrams example combinations for providing two-dimensional data;FIG. 29 illustrates a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a camera including a three-dimensional 
<a href="https://en.wikipedia.org/wiki/Image_sensor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image sensor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to some example embodiments of the inventive concept;FIG. 30 illustrates a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a 
<a href="https://en.wikipedia.org/wiki/Computer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 including a motion recognizing device according to some example embodiments of the inventive concept; andFIG. 31 illustrates a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of interface employable in the computing system of FIG. 30 according to some example embodiments of the inventive concept._____c:1. A method of recognizing motion of an object, the method comprising:periodically obtaining 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a first resolution and two-dimensional data of a second resolution with respect to a scene using an image capturing device, wherein the second resolution is higher than the first resolution;determining a 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region by recognizing a target object in the scene based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, such that the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region corresponds to a portion of a frame and the portion includes the target object;periodically obtaining tracking region data of the second resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region; andanalyzing the motion of the target object based on the tracking region data._______________
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    IMAGE PROCESSING
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 APPARATUS, 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    IMAGE PROCESSING
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 METHOD, AND IMAGE PROCESSING SYSTEM_____20171228_____XMLs/xml/ipa171228.xml_____US-20170372485-A1 : US-15542685 : JP-2015-082275 : WO-PCT/JP2016/001897-00_____G06T0007254000 : G06K0009000000An 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus, including circuitry configured to generate or receive a first image of a sequence of images including an object. The circuitry is configured to determine a length of time movement of the object is below a predetermined movement threshold. The circuitry is further configured to identify the object as a target object based on the determined length of time the movement of the object is below the predetermined movement threshold._____d:CROSS REFERENCE TO RELATED APPLICATIONSThis application claims the benefit of Japanese Priority Patent Application JP 2015-082275 filed Apr. 14, 2015, the entire contents of which are incorporated herein by reference.TECHNICAL FIELDThe present disclosure relates to an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus, an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method, and an 
<a href="https://en.wikipedia.org/wiki/Image_processor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.BACKGROUND ARTIn related art, technology for segmenting a region of an object such as a person of a detection target, within a photographed image, has been variously developed.For example, PTL 1 discloses technology that detects moving bodies within an image photographed by a fish-eye lens camera, and respectively segments circumscribed quadrangle regions of each of the detected moving bodies. Further, PTL 2 discloses technology that extracts, based on position information of a partial region extracted with an immediately preceding frame image, and a physical feature amount analyzed from a present frame image, a partial region from each frame image. Further, PTL 3 discloses technology that detects a moving body with a size, an existing time, or a moving speed the largest from among moving bodies extracted from picture data, and segments a region that includes the detected moving body.CITATION LISTPatent LiteraturePTL 1: JP 2001-333422APTL 2: JP 2004-334587APTL 3: JP 2014-222825ASUMMARYTechnical ProblemHowever, in the technology disclosed in PTL 1 to PTL3, there will be cases where the position of a segmented region is restricted. For example, in the technology disclosed in PTL 3, when an object determined once to be a detection target continues to be positioned at the same location, the same location will continue to be set for a long time as a segmented region.Accordingly, the present disclosure proposes a new and improved 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus, 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 method, and 
<a href="https://en.wikipedia.org/wiki/Image_processor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, capable of adaptively determining a segmented region for the length of time that an object of a detection target is stopped.Solution to ProblemAccording to an embodiment of the present disclosure, there is provided an 
<a href="https://en.wikipedia.org/wiki/Digital_image_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus, including circuitry configured to generate or receive a first image of a sequence of images including an object. The circuitry is configured to determine a length of time movement of the object is below a predetermined movement threshold. The circuitry is further configured to identify the object as a target object based on the determined length of time the movement of the object is below the predetermined movement threshold.According to an embodiment of the present disclosure, there is provided a method of an 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus for identifying a target object. The method includes generating or receiving a first image of a sequence of images including an object. A length of time movement of the object is below the predetermined movement threshold is determined by circuitry of the 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus. Further, the method includes identifying, by the circuitry, the object as the target object based on the determined length of time the movement of the object is below the predetermined movement threshold.According to an embodiment of the present disclosure, there is provided a non-transitory computer-readable medium storing instructions which when executed by a computer cause the computer to perform a method of an 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus for identifying a target object. The method includes generating or receiving a first image of a sequence of images including an object. A length of time movement of the object is below the predetermined movement threshold is determined. The method further includes identifying the object as the target object based on the determined length of time the movement of the object is below the predetermined movement threshold.Advantageous Effects of InventionAccording to an embodiment of the present disclosure such as described above, a segmented region can be adaptively determined for the length of time that an object of a detection target is stopped. Note that, the effect described here is not necessarily limited, and may be any of the effects described within the present disclosure.BRIEF DESCRIPTION OF DRAWINGSFIG. 1 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 that shows a configuration example of an 
<a href="https://en.wikipedia.org/wiki/Image_processor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image processing system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to an embodiment of the present disclosure.FIG. 2 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 that shows an example of a reduced image 32 generated by a camera 10.FIG. 3 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 that shows an example of a plurality of cropped images 50 generated from a frame image 30.FIG. 4 is a function 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 that shows a configuration of the camera 10 according to a same embodiment.FIG. 5 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 that shows a relationship between the frame image 30 and a cropped region 40.FIG. 6 is a function 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 that shows a configuration of a monitoring terminal 22 according to a same embodiment.FIG. 7 is an 
<a href="https://en.wikipedia.org/wiki/Diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    explanatory diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 that shows a display example of an evaluation standard setting screen according to a same embodiment.FIG. 8 is a function 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 that shows a configuration of a region setting unit 104 according to a same embodiment.FIG. 9 is a flow chart that shows the operations according to a same embodiment.FIG. 10 is a flow chart that shows a part of the operations of a cropped image generation process according to a same embodiment.FIG. 11 is a flow chart that shows a part of the operations of a cropped image generation process according to a same embodiment._____c:1. An 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus, comprising:circuitry configured togenerate or receive a first image of a sequence of images including an object,determine a length of time movement of the object is below a predetermined movement threshold, andidentify the object as a target object based on the determined length of time the movement of the object is below the predetermined movement threshold.2. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the circuitry is configured to:receive or generate the sequence of images, including the first image and a second image captured before the first image, the object being identified as the target object in the second image,determine whether to continue to identify the object as the target object in the first image based on the determined length of time the movement of the object is below the predetermined movement threshold, andidentify the object as the target object when the circuitry determines to continue to identify the object as the target object in the first image.3. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 2, wherein the circuitry is configured to change the target object from the object to a different object included in the first image when the circuitry determines not to continue identifying the object as the target object.4. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the circuitry is configured to:determine whether the length of time the movement of the object is below the predetermined movement threshold exceeds an upper 
<a href="https://en.wikipedia.org/wiki/Time_limit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time limit
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, andidentify the object as the target object when the length of time the movement of the object is below the predetermined movement threshold is less than or equal to the upper 
<a href="https://en.wikipedia.org/wiki/Time_limit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time limit
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.5. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the circuitry is configured to:determine whether the length of time the movement of the object is below the predetermined movement threshold exceeds an upper 
<a href="https://en.wikipedia.org/wiki/Time_limit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time limit
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, andidentify a different object included in the first image as the target object when the length of the time the movement of the object is below the predetermined movement threshold exceeds the upper 
<a href="https://en.wikipedia.org/wiki/Time_limit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time limit
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.6. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, further comprising:an 
<a href="https://en.wikipedia.org/wiki/Image_sensor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image sensor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 configured to capture the sequence of images,including the first image.7. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, whereinthe target object is a cropping target, andthe circuitry is configured to generate a cropped image by cropping the first image based on a position of the object within the first image when the object is identified as the target object.8. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, whereinthe circuitry is configured to transmit a plurality of image streams,the plurality of image streams includes cropped images of different portions of each of the sequence of images, andeach of the different portions corresponds to a different object.9. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 7, wherein the circuitry is configured to:generate a lower resolution version of the first image, andtransmit the cropped image and the lower resolution version of the first image.10. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the circuitry is configured to:identify the object as the target object based on the determined length of time the movement of the object is below the predetermined movement threshold and a length of time the object is identified as the target object.11. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the predetermined movement threshold corresponds to an amount of movement between successive images in the sequence of images.12. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the circuitry is configured to:set an upper 
<a href="https://en.wikipedia.org/wiki/Time_limit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time limit
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 based on a 
<a href="https://en.wikipedia.org/wiki/Input/output"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    user input
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, andidentify the object as the target object based on a comparison of the length of time the movement of the object is below the predetermined movement threshold and the set 
<a href="https://en.wikipedia.org/wiki/Upper-convected_time_derivative"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    upper time
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 limit.13. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the circuitry is configured to:set different 
<a href="https://en.wikipedia.org/wiki/Upper-convected_time_derivative"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    upper time
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 limits for different images, including the first image, in the sequence of images, andidentify the object as the target object in the first image based on a comparison of the length of time the movement of the object is below the predetermined movement threshold and the upper time limit set for the first image.14. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the circuitry is configured to:detect a plurality of objects included in the first image, andidentify a subset of the plurality of objects as target objects in the first image when a number of the plurality of objects exceeds a predetermined maximum number of target objects, the predetermined maximum number being greater than 1.15. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the circuitry is configured to:continue to identify the object as the target object in subsequent successive images of the sequence of images, including the object and captured after the first image, until the length of time the movement of the object is below the predetermined movement threshold exceeds an 
<a href="https://en.wikipedia.org/wiki/Limit_inferior_and_limit_superior"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    upper limit
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
<a href="https://en.wikipedia.org/wiki/Time_limit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    time limit
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.16. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the circuitry is configured to:determine a plurality of cropped regions of the first image, each of the cropped images corresponding to a different target object, anddetermine whether a first one of the plurality of cropped regions overlaps with a second one of the plurality of cropped regions.17. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 16, wherein the circuitry is configured to:determine that the first one of the plurality of cropped regions overlaps with the second one of the plurality of cropped regions when an area of an overlapping region for the first one of the plurality of cropped regions and the second one of the plurality of cropped regions exceeds a first predetermined overlap threshold or a distance between the centers of the first one of the plurality of cropped regions and the second one of the plurality of cropped regions exceeds a second predetermined overlap threshold.18. The 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus according to claim 1, wherein the length of time the movement of the object is below the predetermined movement threshold indicates a length of time the object has stopped.19. A method of an 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus for identifying a target object, the method comprising:generating or receiving a first image of a sequence of images including an object;determining, by circuitry of the 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus, a length of time movement of the object is below the predetermined movement threshold, andidentifying, by the circuitry, the object as the target object based on the determined length of time the movement of the object is below the predetermined movement threshold.20. A non-transitory computer-readable medium storing instructions which when executed by a computer cause the computer to perform a method of an 
<a href="https://en.wikipedia.org/wiki/Information_processing"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    information processing
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 apparatus for identifying a target object, the method comprising:generating or receiving a first image of a sequence of images including an object;determining, a length of time movement of the object is below the predetermined movement threshold, andidentifying the object as the target object based on the determined length of time the movement of the object is below the predetermined movement threshold._______________METHODS OF AND APPARATUSES FOR RECOGNIZING MOTION OF OBJECTS, AND ASSOCIATED SYSTEMS_____20171228_____XMLs/xml/ipa171228.xml_____US-20170372486-A1 : US-15636751 : KR-10-2012-0120883 : US-14064639 : US-9715740 : US-15636751_____G06T0007254000A method of recognizing motion of an object may include periodically obtaining 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a first resolution and two-dimensional data of a second resolution with respect to a scene using an image capturing device, wherein the second resolution is higher than the first resolution; determining a 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region by recognizing a target object in the scene based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, such that the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region corresponds to a portion of a frame and the portion includes the target object; periodically obtaining tracking region data of the second resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region; and/or analyzing the motion of the target object based on the tracking region data._____d:CROSS-REFERENCE TO RELATED APPLICATION(S)This application is a continuation of U.S. application Ser. No. 14/064,639, filed on Oct. 28, 2013, which claims priority from Korean Patent Application No. 10-2012-0120883, filed on Oct. 30, 2012, in the Korean 
<a href="https://en.wikipedia.org/wiki/Intellectual_property"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Intellectual Property
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 Office (KIPO), the entire contents of each of which are incorporated herein by reference.BACKGROUND1. FieldSome example embodiments may relate generally to processing of image data. Some example embodiments may relate to methods of and/or apparatuses for recognizing motion of objects based on 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and/or two-dimensional data.2. Description of Related ArtA two-dimensional 
<a href="https://en.wikipedia.org/wiki/Image_sensor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image sensor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may be used to obtain two-dimensional data and the two-dimensional data may be used to recognize a shape and/or a motion of an object. Particularly the technology for recognizing the motion of a user is developed to support a 
<a href="https://en.wikipedia.org/wiki/User_interface"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    user interface
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
. The two-dimensional data for the 
<a href="https://en.wikipedia.org/wiki/Golden_Globe_Award_for_Best_Actor_–_Motion_Picture_Drama"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion recognition
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may include color image data or black and white image data.Alternatively a 
<a href="https://en.wikipedia.org/wiki/Range_imaging"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth sensor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may be used to obtain 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may be used to recognize the shape and/or the motion of the object. The 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for the 
<a href="https://en.wikipedia.org/wiki/Golden_Globe_Award_for_Best_Actor_–_Motion_Picture_Drama"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion recognition
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may include information of a distance to the object from the sensor.In general, the two-dimensional data may be provided with relatively a higher resolution, but it is difficult to distinguish the object from the background based on the two-dimensional data during the data process for the motion recognition. The 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may be provided with relatively a lower resolution and thus it is difficult to discern the complex shape of the object based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 during the data process for the 
<a href="https://en.wikipedia.org/wiki/Golden_Globe_Award_for_Best_Actor_–_Motion_Picture_Drama"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion recognition
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.SUMMARYSome example embodiments of the inventive concept may provide methods of recognizing motion of objects, capable of discerning the motion of the objects efficiently based on 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and/or two-dimensional data.Some example embodiments of the inventive concept may provide apparatuses for recognizing motion of objects, capable of discerning the motion of the objects efficiently based on 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and/or two-dimensional data.Some example embodiments of the inventive concept may provide systems adopting the methods and/or apparatuses of recognizing the motion of the objects.In some example embodiments, a method of recognizing motion of an object may comprise periodically obtaining 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a first resolution and two-dimensional data of a second resolution with respect to a scene using an image capturing device, wherein the second resolution is higher than the first resolution; determining a 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region by recognizing a target object in the scene based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, such that the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region corresponds to a portion of a frame and the portion includes the target object; periodically obtaining tracking region data of the second resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region; and/or analyzing the motion of the target object based on the tracking region data.In some example embodiments, periodically obtaining the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the two-dimensional data may comprise providing the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 corresponding to the frame with a first frame period using depth pixels of the first resolution; and/or providing the two-dimensional data corresponding to the frame with a second frame period using color pixels of the second resolution.In some example embodiments, the method may further comprise synchronizing the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the two-dimensional data to be matched with each other, when the first frame period is different from the second frame period.In some example embodiments, the tracking region data corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region may be provided with the first frame period or the second frame period.In some example embodiments, periodically obtaining the tracking region data of the second resolution may comprise extracting region image data of the second resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region from the two-dimensional data of the second resolution corresponding to the frame; and/or providing the region image data of the second resolution as the tracking region data.In some example embodiments, periodically obtaining the tracking region data of the second resolution may comprise extracting region depth data of the first resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region from the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the first resolution corresponding to the frame; extracting region image data of the second resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region from the two-dimensional data of the second resolution corresponding to the frame; compensating for the region depth data of the first resolution using the region image data of the second resolution to generate region depth data of the second resolution; and/or providing the region depth data of the second resolution as the tracking region data.In some example embodiments, the depth pixels and the color pixels may be arranged in a common pixel array.In some example embodiments, the depth pixels and the color pixels may be arranged respectively in distinct pixel arrays that are spaced apart from each other.In some example embodiments, periodically obtaining the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and the two-dimensional data may comprise periodically providing raw data corresponding to the frame using time-of-flight (TOF) depth pixels, the TOF depth pixels operating in response to a plurality of demodulation signals having different phases from each other; and/or calculating the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the first resolution and the two-dimensional data of the second resolution based on the raw data.In some example embodiments, calculating the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the first resolution and the two-dimensional data of the second resolution may comprise providing the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the first resolution by combining every M bits of the raw data, where M is a positive integer equal to or greater than two and/or providing the two-dimensional data of the second resolution by combining every N bits of the raw data, where N is a positive integer equal to or smaller than M.In some example embodiments, the demodulation signals may have phase difference of 0, 90, 180 and 270 degrees, respectively, with respect to transmission light radiated from the image capturing device.In some example embodiments, providing the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the first resolution may comprise providing one 
<a href="https://en.wikipedia.org/wiki/Parity_bit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    bit value
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 based on four bit values of the raw data, the four bit values respectively corresponding to the four demodulation signals having the phase difference of 0, 90, 180 and 270 degrees, respectively.In some example embodiments, providing the two-dimensional data of the second resolution may comprise providing one 
<a href="https://en.wikipedia.org/wiki/Parity_bit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    bit value
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the two-dimensional data by summing two bit values of the raw data, the two bit values respectively corresponding to the two demodulation signals having the phase differences of 0 and 180 degrees; and/or providing another bit value of the two-dimensional data by summing other two bit values of the raw data, the other two bit values respectively corresponding to the two demodulation signals having the phase differences of 90 and 270 degrees.In some example embodiments, periodically obtaining the tracking region data of the second resolution may comprise extracting region depth data of the first resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region from the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the first resolution corresponding to the frame; extracting region image data of the second resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region from the two-dimensional data of the second resolution corresponding to the frame; compensating for the region depth data of the first resolution using the region image data of the second resolution to generate region depth data of the second resolution; and/or providing the region depth data of the second resolution as the tracking region data.In some example embodiments, determining the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region may comprise determining coordinates of a center point of the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region in the frame; and/or determining a size of the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region in the frame.In some example embodiments, the method may further comprise upgrading the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region according to the motion of the target object.In some example embodiments, upgrading the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region may comprise detecting a change of a position of the target object in the scene based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and/or changing coordinates of a center point of the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region in the frame based on the change of the position of the target object in the scene.In some example embodiments, upgrading the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region may comprise detecting a change of distance to the target object based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
; decreasing a size of the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region when the distance to the target object increases; and/or increasing the size of the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region when the distance to the target object decreases.In some example embodiments, an apparatus for recognizing motion of an object may comprise an image capturing device configured to periodically provide 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a first resolution and two-dimensional data of a second resolution with respect to a scene, wherein the second resolution is higher than the first resolution; a motion region tracker configured to determine a 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region by recognizing a target object in the scene based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, such that the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region corresponds to a portion of a frame and the portion includes the target object, and configured to periodically provide tracking region data of the second resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region; and/or a motion analyzer configured to analyze the motion of the target object based on the tracking region data.In some example embodiments, the image capturing device may comprise a pixel array in which depth pixels of the first resolution and color pixels of the second resolution are alternatively arranged, the depth pixels providing the depth data with a first frame period, and the color pixels providing the two-dimensional data with a second frame period.In some example embodiments, the image capturing device may comprise a first pixel array in which depth pixels of the first resolution are arranged, the depth pixels providing the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 with a first frame period; and/or a second pixel array in which color pixels of the second resolution are arranged, the color pixels providing the two-dimensional data with a second frame period.In some example embodiments, the image capturing device may comprise a pixel array in which time-of-flight (TOF) depth pixels are arranged, the TOF depth pixels operating in response to a plurality of demodulation signals having different phases from each other to periodically provide raw data corresponding to the frame.In some example embodiments, the demodulation signals may have phase difference of 0, 90, 180, and 270 degrees, respectively, with respect to transmission light radiated from the image capturing device, and/or one 
<a href="https://en.wikipedia.org/wiki/Parity_bit"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    bit value
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the depth data may be provided based on four bit values of the raw data, the four bit values respectively corresponding to the four demodulation signals having the phase difference of 0, 90, 180, and 270 degrees, respectively.In some example embodiments, a system may comprise an image capturing device configured to periodically provide 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a first resolution corresponding to a frame of a scene and two-dimensional data of a second resolution corresponding to the frame, wherein the second resolution is higher than the first resolution; a motion region tracker configured to determine a 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region by recognizing a target object in the scene based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, such that the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region corresponds to a portion of the frame and the portion includes the target object, and configured to periodically provide tracking region data of the second resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region; a motion analyzer configured to analyze motion of the target object based on the tracking region data; and/or a 
<a href="https://en.wikipedia.org/wiki/Remote_control"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    control device
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 configured to generate an event corresponding to the motion of the target object based on an analysis result of the 
<a href="https://en.wikipedia.org/wiki/Mass_spectrometry"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion analyzer
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
.In some example embodiments, 
<a href="https://en.wikipedia.org/wiki/The_System"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    the system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 may be a 
<a href="https://en.wikipedia.org/wiki/User_interface"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    user interface
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 system that operates by recognizing motion of a user. The target object may include a body of the user or a portion of the body of the user.In some example embodiments, an apparatus for recognizing motion of an object may comprise a first device configured to provide 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 for a scene that includes the object at a first resolution and two-dimensional data for the scene at a second resolution; a second device configured to determine a 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region by recognizing the object based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 and configured to provide tracking region data of the second resolution corresponding to the motion tracking region; and/or a third device configured to analyze the motion of the object based on the tracking region data. The second resolution may be higher than the first resolution. The 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region may correspond to a portion of a frame. The portion of the frame may include the object.In some example embodiments, the first device may comprise a sensing unit. The sensing unit may comprise a depth pixel array. The depth pixel array may be configured to output depth information.In some example embodiments, the first device may comprise a sensing unit. The sensing unit may comprise a color pixel array. The color pixel array may be configured to output color information.In some example embodiments, the first device may comprise a sensing unit. The sensing unit may comprise a depth pixel array and a color pixel array. The depth pixel array may be configured to output depth information. The color pixel array may be configured to output color information.In some example embodiments, the first device may comprise a sensing unit. The sensing unit may comprise a pixel array. The pixel array may be configured to output depth information, color information, or depth and color information.In some example embodiments, a method for recognizing motion of an object may comprise obtaining 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a first resolution with respect to a scene; obtaining two-dimensional data of a second resolution with respect to the scene; recognizing the object in the scene based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
; tracking the object using a 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region to provide tracking region data of the second resolution; and/or analyzing the motion of the object based on the tracking region data. The second resolution may be higher than the first resolution.In some example embodiments, obtaining the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the first resolution may comprise using a depth pixel array of a sensing unit to output depth information.In some example embodiments, obtaining the two-dimensional data of the second resolution may comprise using a color pixel array of a sensing unit to output color information.In some example embodiments, obtaining the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the first resolution may comprise using a depth pixel array of a sensing unit to output depth information and/or obtaining the two-dimensional data of the second resolution may comprise using a color pixel array of the sensing unit to output color information.In some example embodiments, obtaining the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of the first resolution may comprise using a pixel array of a sensing unit to output depth information and/or obtaining the two-dimensional data of the second resolution may comprise using the pixel array of the sensing unit to output color information.BRIEF DESCRIPTION OF THE DRAWINGSThe above and/or other aspects and advantages will become more apparent and more readily appreciated from the following detailed description of example embodiments, taken in conjunction with the accompanying drawings, in which:FIG. 1 is a flowchart illustrating a method of recognizing a motion of an object according to some example embodiments of the inventive concept;FIG. 2 is a diagram illustrating an example of using a system according to some example embodiments of the inventive concept;FIG. 3 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating a system according to some example embodiments of the inventive concept;FIG. 4 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating an example of an image capturing device in a motion recognizing device according to some example embodiments of the inventive concept;FIG. 5 is a diagram illustrating an example of a sensing unit in the image capturing device of FIG. 4;FIG. 6 is a diagram illustrating an example of a pixel array in the sensing unit of FIG. 5;FIG. 7 is a diagram illustrating a frame of an example 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 obtained by an image capturing device;FIG. 8 is a diagram illustrating a frame of an example two-dimensional data obtained by an image capturing device;FIG. 9 is a diagram illustrating an example of a 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region determined according to some example embodiments of the inventive concept;FIG. 10 is a diagram illustrating an example of a sensing unit in the image capturing device of FIG. 4;FIGS. 11A and 11B are diagrams illustrating example pixel arrays in the sensing unit of FIG. 10;FIGS. 12A, 12B, 12C, and 12D are circuit diagrams illustrating some example unit pixels in a pixel array;FIG. 13 is a flowchart illustrating a method of recognizing a motion of an object according to some example embodiments of the inventive concept;FIG. 14 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating an example of a motion region tracker in a motion recognizing device according to some example embodiments of the inventive concept;FIGS. 15A, 15B, and 15C are diagrams illustrating example operations of a synchronizer in the motion region tracker of FIG. 14;FIG. 16 is a diagram for describing tracking region data provided by the motion region tracker of FIG. 14;FIGS. 17A and 17B are diagrams for describing a method of upgrading a 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region according to some example embodiments of the inventive concept;FIG. 18 is a flowchart illustrating a method of recognizing a motion of an object according to some example embodiments of the inventive concept;FIG. 19 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating an example of a motion region tracker in a motion recognizing device according to some example embodiments of the inventive concept;FIG. 20 is a diagram for describing tracking region data provided by the motion region tracker of FIG. 19;FIG. 21 is a flowchart illustrating a method of recognizing a motion of an object according to some example embodiments of the inventive concept;FIG. 22 is a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating an example of a motion region tracker in a motion recognizing device according to some example embodiments of the inventive concept;FIG. 23 is a diagram illustrating an example of a pixel array included in a 
<a href="https://en.wikipedia.org/wiki/Range_imaging"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth sensor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
;FIG. 24 is a 
<a href="https://en.wikipedia.org/wiki/Circuit_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    circuit diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating example time-of-flight (TOF) depth pixels in the pixel array of FIG. 23;FIG. 25 is a 
<a href="https://en.wikipedia.org/wiki/Timing_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    timing diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 illustrating an operation of the TOF pixels of FIG. 24;FIG. 26 is a diagram illustrating an example combination for providing 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
;FIG. 27 is a diagram for describing a method of calculating two-dimensional data based on raw data obtained using depth pixels;FIGS. 28A and 28B are diagrams example combinations for providing two-dimensional data;FIG. 29 illustrates a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a camera including a three-dimensional 
<a href="https://en.wikipedia.org/wiki/Image_sensor"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    image sensor
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 according to some example embodiments of the inventive concept;FIG. 30 illustrates a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a 
<a href="https://en.wikipedia.org/wiki/Computer"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    computer system
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 including a motion recognizing device according to some example embodiments of the inventive concept; andFIG. 31 illustrates a 
<a href="https://en.wikipedia.org/wiki/Block_diagram"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    block diagram
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of interface employable in the computing system of FIG. 30 according to some example embodiments of the inventive concept._____c:1. A method of recognizing motion of an object, the method comprising:periodically obtaining 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 of a first resolution and two-dimensional data of a second resolution with respect to a scene using an image capturing device, wherein the second resolution is higher than the first resolution;determining a 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region by recognizing a target object in the scene based on the 
<a href="https://en.wikipedia.org/wiki/Depth-first_search"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    depth data
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
, such that the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region corresponds to a portion of a frame and the portion includes the target object;periodically obtaining tracking region data of the second resolution corresponding to the 
<a href="https://en.wikipedia.org/wiki/Motion_tracking"><mark class="entity" style="background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    motion tracking
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">TERM</span>
</mark></a>
 region; andanalyzing the motion of the target object based on the tracking region data.</div>
</figure>
</body>
</html>