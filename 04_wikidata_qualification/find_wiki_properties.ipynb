{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import spacy \n",
    "import json\n",
    "import requests \n",
    "import urllib\n",
    "import re\n",
    "import wikipedia\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from wasabi import msg\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_properties = {'P31': 'instance of',\n",
    "                     'P279': 'subclass of',\n",
    "                     'P361': 'part of',\n",
    "                     'P366': 'use',\n",
    "                     'P527': 'has part',\n",
    "                     'P1269': 'facet of'\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wiki_title(term):\n",
    "    title = wikipedia.search(term)\n",
    "    if title:\n",
    "        return title[0]\n",
    "    \n",
    "def find_wiki_summary(term):\n",
    "    try:\n",
    "        return wikipedia.summary(term)\n",
    "    # if it is a ambiguous term, the function will return None as value of summary\n",
    "    except wikipedia.exceptions.WikipediaException:\n",
    "        return None    \n",
    "    \n",
    "def get_wikidata_id(term):  \n",
    "    encoded_term = urllib.parse.quote(term)\n",
    "\n",
    "    url = f\"https://en.wikipedia.org/w/api.php?action=query&format=json&prop=pageprops&ppprop=wikibase_item&redirects=1&titles={encoded_term}\"\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'html.parser').get_text()\n",
    "    \n",
    "    wikidata_id = re.findall('wikibase_item\\\"\\:\\\"(.*)?\\\".*', soup)\n",
    "    \n",
    "    if wikidata_id != []:\n",
    "        return wikidata_id[0]    \n",
    "    \n",
    "def not_disambiguation_page(wikidata_id):\n",
    "    url = \"https://www.wikidata.org/wiki/\" + wikidata_id\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    \n",
    "    div = soup.find(\"div\", {\"class\": \"wikibase-entitytermsview-heading-description\"}).text\n",
    "    \n",
    "    return div != 'Wikimedia disambiguation page'\n",
    "\n",
    "\n",
    "def retrieve_value_P(P):\n",
    "    return target_properties[P]\n",
    "\n",
    "\n",
    "def retrieve_value_Q(Q, reference): \n",
    "    if Q in reference:\n",
    "        return reference[Q]\n",
    "    else:\n",
    "        url = f\"https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&props=labels&languages=en&ids={Q}\"\n",
    "        json_response = requests.get(url).json()\n",
    "        entities = json_response.get('entities')\n",
    "\n",
    "        entity = entities.get(Q)\n",
    "        if entity:\n",
    "            labels = entity.get('labels')\n",
    "            if labels:\n",
    "                en = labels.get('en')\n",
    "                if en:\n",
    "                    value = en.get('value')\n",
    "                    return value\n",
    "\n",
    "\n",
    "def get_target_ItemProperties(wikidata_item, wikidata_id, reference):\n",
    "    url = \"https://www.wikidata.org/w/api.php?action=wbgetclaims&format=json&entity=\"+wikidata_id\n",
    "    json_response = requests.get(url).json()\n",
    "    \n",
    "    # if returns an error page\n",
    "    if list(json_response.keys())[0]=='error':\n",
    "        return None \n",
    "    \n",
    "    properties = [*json_response.get('claims').values()]\n",
    "    \n",
    "    res = np.empty(shape=[0, 3])\n",
    "    \n",
    "    for p in properties:   \n",
    "        for d in p: \n",
    "            dict_ = d['mainsnak']\n",
    "        \n",
    "            # ignore if not a wikibase item or not in target properties\n",
    "            if dict_['datatype'] != 'wikibase-item' or dict_['snaktype'] != 'value' or dict_['property'] not in target_properties:\n",
    "                continue \n",
    "                \n",
    "            # replace all the wikidataItem ID by wikidataItem name          \n",
    "            property_value = retrieve_value_Q(dict_['datavalue']['value']['id'], reference)\n",
    "            \n",
    "            if property_value is None:\n",
    "                continue\n",
    "            \n",
    "            # find property value in the previous lookup table\n",
    "            property_ = retrieve_value_P(dict_['property'])         \n",
    "            row_to_append = [wikidata_item, property_, property_value]  \n",
    "            res = np.append(res, [row_to_append], 0)  \n",
    "  \n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load input/ output file\n",
    "input_path = Path('../data/G06F0011160000.txt') \n",
    "\n",
    "if not input_path.exists():\n",
    "    msg.fail(\"Can't find input file\", in_file, exits=1)   \n",
    "else:\n",
    "    with input_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "        patents = f.read().split('\\n\\n\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using spaCy model <spacy.lang.en.English object at\n",
      "0x7fcb55a1d850>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# load spaCy model\n",
    "nlp = spacy.load('../03_spaCy_ner/output/G_2018/model-last/')\n",
    "msg.info(f\"Using spaCy model {nlp}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading wikidata id lookup table...\n",
      "Loading wikidata property file...\n"
     ]
    }
   ],
   "source": [
    "# load wikidata_id file and wikidata_property file\n",
    "msg.text(\"Loading wikidata id lookup table...\")\n",
    "with open('./wikidata_id.json', 'r', encoding='utf-8') as f:\n",
    "    DICT_WIKIDATA_ID = json.load(f)\n",
    "    \n",
    "msg.text(\"Loading wikidata property file...\")\n",
    "wikidata_property = pd.read_csv('./wikidata_properties.txt', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json file of wikipedia page title and summary\n",
    "with open('../01_make_matching_list/title_summary.json', 'r', encoding='utf-8') as f:\n",
    "    DICT_PAGE_TITLE = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/356 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting wiki information for entities in patent file:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 356/356 [4:09:17<00:00, 42.02s/it]   \n"
     ]
    }
   ],
   "source": [
    "def ner2wiki(text, nlp, wikidata_property): # find entities and complete its wiki information (wiki title, summary, wikidata id and properties)                                                             \n",
    "    doc = nlp(text)\n",
    "    ents = set([ent.text for ent in doc.ents]) - set(wikidata_property.term1.values)\n",
    "    for term in ents:\n",
    "        try:\n",
    "            wiki_title = DICT_PAGE_TITLE[term]['title']\n",
    "        except KeyError:\n",
    "            wiki_title = find_wiki_title(term)  \n",
    "#             if wiki_title:\n",
    "#                 wiki_summary = find_wiki_summary(wiki_title)\n",
    "#                 DICT_PAGE_TITLE.update({term:{'title':wiki_title, 'summary': wiki_summary}})\n",
    "#             else:\n",
    "#                 DICT_PAGE_TITLE.update({term:{'title': None, 'summary': None}})\n",
    "\n",
    "        # find wikidata id and properties\n",
    "        if wiki_title and (wiki_title not in wikidata_property.term1.values) and (wiki_title not in DICT_WIKIDATA_ID.values()):\n",
    "            wikidata_id = get_wikidata_id(wiki_title)\n",
    "            if wikidata_id and not_disambiguation_page(wikidata_id):\n",
    "                DICT_WIKIDATA_ID.update({wiki_title: wikidata_id})\n",
    "                # find wiki properties \n",
    "                list_to_append = get_target_ItemProperties(wiki_title, wikidata_id, DICT_WIKIDATA_ID)\n",
    "                wikidata_property = wikidata_property.append(pd.DataFrame(list_to_append, columns=wikidata_property.columns), ignore_index=True)\n",
    "                wikidata_property.drop_duplicates()\n",
    "    return wikidata_property     \n",
    "\n",
    "msg.text('Extracting wiki information for entities in patent file:')\n",
    "for patent in tqdm(patents[285:]):\n",
    "    wikidata_property = ner2wiki(patent, nlp, wikidata_property)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the update files\n",
    "with open('./wikidata_id.json', \"w\", encoding='utf-8') as f: \n",
    "    json.dump(DICT_WIKIDATA_ID, f, indent = 4)\n",
    "wikidata_property.to_csv('./wikidata_properties.txt', index = False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.title_summary, \"w\", encoding='utf-8') as f: \n",
    "    json.dump(DICT_PAGE_TITLE, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base')",
   "language": "python",
   "name": "python37464bitbase5e3e771f486c4f06aa164a453e60de03"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
